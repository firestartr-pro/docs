[{"id":0,"href":"/docs/features/build_and_dispatch_docker_images/","title":"Build and Dispatch Docker Images","section":"Features","content":"Build and dispatch workflows# This feature is a collection of seven workflows (along with their configuration files), which when used in conjunction allow a user to build final docker images from a release tag, or snapshot images, from a pre-release tag, or commit SHA; Once the images are built, the make_dispatches workflow dispatches the image update automatically to their respective state repositories. They can be thought of as two different but closely related features.\nThe workflows trigger_dispatch_on_releases.yaml, trigger_dispatch_on_pre-releases.yaml and trigger_dispatch_on_snapshot.yaml only serve to trigger make_dispatches.yaml after a successful automated call to any of the build_docker_\u0026lt;type\u0026gt;.yaml workflows. They require no other explanation so they won\u0026rsquo;t be discussed in this README\nBuild images# Composed of the workflows .github/workflows/build_docker_releases.yaml, .github/workflows/build_docker_pre-releases.yaml and .github/workflows/build_docker_snapshots.yaml.\nConfiguration# All build workflows use the same configuration file located at .github/build_images.yaml. Its format is as follows:\nsnapshots: # Configuration specific for snapshots, used by build_docker_pre-releases and build_docker_snapshots flavor: # Flavor-specific configuration. A flavor can be named anything as long as it\u0026#39;s a valid YAML key dockerfile: path/to/dockerfile # Path relative to the repo root folder auto: false # Whether or not to automatically build this flavor when the * input is specified (see \u0026#34;Inputs\u0026#34; below). Defaults to false build_args: # Environment variables to set during the image building process # ENV_VARIABLE_NAME: env_variable_value API_URL: https://api.com/url UI_COLOR: \u0026#39;#125690\u0026#39; extra_registries: # List of registries, other than the default one (see \u0026#34;Defaults\u0026#34; below), where to upload the image - name: registry.azure.io repository: service/repo auth_strategy: azure_oidc # Multiple kinds of registries can be specified - name: aws.amazon.com repository: programs/program-name # Repository names can be different between registries auth_strategy: aws_oidc # Multiple auth strategies should be supported but they currently aren\u0026#39;t extra_tags: # List of extra tags to publish the image as - latest - stable platforms: # List of platforms for which to build the image. If unspecified, the image will be built for linux/amd64 only # Only linux/amd64 and linux/arm64 are currently supported - linux/amd64 - linux/arm64 another-flavor: dockerfile: path/to/dockerfile registry: # The default registry can be overridden (see \u0026#34;Defaults\u0026#34; below) name: nondefault.registry.es repository: nondefault/repo auth_strategy: azure_oidc # Can be any of azure_oidc or aws_oidc releases: # Configuration specific for releases, used by build_docker_releases release-flavor: ... # The same configuration parameters as snapshots can be used here snapshots: all image flavors included within this key will count as a snapshot flavor, and thus will be built when the build_docker_snapshots or build_docker_pre-releases workflows are executed releases: all image flavors included within this key will count as a release flavor, and thus will be built when the build_docker_releases workflow is executed Inside each of these two keys (type keys) there will be one or more flavor keys:\n\u0026lt;flavor\u0026gt;: the name of the flavor to build. Will be appended to the end of the final image tag. The same flavor name cannot be duplicated inside the same type key, but it can be in both type keys at the same time (i.e. snapshots cannot contain two default flavor keys, but both snapshots and releases can contain a single default key at the same time) Each flavor key can contain:\ndockerfile: path to the dockerfile to use when building this image, relative to the repo root (i.e. if the dockerfile is located at https://www.github.com/firestartr-test/code-repo/docker/Dockerfile, this keys\u0026rsquo; value will be docker/Dockerfile) auto: whether or not to build this flavor when the flavors input equals *. Defaults to false build_args: key-value pairs that are set as environment variables when building the image. The key is the environment value name, and the value its value, and any number of them can be specified. registry: a registry object that overrides the default registry. If this key is specified, no image will be uploaded to the default registry extra_registries: a list of registry object to where the image will be uploaded to, in addition to the default registry. If this key is specified, the image will still be uploaded to the default registry extra_tags: optional list of strings representing extra tags to publish the image as. While the default tag is constructed as \u0026lt;flavor\u0026gt;_\u0026lt;version\u0026gt;, no extra info will be appended to these tags. E.g. if the flavor is default and the version is v1.2.3, the default tag will be default_v1.2.3, and if latest is specified as an extra tag, the image will also be tagged as latest. For this reason, all values listed as extra_tags must be unique across all flavors and types (i.e., two different flavors cannot both have latest as an extra tag, even if one is a snapshot and the other a release) platforms: optional list of strings representing platforms for which to build the image. If unspecified, the image will be built for linux/amd64 only. Currently, only linux/amd64 and linux/arm64 are supported. If specified, the image will be built only for the platform(s) listed. Registry objects# A registry object contains the following keys:\nname: base URL of the registry. E.g. when uploading a image to registry.com/image_repo/image_tag, this key\u0026rsquo;s value should be registry.com repository: name of the repository inside of the registry name to where upload the image to. E.g. when uploading a image to registry.com/image_repo/image_tag, this key\u0026rsquo;s value should be image_repo auth_strategy: type of authentication to use for login, as different registries require different authentication methods. Though many are defined, currently only azure_oidc and aws_oidc are supported Inputs# All build workflows use the same inputs. They are:\nfrom: point of the code history from which the image will be built. Can be a short or long commit SHA, a branch name or a tag name flavors: which flavors to build. Each workflow will only look for flavors in their respective section (i.e., build_docker_releases will only build flavors from releases, while the other two will only build flavors from snapshots). Can be a single flavor, a list of comma separated flavors (spaces are trimmed) or *. * builds all flavors that are set as auto in build_images.yaml (see Configuration) Defaults# All build workflows use the same defaults. There are two types of defaults: environment variables and defaults defined by code. The environment variables are:\nDOCKER_REGISTRY_RELEASES: base URL of the Docker registry for releases. Follows the same format as extra_registries.name and registry.name (e.g. prefapp.azureacr.io. See Configuration) DOCKER_REGISTRY_SNAPSHOTS: base URL of the Docker registry for snapshots. Follows the same format as extra_registries.name and registry.name (e.g. prefapp.azureacr.io. See Configuration) The defaults defined by code are:\nregistry.repository defaults to the name of the repo the workflow is being executed on, including the owner. E.g. prefapp/test-repo-rundagger flavor.auto defaults to false Make dispatches# Composed of the workflow .github/workflows/make_dispatches.yaml.\nConfiguration# The make_dispatches workflow uses the configuration file located at .github/make_dispatches.yaml. Its format is as follows:\ndeployments: # Has a list of configurations - tenant: prefapp platform: az-cluster # Platform where we want to dispatch changes to type: snapshots # Either snapshots or releases flavor: flavor1 # Flavor to dispatch for this deployment. Must have the same name as a flavor defined in build_images.yaml, under the chosen type (see \u0026#34;Build images -\u0026gt; Configuration\u0026#34; above) version: $branch_dev # See \u0026#34;About the version field\u0026#34; below registry: prefapp.azureacr.io # Optional. Registry where the image was uploaded to (see \u0026#34;Defaults\u0026#34; below) image_repository: service/test-repo # Optional. Repo where the image was uploaded to (see \u0026#34;Defaults\u0026#34; below) base_path: apps # Can be left empty or unspecified application: test-repo-rundagger env: dev # You can specify one of the following service_names: [\u0026#39;test-repo-service\u0026#39;] # List, multiple services can be specified # OR image_keys: [\u0026#39;/test-repo-service/image\u0026#39;] # List, multiple image keys can be specified # So, in this example github.com/prefapp/prefapp-state/apps/\u0026lt;platform-type\u0026gt;/az-cluster/prefapp/test-repo-rundagger/dev would be updated when making this dispatch # An entry is created for each tenant - platform - type - flavor combination we want to dispatch - tenant: prefapp platform: az-cluster type: snapshots flavor: another-flavor ... - tenant: prefapp platform: aks-cluster type: snapshots flavor: flavor1 ... deployments: contains the list of deployments to dispatch. Has no special data or meaning but must be specified for the dispatch workflow to work Each deployment contains:\ntenant: name of the tenant that will be used for this deployment. Used to determine which file to update when dispatching, and also for validation: the tenant must be specified in the list of valid tenants in the platform configuration of the .firestartr repo platform: name of the platform that will be used for this deployment. Used to determine which file to update when dispatching, and also for validation: the specified platform must have an associated configuration file. From the platform configuration file, it\u0026rsquo;s type will also be used when determining which file to update. type: deployment type, can be either of snapshots or releases. Used when calling the dispatch workflow to filter which dispatches to make. flavor: name of the flavor that will be used for this deployment. Used to compose the image tag, which will be written to the corresponding deployment files and must have been built beforehand. version: version of the code used in the image. Used to compose the image tag, which will be written to the corresponding deployment manifests and must have been built beforehand. See About the version field for more info on the possible values this parameter could have. registry: registry to where the image has been uploaded to. This parameter is optional, and the registry specified in either the DOCKER_REGISTRY_RELEASES and DOCKER_REGISTRY_SNAPSHOTS variables will be used by default (see Defaults). image_repository: repository to where the image has been uploaded to. This parameter is optional, and the repository specified in the application configuration will be used by default. base_path: legacy parameter. Appends this value to the deployment path, so that the final result ends like: \u0026lt;base_path\u0026gt;/\u0026lt;platform-type\u0026gt;/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;env\u0026gt;. Should only be used when dispatching to an old state repository. application: name of the application that will be dispatched. Used for validation, as a valid configuration must exist for the application, and to get the default state and image repo. env: environment where to deploy to. Could technically be any string, but usually is one of dev, pre or pro. Must be a value included in the registry configuration file. service_names: names of the services this deployment will be dispatched to. These values will be validated against the application configuration in the .firestartr repo: each value of this field must be included within the service_names of the current repository for this deployment\u0026rsquo;s application. Cannot be used at the same time as the image_keys parameter. image_keys: list of JSONPatch paths to patch the deployment YAMLs. These values aren\u0026rsquo;t validated against any configuration file. Cannot be used at the same time as the service_names parameter. Workflow inputs# image_type: which types of images to build. Can be releases, snapshots (default) or * (both) flavors: which flavors to dispatch. Will only look for flavors inside image_type. Can be a single flavor, a list of comma separated flavors (spaces are trimmed) or *. * builds all flavors that are set as auto in build_images.yaml (see Build Images -\u0026gt; Configuration) overwrite_version: instead of using version, the value of this input will be used for each image_type and flavors if specified. Can be any of the valid make_dispatches.yaml configuration keywords (see About the version field) overwrite_tenant: instead of using tenant, the value of this input will be used for each image_type and flavors if specified. overwrite_env: instead of using env, the value of this input will be used for each image_type and flavors if specified. filter_by_tenant: only dispatch to the tenant specified by this input. These tenants must already have an entry in make_dispatches.yaml for image_type and flavors (i.e. this filter won\u0026rsquo;t do anything if the flavors of image_type could not be dispatched to filter_by_tenant to begin with). Can be a single tenant, a list of comma separated tenants (spaces are trimmed) or * (all tenants for a given image_type and flavors) filter_by_env: only dispatch to the env specified by this input. These envs must already have an entry in make_dispatches.yaml for image_type and flavors (i.e. this filter won\u0026rsquo;t do anything if the flavors of image_type could not be dispatched to filter_by_env to begin with). Can be a single env, a list of comma separated envs (spaces are trimmed) or * (all envs for a given image_type and flavors) filter_by_platform: only dispatch to the platform specified by this input. These platforms must already have an entry in make_dispatches.yaml for image_type and flavors (i.e. this filter won\u0026rsquo;t do anything if the flavors of image_type could not be dispatched to filter_by_platform to begin with). Can be a single platform, a list of comma separated platforms (spaces are trimmed) or * (all platforms for a given image_type and flavors) workflow_run_id: the workflow needs the build summary of the last build_docker_\u0026lt;type\u0026gt;. If you have access to it, you can give it as an input here. If left blank, it will be automatically calculated by the workflow Defaults# The defaults are defined as environment variables and are:\nDOCKER_REGISTRY_RELEASES: base URL of the Docker registry for releases. Follows the same format as extra_registries.name and registry.name (e.g. prefapp.azureacr.io. See Build Images -\u0026gt; Configuration) DOCKER_REGISTRY_SNAPSHOTS: base URL of the Docker registry for snapshots. Follows the same format as extra_registries.name and registry.name (e.g. prefapp.azureacr.io. See Build Images -\u0026gt; Configuration) About the version field# Specifies which image to dispatch. Can have any of the following values:\n$latest_release: the latest available release $latest_prerelease: the latest available prerelease $branch_\u0026lt;branch_name\u0026gt;: the latest available image associated to \u0026lt;branch_name\u0026gt; Any commit SHA (both short and long) or tag: the latest available image associated to the input About the Trigger deployment workflow# A second workflow, Trigger deployment (with filename trigger_deployment.yaml), is also included. This is a simplified version of the more complex make_dispatches workflow, and is meant to be used by users who want to trigger simple deployments, to a specific tenant, platform and env. Its inputs are:\nflavor: which flavor to dispatch. Contrary to the normal make_dispatches workflow, only a single flavor can be specified here, and it will also dictate the image_type (i.e., if the flavor exists in snapshots, the image type will be snapshots, and the same for releases. If it exists in both, * will be used). Defaults to default version: equivalent to the overwrite_version input of the normal make_dispatches workflow, with one notable difference: if the value is left empty, $latest_release will be used tenant: equivalent to the filter_by_tenant input of the normal make_dispatches workflow. Mandatory platform: equivalent to the filter_by_platform input of the normal make_dispatches workflow. Mandatory env: equivalent to the filter_by_env input of the normal make_dispatches workflow. Mandatory For ease of use, the tenant, platform and env inputs have been made into choice dropdowns, populated from the configuration files in the .firestartr repository. This population is done automatically via the update-features-with-dot-firestartr-info.yaml workflow from the claims_repo feature. When this feature is installed for the first time, it\u0026rsquo;s needed to either launch that workflow manually or wait until it automatically runs at midnight UTC, so that the dropdowns get populated with the correct values.\nFeature arguments# build_snapshots_branch: which branch triggers an automatic snapshot build when pushed to. Defaults to the default branch of the repository, specified under providers.github.branchStrategy.defaultBranch inside the claim (usually main or master) auth_strategy: which authentication strategy to use when logging into the docker registries. Defaults to azure_oidc build_snapshots_filter: filter to apply when automatically building snapshot images. Defaults to an empty string (no filter) build_pre_releases_filter: filter to apply when automatically building pre-release images. Defaults to an empty string (no filter) build_releases_filter: filter to apply when automatically building release images. Defaults to an empty string (no filter) default_snapshots_flavors_filter: default filter shown in the UI when manually executing the build snapshots workflow. Defaults to * default_pre_releases_flavors_filter: default filter shown in the UI when manually executing the build pre-releases workflow. Defaults to * default_releases_flavors_filter: default filter shown in the UI when manually executing the build releases workflow. Defaults to * firestartr_config_repo: name of the repository that houses all the config files used by the make_dispatches workflow. Defaults to ${{ github.repository_owner }}/.firestartr make_dispatches_config_file_path: path to the make_dispatches config file to be used by the make_dispatches workflow, relative to the root of the repository. Defaults to .github/make_dispatches.yaml apps_folder_path: path to the apps folder to be used by the make_dispatches workflow. This is a path local to the runner, used after all the configuration repositories have been downloaded. Defaults to .firestartr/apps platform_folder_path: path to the platforms folder to be used by the make_dispatches workflow. This is a path local to the runner, used after all the configuration repositories have been downloaded. Defaults to .firestartr/platforms registries_folder_path: path to the docker_registries folder to be used by the make_dispatches workflow. This is a path local to the runner, used after all the configuration repositories have been downloaded. Defaults to .firestartr/docker_registries NOTE: make_dispatches downloads the firestartr_config_repo repository to access the configuration files via the apps_folder_path, platform_folder_path and registries_folder_path feature arguments. However, firestartr_config_repo is always downloaded under the .firestartr folder, regardless of what the actual name of the repository is, so the paths specified in the *_folder_path arguments should all start with .firestartr/ (unless the folders are located in another repo)\n"},{"id":1,"href":"/docs/features/catalog_repo/","title":"Catalog Repo","section":"Features","content":"Catalog Repo# The catalog_repo feature provides the initial setup for the catalog repository (generally named catalog).\nThis repository stores the rendered Custom Resources (CRs) that represent the organization\u0026rsquo;s service catalog. The CRs are generated (hydrated) by rendering all claims from the claims repository using the Firestartr CLI, and the results are managed through pull requests.\nRepository Structure# .config/ - Configuration directory for globals and initializers used during the hydration process. Root directory - Contains the rendered Backstage catalog entities YAML files representing the catalog entities. Workflows Provided# Hydrate (hydrate.yaml) - Renders all claims into catalog CRs and opens a pull request with the changes. Runs on a schedule (configurable via crontab argument, defaults to every 6 hours) and can also be triggered manually. Supports automatic merging via the AUTO_MERGE control file. Auto-Merge Hydration PRs# Hydration pull-requests can be automatically merged by adding an empty AUTO_MERGE file to the root of this repository, in the default branch.\nHow to enable# Create an empty AUTO_MERGE file at the root of the repository:\ntouch AUTO_MERGE git add AUTO_MERGE git commit -m \u0026#34;Enable auto-merge for hydration PRs\u0026#34; git pushHow it works# When the AUTO_MERGE file is present, the hydrate workflow will automatically merge the PR after creating it. If the file is removed, hydration PRs will require manual review and merge. "},{"id":2,"href":"/docs/features/charts_repo/","title":"Charts Repo","section":"Features","content":"GitHub Actions Workflows Overview# This repository uses four main workflows to automate the Helm chart lifecycle and version management. The following sections provide a concise description of what each workflow does without delving into implementation details.\n1. release-please.yaml# Automates semantic versioning and release generation.\nRuns on: every push to the main branch or via workflow_dispatch.\nKey actions:\nExecutes Release‚ÄØPlease to scan commits, update the manifest, and create PRs/tags when required. Exposes outputs with the impacted charts (paths_released) and their new versions. If releases are detected, triggers generate-artifact.yaml (one run per chart) through a matrix with max-parallel:‚ÄØ1 to build and publish artifacts. 2. generate-artifact.yaml# Builds and publishes a Helm chart as either a release or snapshot artifact.\nInvocation method: called via workflow_call with inputs for chart path, version, and publication type (releases or snapshots).\nMain steps:\nRetrieves a token from a GitHub¬†App to access the private .firestartr repository with registry configs. Checks out the charts repository and recursively updates dependencies using helm dep up. Packages the chart, determines its name and version, then uploads it to (a) an OCI registry or (b) GitHub¬†Pages, based on the HELM_CHARTS_PUBLICATION_TYPE variable. When publishing to GitHub¬†Pages, indexes the Helm repo and pushes to the configured branch and path. 3. generate-snapshot.yaml# Automatically generates chart snapshots during Pull Requests.\nRuns on:\nEvery pull_request event when a PR is labeled or updated (labeled, synchronize). Manually via workflow_dispatch, allowing the user to specify a chart. What it does:\nDetects charts modified in the PR. Calls generate-artifact.yaml with release_type:‚ÄØsnapshots to build and upload a snapshot version to the designated registry. 4. pr-verify.yaml# Validates chart changes before a Pull Request is merged.\nRuns on: every PR that touches charts/** or manually via workflow_dispatch.\nValidation tasks:\nScope¬†Check¬†‚Äî blocks PRs that modify more than one chart at a time. Dependency¬†Update¬†‚Äî refreshes Helm dependencies, including local and remote subcharts. Linting \u0026amp; Template¬†‚Äî executes helm lint --strict and renders templates for inspection. Yamllint¬†‚Äî runs yamllint on rendered output and posts results in a persistent comment using sticky‚Äëpull‚Äërequest‚Äëcomment. Fails the workflow if lint errors are found. Summary Flow# Push to main ‚Üí release-please.yaml decides if new releases are required.\nFor each chart needing a release ‚Üí generate-artifact.yaml publishes the new version.\nPull Request cycle:\npr-verify.yaml validates the PR and leaves feedback. If on‚Äëdemand testing is needed, generate-snapshot.yaml builds and uploads a snapshot. "},{"id":3,"href":"/docs/features/claims_repo/","title":"Claims Repo","section":"Features","content":"Claims repo feature# This feature installs the workflows necessary for manually hydrating and deleting GitHub and TFWorkspace claims, plus two additional workflows to import manually created GitHub resources into GitHub claims and a pr-verify workflow.\nHydrating claims# Create a PR with the desired changes. Check the changes that have been commited are the changes you\u0026rsquo;ve actually done.\nOnce the pr-verify workflow is finished, if there are no errors, merge your PR into the main branch.\nHead over to the Actions tab of the claims repo and select the \u0026lt;Your claim type\u0026gt; claim: hydrate workflow on the left side list.\nThe Actions tabs\nSelect Run workflow on the right hand side of the screen, introduce the name of the claim to be hydrated and select the claim type (if it is a GitHub claim). NOTE: the name of the claim should be the value of the field name inside the claim YAML. Setting this value to the file name may result in the workflow not working as expected. Hydrating a GitHub claim\nHydrating a TFWorkspace claim\nPress the green button and wait for the workflow to finish. When it\u0026rsquo;s done, click its entry in the list and there\u0026rsquo;ll be a link to the state-github or state-infra repo PR. You can use it to review the PR and merge it if everything is correct. The action summary with a link to the state repo\nDeleting claims# Head over to the Actions tab of the claims repo and select the \u0026lt;Your claim type\u0026gt; claim: delete workflow on the left side list. The Actions tabs\nSelect Run workflow on the right hand side of the screen, introduce the name of the claim to be deleted and select the claim type (if it is a GitHub claim). NOTE: the name of the claim should be the value of the field name inside the claim YAML. Setting this value to the file name may result in the workflow not working as expected. Deleting a GitHub claim\nDeleting a TFWorkspace claim\nPress the green button and wait for the workflow to finish. When it\u0026rsquo;s done, click its entry in the list and there\u0026rsquo;ll be two links: one to the state-github or state-infra repo PR, deleting the CR file, and another to the claims repo PR, deleting the claim file. You can use them to review both PRs and merge them if everything is correct. The action summary with two links to the state and claims repo\nImport already created GitHub resources# Head over to the Actions tab of the claims repo and select the üìú Import GitHub resources workflow on the left side list. The Actions tabs\nSelect Run workflow on the right hand side of the screen, and add filters to the type of resources you want to import. The filters can be: REGEXP=[regex]: import all resources that match the regex [regex]. NAME=[name]: import all resources with the name [name]. The name must be an exact match. SKIP=SKIP: skip this import. Press the green button and wait for the workflow to finish. Once the workflow is finished, a PR will be created both in the state-github repo and the claims repo. Merge them both and the resource will be imported Massively updating claims\u0026rsquo; features# Head over to the Actions tab of the claims repo and select the Update claims' features workflow on the left side list. The Actions tabs\nSelect Run workflow on the right hand side of the screen, and add filters to narrow the features and claims you want to update. The filters can be: Features list: mandatory filter. Either a single feature name or a list of comma separated values, each being a different feature name. Claims list: optional filter. Either a single claim.name, a list of comma separated values, each being a different claim.name or an empty value, which will update all applicable claims. Note that claim.name refers to the value of the name field inside the claim YAML file and not to the name of the YAML file itself. Version constraint: optional filter. Either a valid version, a valid version constraint or an empty value, which will update to the latest avaliable version. A valid version is a version that\u0026rsquo;s avaliable for all features in Features list, in the major.minor.patch semver format, and will result in the feature being updated to that exact version. Valid version constraint documentation can be found here (comparisions work fine too) and will result in the feature being updated to the latest version avaliable that satisfies the specified constraint. Press the green button and wait for the workflow to finish. Once the workflow is finished, a PR will be created for each claim that was updated. The PR body will contain a list of changes made since the version that was currently installed up until the version that was updated to. Merge them all and then hydrate the claims About the update-features-with-dot-firestartr-info workflow# This workflow is executed every day at 00:00 UTC. It clones the .firestartr repository and uses it to populate arguments for the following features:\nbuild_and_dispatch_docker_images: populates the arguments tenant_list, platform_list and env_list This workflow can also be executed manually if needed.\nFeature arguments# state_github_repo: the name of the repository where the GitHub related CRs are stored. Defaults to state-github state_infra_repo: the name of the repository where the Terraform and ExternalSecrets related CRs are stored. Defaults to state-infra catalog_repo: the name of the repository where the Backstage catalog related CRs are stored. Defaults to catalog "},{"id":4,"href":"/docs/features/features_repo/","title":"Features Repo","section":"Features","content":"Firestartr Features Repository# About# This repository is a monorepo that holds all custom Firestartr features for your organization.\nEvery feature lives under packages/, uses Mustache templating, and is defined via a powerful config.yaml (file rendering + JSON Patch support for Backstage catalog, etc.).\nFeatures are automatically versioned with Release Please using Conventional Commits.\nRepository structure# . ‚îú‚îÄ‚îÄ packages/ ‚îÇ ‚îú‚îÄ‚îÄ my-awesome-feature/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ templates/ # Mustache templates go here ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.yaml # Feature definition ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ package.json ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ README.md ‚îÇ ‚îî‚îÄ‚îÄ another-feature/ ‚îú‚îÄ‚îÄ .release-please-manifest.json ‚îú‚îÄ‚îÄ release-please-config.json ‚îú‚îÄ‚îÄ .github/ ‚îÇ ‚îî‚îÄ‚îÄ workflows/ # Optional CI/release workflows ‚îî‚îÄ‚îÄ README.mdContributing to Features# Creating a new feature# Create a new branch from main with the name of the feature you want to create. Create a new folder in packages/ with the name of the feature (kebab-case recommended). Create a templates/ folder inside it. Create a config.yaml file using the structure below. Add the new feature to .release-please-manifest.json and release-please-config.json. Create a package.json file using npm init. config.yaml structure\nfeature_name: example # The following are the args that will be used to render the templates. # There are two types of args: # $ref: replaced by the value from the metadata section of the config.yaml file # $lit: literal value args: ORG: $ref: [spec, org] REPO_NAME: $ref: [metadata, name] # Files to render from the templates/ folder files: - src: mkdocs.yaml dest: mkdocs.yaml # If upgradable is true, the user can modify it. It will not be overridden or deleted on update/uninstall. upgradable: true - src: docs/index.md dest: docs/index.md upgradable: true # Patches to apply in the component catalog file using JSON Patch (RFC 6902) when the feature is installed patches: - name: \u0026#34;add_annotation\u0026#34; op: \u0026#34;add\u0026#34; path: \u0026#34;/metadata/annotations/backstage.io~1techdocs-ref\u0026#34; # Values can use Mustache syntax with {{| VAR |}} value: \u0026#34;url:https://github.com/{{| ORG |}}/{{| REPO_NAME |}}/tree/main\u0026#34;Template syntax# We use the Mustache template engine. You can add logic with conditionals:\n{{| #condition |}} {{| variable |}} ‚Üê this renders when condition is true {{| /condition |}} {{| ^condition |}} In case the condition is false {{| /condition |}}Updating an existing feature# Create a new branch from main. Modify the feature inside packages//. Merge the branch using a Conventional Commit ‚Üí Release Please will automatically create a new release. Removing an existing feature# Create a new branch from main. Remove the feature directory under packages/. Remove the feature from .release-please-manifest.json. Merge the branch using a Conventional Commit ‚Üí new release is created without the feature. Testing Features# Using generic-fixtures/cr.yaml# The generic-fixtures/cr.yaml file provides a reusable Custom Resource (CR) fixture for testing feature rendering. This fixture contains a complete example of a FirestartrGithubRepository resource with all common fields populated.\nHow to use it# Each feature package includes a render_tests.yaml file that defines test cases. To use the generic fixture:\n# packages/\u0026lt;feature-name\u0026gt;/render_tests.yaml tests: - name: test1 cr: \u0026#34;../../generic-fixtures/cr.yaml\u0026#34;What the fixture provides# The generic CR fixture includes:\nMetadata: annotations, labels, and resource name Spec.org: Organization name (firestartr-test) Spec.context: Backend and provider references Spec.firestartr: Technology stack and state key configuration Spec.repo: Repository settings (visibility, branches, merge options, etc.) Spec.actions: OIDC configuration Spec.permissions: Team/group permissions Spec.branchProtections: Branch protection rules Creating custom fixtures# If your feature requires specific CR fields not covered by the generic fixture, you can:\nCreate a custom fixture in your feature\u0026rsquo;s __tests__/ folder:\n# packages/\u0026lt;feature-name\u0026gt;/__tests__/custom-cr.yaml apiVersion: firestartr.dev/v1 kind: FirestartrGithubRepository metadata: name: my-custom-resource spec: # ... your custom fields Reference it in your render_tests.yaml:\ntests: - name: custom-test cr: \u0026#34;./__tests__/custom-cr.yaml\u0026#34; Links# Firestartr Documentation All Official Features Built with ‚ù§Ô∏è using Firestartr\n"},{"id":5,"href":"/docs/features/firestartr_repo/","title":"Firestartr Repo","section":"Features","content":"Firestartr Repo# "},{"id":6,"href":"/docs/features/issue_templates/","title":"Issue Templates","section":"Features","content":"Issue Templates# "},{"id":7,"href":"/docs/features/release_please/","title":"Release Please","section":"Features","content":"release-please# Overview# This repository is managed with release-please, a tool that automates the creation of release PRs and GitHub tags based on Conventional Commits.\nThe purpose of this document is to describe how to:\nTag and release the repo using conventional commits. Understand how release-please works under the hood. Configure the repository for automated releases. üè∑Ô∏è Tagging and Releasing Terraform Modules# release-please automatically creates releases based on conventional commit messages.\nSemantic Versioning# Each release follows semantic versioning: MAJOR.MINOR.PATCH\nLevel Description Example of change MAJOR Breaking change Modify resources or outputs in an incompatible way MINOR New features (non-breaking) Add new variables or optional functionality PATCH Bug fixes / small improvements Fix naming, typos, or small logic issues ‚úÖ Example Commit Messages# Commit Type Example Commit Effect fix fix: correct bucket tag format Increments PATCH version feat feat: add bucket versioning support Increments MINOR version feat! feat!: enable mandatory encryption (breaking change) Increments MAJOR version chore chore: update root module version No version bump (metadata only) docs docs: update README with usage examples No version bump ci ci: update GitHub Actions workflow for terraform validation No version bump ‚öôÔ∏è How It Works# The following diagram shows how release-please integrates with GitHub Actions:\nflowchart TD A[Developer pushes or merges PR to main] --\u0026gt; B[GitHub Action triggers release-please] B --\u0026gt; C[release-please analyzes commit history] C --\u0026gt; D{New version required?} D --\u0026gt;|Yes| E[Generate release PR with changelog and updated versions] D --\u0026gt;|No| F[Do nothing] E --\u0026gt; G[Developer reviews and merges release PR] G --\u0026gt; H[GitHub creates new tag and release automatically]Detailed Flow# When a commit is pushed or a PR is merged into main (typically via squash merge), a GitHub Action runs. The action executes release-please, which scans commit messages since the last release. If commits with feat, fix, or breaking changes (!) are found, it determines the appropriate version bump (patch, minor, or major). A release PR is automatically created or updated, containing: Updated changelog. Updated version numbers in module files. When the release PR is merged, GitHub automatically: Creates a tag (e.g., v1.2.3) Publishes a release entry. üß© Configuration# The behavior of release-please is controlled via the release-please-config.json and .release-please-manifest.json files, which define module paths, release types, and version tracking.\nIn the file release-please-config.json, we define how release-please should behave. It can be configured for single repositories or monorepos, and supports different release types (e.g., terraform-module, node, python, etc.). For more details, see the manifest releaser documentation.\nBy default, a simple configuration for a repository without monorepo structure looks like this:\n{ \u0026#34;packages\u0026#34;: { \u0026#34;.\u0026#34;: {} } }The file .release-please-manifest.json keeps track of the current released versions of each package or module, for example:\n{ \u0026#34;.\u0026#34;: \u0026#34;1.2.3\u0026#34; } üß† Notes# Use clear, descriptive commit messages that follow the Conventional Commits format. Use types like ci: or docs: for changes that should not trigger new releases. Always review the release PR before merging to ensure that changelogs and versions are correct. Tag prefixes (e.g., aws-s3-v1.2.3) are derived from the module name defined in the configuration file. "},{"id":8,"href":"/docs/features/state_github/","title":"State Github","section":"Features","content":"State GitHub# The state_github feature provides the initial setup for the GitHub state repository (generally named state-github).\nThis repository stores the rendered Custom Resources (CRs) for GitHub-related claims such as ComponentClaim (repositories), GroupClaim (teams), UserClaim (memberships), and OrgWebhookClaim (organization webhooks). The CRs in this repository are generated (hydrated) by the claims repository workflows and managed through pull requests.\nRepository Structure# .config/ - Configuration directory containing: resources/ - Default values for GitHub resource claims (defaults_github_group.yaml, defaults_github_membership.yaml, defaults_github_repository.yaml). branch_strategies.yaml - Branch protection strategies configuration. expander_branch_strategies.yaml - Expander branch strategies for component claims. Root directory - Contains the rendered CR files (.yaml) generated from the claims repository. Workflows Provided# Auto-merge (auto-merge.yaml) - Automatically merges hydration pull requests when the AUTO_MERGE control file is present. Auto-Merge Hydration PRs# Hydration pull-requests created by the claims repository workflows can be automatically merged by adding an empty AUTO_MERGE file to the root of this repository, in the default branch.\nHow to enable# Create an empty AUTO_MERGE file at the root of the repository:\ntouch AUTO_MERGE git add AUTO_MERGE git commit -m \u0026#34;Enable auto-merge for hydration PRs\u0026#34; git pushHow it works# When the AUTO_MERGE file is present, any hydration PR (branches starting with automated/) will be automatically merged. If the file is removed, hydration PRs will require manual review and merge. The auto-merge is also supported via the automerge input in the claims repository hydrate workflow. "},{"id":9,"href":"/docs/features/state_infra/","title":"State Infra","section":"Features","content":"State Infra# The state_infra feature provides the initial setup for the infrastructure state repository (generally named state-infra).\nThis repository stores the rendered Custom Resources (CRs) for infrastructure-related claims such as TFWorkspaceClaim and SecretsClaim. The CRs in this repository are generated (hydrated) by the claims repository workflows and managed through pull requests.\nRepository Structure# .config/ - Configuration directory for resource defaults and initializers used during hydration. Root directory - Contains the rendered CR files (.yaml) generated from the claims repository. Workflows Provided# Auto-merge (auto-merge.yaml) - Automatically merges hydration pull requests when the AUTO_MERGE control file is present. Auto-Merge Hydration PRs# Hydration pull-requests created by the claims repository workflows can be automatically merged by adding an empty AUTO_MERGE file to the root of this repository, in the default branch.\nHow to enable# Create an empty AUTO_MERGE file at the root of the repository:\ntouch AUTO_MERGE git add AUTO_MERGE git commit -m \u0026#34;Enable auto-merge for hydration PRs\u0026#34; git pushHow it works# When the AUTO_MERGE file is present, any hydration PR (branches starting with automated/) will be automatically merged. If the file is removed, hydration PRs will require manual review and merge. The auto-merge is also supported via the automerge input in the claims repository hydrate workflow. "},{"id":10,"href":"/docs/features/state_repo/","title":"State Repo","section":"Features","content":"State repo deploy workflows# Summarizing up it deploys applications using a push model, using Helm and Helmfile to manage the deployments. It is triggered by pull requests made to the state repository, which contains configuration files for the applications to be deployed. It is designed to work with multiple cloud providers, including Azure and AWS, by leveraging GitHub\u0026rsquo;s OIDC capabilities for secure authentication.\nOverview# This feature is a collection of two sets of workflows, with one configuration file for each of them. In general lines, the workflows perform the following actions:\nWhen a pull request is opened, the feature analyzes the changes in the state repo and writes a commentary for each affected environment. When a pull request is merged, the feature analyzes the changes in the state repo and deploys the upcoming changes. When triggered manually, the feature can also apply or destroy the deployed resources in a specific environment. apps# apps-apply.yml: triggered when a pull request is merged, it deploys the applications defined in the state repo to the target environments. apps-destroy.yml: triggered manually, it removes the applications defined in the state repo from the target environment. apps-manual-apply.yml: triggered manually, it deploys the applications defined in the state repo to the target environment. apps-pr-verify.yml: triggered when a pull request is opened, it comments on the pull request with the changes that will be applied to each environment. dispatch-image-v4.yml: (legacy) triggered by a repository dispatch event, from the source code repository, it dispatches a workflow to update the image tags in the application configuration files to a new version. dispatch-image-v5.yml: triggered manually, it dispatches a workflow to update the image tags in the application helmfiles to the latest version. The new v5 version has newer structure for the image update configuration received from the dispatch event. See the action-state-repo-update-image documentation for more information about client_payload structures. sys-services# sys-services-apply.yml: triggered when a pull request is merged, it deploys the system services defined in the state repo to the target environments. sys-services-destroy.yml: triggered manually, it removes the system services defined in the state repo from the target environment. sys-services-manual-apply.yml: triggered manually, it deploys the system services defined in the state repo to the target environment. sys-services-pr-verify.yml: triggered when a pull request is opened, it comments on the pull request with the changes that will be applied to each environment. Prerequisites# In order to use this feature, you need to have the following prerequisites\nProvider Resources# A registered app or a role, depending on the provider you are using, to establish the OIDC connection from these GitHub workflows. Also make sure that the app or role has the necessary permissions:\nAccess private chart repositories Kubernetes cluster access to deploy the applications State repo file structure# Every deployment is governed by a Helmfile, which defines the charts to be deployed and their configuration. The Helmfile is located in a specific path depending on the type of deployment: application or system service. The following is the required file structure for each type of deployment:\nApplications:\napps/ ‚îî‚îÄ‚îÄ \u0026lt;tenant\u0026gt;/ ‚îî‚îÄ‚îÄ \u0026lt;app-name\u0026gt;/ ‚îÇ \u0026lt;environment\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;environment\u0026gt;.yaml ‚îî‚îÄ‚îÄ helmfile.yaml(.gotmpl) # helmfile for the appExample:\napps/ ‚îî‚îÄ‚îÄ contoso/ ‚îî‚îÄ‚îÄ webapp/ ‚îú‚îÄ‚îÄ‚îÄ dev/ ‚îÇ ‚îú‚îÄ‚îÄ globals.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îú‚îÄ‚îÄ dev.yaml ‚îú‚îÄ‚îÄ pre/ ‚îÇ ‚îú‚îÄ‚îÄ globals.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îú‚îÄ‚îÄ pre.yaml ‚îú‚îÄ‚îÄ pro/ ‚îÇ ‚îú‚îÄ‚îÄ globals.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îî‚îÄ‚îÄ helmfile.yaml(.gotmpl) # helmfile for the appSystem Services:\nsys-services/ ‚îî‚îÄ‚îÄ \u0026lt;sys-service-name\u0026gt;/ ‚îú‚îÄ‚îÄ \u0026lt;cluster1\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ cluster.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îú‚îÄ‚îÄ \u0026lt;cluster2\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ cluster.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îî‚îÄ‚îÄ helmfile.yaml(.gotmpl) # helmfile for the sys-serviceExample:\nsys-services/ ‚îî‚îÄ‚îÄ cert-manager/ ‚îú‚îÄ‚îÄ cluster-a/ ‚îÇ ‚îú‚îÄ‚îÄ cluster.yaml ‚îÇ ‚îú‚îÄ‚îÄ secrets.yaml ‚îÇ ‚îî‚îÄ‚îÄ values.yaml ‚îî‚îÄ‚îÄ helmfile.yaml(.gotmpl) # the cert-manager sys-service installation chartConfiguration# The feature requires two configuration file, expected in the .github/ directory, one for each workflow:\n.github/apps-config.yaml. .github/sys-services-config.yaml. The configuration file has the following structure depending on the provider:\nAzure# provider: kind: azure tenant_id: 9998228a-e7c9-4ee7-ae9d-a57ba495ab64 subscription_id: 2968ccbc-92a1-49ae-bd09-14e44a85eefd helm_registries: - \u0026lt;REGISTRY\u0026gt;.azurecr.io environments: dev: cluster_name: \u0026lt;CLUSTER_NAME\u0026gt; resource_group_name: \u0026lt;RESOURCE_GROUP_NAME\u0026gt; identifier: 5b54c04e-5d7d-4d66-954b-144ebd50ac67 pre: cluster_name: \u0026lt;CLUSTER_NAME\u0026gt; resource_group_name: \u0026lt;RESOURCE_GROUP_NAME\u0026gt; identifier: 5b54c04e-5d7d-4d66-954b-144ebd50ac67 pro: cluster_name: \u0026lt;CLUSTER_NAME\u0026gt; resource_group_name: \u0026lt;RESOURCE_GROUP_NAME\u0026gt; identifier: 5b54c04e-5d7d-4d66-954b-144ebd50ac67AWS# provider: kind: aws region: us-west-2 helm_registries: - \u0026lt;AWS_ACCOUNT_ID\u0026gt;.dkr.ecr.\u0026lt;REGION\u0026gt;.amazonaws.com environments: dev: cluster_name: \u0026lt;CLUSTER_NAME\u0026gt; role-to-assume: \u0026lt;ARN\u0026gt;"},{"id":11,"href":"/docs/features/state_repo_apps/","title":"State Repo Apps","section":"Features","content":"State Repo Apps# This feature enables GitOps-based deployment workflows for application state repositories.\nAn application state repository (state repo, generally named app-\u0026lt;name\u0026gt;) is a Git repository that holds the Kubernetes deployment configurations for an application, or infrastructure, and secrets related to a specific application or set of services that compose it.\nIt provides automated CI/CD pipelines for managing Kubernetes workloads, Terraform workspaces, and secrets through GitHub Actions and ArgoCD.\nOverview# The state_repo_apps feature installs GitHub Actions workflows that manage an application state repository, enabling three types of deployments:\nKubernetes Workloads - Deploy containerized applications Terraform Workspaces - Manage infrastructure as code with TFWorkspace claims Secrets Management - Deploy and sync secrets using External Secrets Operator All deployments follow a GitOps pattern where:\nchanges are validated through pull requests to the repository default branch After merge, the deployment manifests (CRs) are rendered (hydrated) into new pull requests to the deployment branch, one for every affected deployment coordinates. ArgoCD is in charge of monitoring the deployment branch and pull the changes from the repository to the destination cluster. This feature follows the pull model, where the changes are hydrated and committed to the deployment branch and then ArgoCD detects those changes and applies them to the cluster, as opposed to the push model, where the changes are pushed to the cluster directly after being rendered.\nRepository Structure# The repository is structured with 2 head branches:\nthe default branch (usually master or main), where users upload their deployment configurations. the deployment branch, where the CRs to be deployed are hydrated by the generate-deployment-\u0026lt;type\u0026gt;.yaml. Each branch is structured as follows:\nDefault branch (main)# Upon installation, three main directories are created: kubernetes, tfworkspaces, and secrets.\nEach directory holds the dehydrated deployment files for its type and follows the expected folder structure described below:\nkubernetes folder structure: \u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;. At the same level as the environment folder, an \u0026lt;environment_name\u0026gt;.yaml file is expected, which contains the Helmfile release configuration for rendering the Helm charts. tfworkspaces folder structure: \u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;. This folder can contain multiple different, unrelated TFWorkspaceClaims. secrets: the expected structure is: \u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;. This folder can contain multiple SecretsClaims. Note that the deployment coordinates are defined by the folder structure, and must match the configuration in the .firestartr repository.\nDeployment branch (deployment)# This branch contains the hydrated deployments generated from the default branch config files. These files are Custom Kubernetes Resources (CRs) that will be deployed to the cluster and managed by its corresponding controllers.\nThe folder structure is similar to the default branch, as the same kubernetes, tfworkspaces, and secrets folders are created in both branches, with one exception: the tfworkspaces folder has no additional sub-folders, and all rendered TFWorkspaceClaim\u0026rsquo;s are placed directly inside it.\nThis branch shouldn\u0026rsquo;t be edited manually unless necessary; however, all changes should be done via PRs to the default branch.\nWorkflows Provided# Validation# Validate PR (validate-pr.yml) - Validates all pull request changes before merging Manual Deployments# Generate Kubernetes Deployment (generate-deployment-kubernetes.yml): Manually deploy Kubernetes workloads Generate TFWorkspace Deployment (generate-deployment-tfworkspaces.yml): Manually deploy Terraform workspaces Generate Secrets Deployment (generate-deployment-secrets.yml): Manually deploy secrets Automatic Deployments# Auto-generate Deployments (auto-generate-deployments.yml): Automatically create deployments pull-requests when changes are merged to the main branch, for all deployments affected by the changes. Delete Deployments# Delete Kubernetes Deployment (delete-deployment-kubernetes.yml) - Create deletion PR for Kubernetes workloads Delete TFWorkspace Deployment (delete-deployment-tfworkspaces.yml) - Create deletion PR for Terraform workspaces Delete Secrets Deployment (delete-deployment-secrets.yml) - Create deletion PR for deployed secrets Auto-Update Workflows# Dispatch Image to Kubernetes (dispatch-image-kubernetes.yml) - Auto-update Kubernetes workloads when new images are pushed Dispatch Image to TFWorkspaces (dispatch-image-tfworkspaces.yml) - Auto-update TFWorkspace images Documentation# For detailed information on each deployment type:\nKubernetes Deployments - Complete guide for deploying containerized applications TFWorkspace Deployments - Guide for managing Terraform infrastructure Secrets Management - How to deploy and manage secrets Key Features# GitOps-based deployment using ArgoCD Automated image updates with optional auto-merge Support for multiple helm chart registries (OCI, HTTPS) OIDC authentication for cloud providers (Azure, AWS) Pull request validation and preview Customizable rendering configurations Quick Start# Configure your workload - Add configuration files to kubernetes/, tfworkspaces/, or secrets/ directories, following the expected structure seen above. Create a pull request to the default branch - The PR will be validated automatically. Merge to main - Once approved, merge your changes. This will trigger the hydration workflows. Deploy manually - Run the appropriate \u0026ldquo;Generate deployment\u0026rdquo; workflow from the Actions tab to create a PR to the deployment branch with the rendered manifests. Monitor ArgoCD - ArgoCD will detect changes in the deployment branch and apply them to the cluster. Additionally, within your organization, a .firestartr repository should exist (if it doesn\u0026rsquo;t, contact platform team). This repository contains various configuration files that need to be set up for the deployment process to work correctly. You can read more about it in the Firestartr documentation.\nAuto-Update Setup# Every time a new image is pushed to a container registry, an event can be sent to this repository to trigger an automatic update of the deployments using that image.\nThis is done through the dispatch-image-\u0026lt;type\u0026gt;.yaml workflows, which listen for repository dispatch events with the appropriate payload.\nThe payload is triggered normally from the service code repository, using the Firestartr\u0026rsquo;s Build and Dispatch Docker Images feature.\nAuto-Merge Deployment PRs# This feature can be optionally combined with automatic merging of the generated deployment pull-requests, by adding an empty AUTO_MERGE file to the deployment directory (e.g., kubernetes/my-platform/my-tenant/my-environment/AUTO_MERGE), in the repository default branch.\nConfiguration# The workflows support customization through configuration files in .github/:\nhydrate_k8s_config.yaml - Config file for Kubernetes deployments. Allows configuring the helmfile image version and additional commands for the container, before executing the helmfile rendering process. hydrate_tfworkspaces_config.yaml - Config file for TFWorkspace deployments. Allows configuring the firestartr image version and additional commands for the container, before executing the rendering process. See the individual documentation files for complete configuration details and GitHub variables/secrets required.\nThe .firestartr Repository# Additionally, within your GitHub organization, a .firestartr repository should exist (if it doesn\u0026rsquo;t, contact platform team).\nThis repository, managed by the Platform team, contains various configuration guardrails files that need to be set up for the deployment process to work correctly. You can read more about it in The dot-firestartr repository, but we will provide a small overview of the relevant configurations for this feature here:\nThe platforms folder: contains the configuration for each platform inside the kubernetes and tfworkspaces deployments. Namely, it contains what tenants and environments are valid for each platform, and will be used to validate the subfolders inside the kubernetes and tfworkspaces folders. You can read the full documentation here. The validations folder: contains the configuration files used to validate the claims inside the tfworkspaces folder. Read more here. The docker_registries folder: will be checked to create the Helm repository information for the Kubernetes deployments, if not already present in the environment.yaml file. Read more here. Rollback# This feature doesn\u0026rsquo;t offer a specific workflow or action to perform rollbacks. Instead, the expected way to rollback a deployment is any of the following:\nNote that for all options the changes only affect the state repo. Further dispatches done from the code repo after a rollback will create pull-requests where the deployments will be updated as normal.\nOption 1: using GitHub to revert a PR (recommended)# Locate the deployment PR you need to rollback using GitHub\u0026rsquo;s interface, and revert it by pressing the Revert PR button. This will create a new PR reverting the deployment PR\u0026rsquo;s changes. Review this new PR and merge it as usual. This is the quickest and easiest way to rollback changes whenever available.\nOption 2: manually edit the deployment branch (not recommended)# Pull the changes from the state repo (git fetch origin), checkout to the deployment branch and manually edit the deployment files. Create a PR with your changes and merge it. This method allows for more flexibility and partial rollbacks, but is more error-prone and not recommended unless you really know what you are doing.\n"},{"id":12,"href":"/docs/features/state_repo_apps/KUBERNETES_README/","title":"Kubernetes Readme","section":"State Repo Apps","content":"‚ò∏Ô∏è How to Deploy a Kubernetes Workload# This feature makes deploying a Kubernetes workload a breeze! It allows changing the deployment configuration manually and also supports automatic updates of the services‚Äô images when new versions are built and pushed to the registry.\nThe deployment is done via GitOps, using ArgoCD. This means that the deployment is triggered when new changes arrive to the deployment branch in the state GitHub repository, which is then automatically picked up by ArgoCD and deployed to the Kubernetes cluster.\nConfiguration# To configure a new Kubernetes deployment for your application, you need to define the Helmfile configuration and Helm values files for each environment, in the repository default branch, inside the kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/ directory.\nBut, you first need to check if the deployment coordinates (platform, tenant, environment) are allowed for your application. You can check this in your .firestartr organization repository.\nIf the coordinates are not allowed, you need to add them in the .firestartr repository, following the instructions in the Firestartr documentation.\nHelmfile Configuration# The Kubernetes workloads are defined using Helm charts, and rendered using Helmfile. Also Kustomize patches and exec commands can be defined to customize the rendered manifests.\nEach Kubernetes environment must have a corresponding environment configuration file, located at kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/\u0026lt;environment_name\u0026gt;.yaml in the repository default branch, and a set of values files inside the kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/ directory.\n\u0026lt;environment\u0026gt;.yaml configuration file# This file defines a set of parameters used by the common helm-apps Helmfile Go template which is used by the hydrate-orchestrator Dagger module, to render the specified charts and render the Kubernetes workload. An example of such file is shown below:\nexample\nversion: 0.1.0 # chart version chart: prefapp/aws-web-service-umbrella # chart name releaseName: sample-app # helm release name namespace: my-namespace # kubernetes namespace hooks: [] extraPatches: #¬†Kustomize patches to apply to the rendered manifests - target: group: apps version: v1 kind: Deployment name: sample-app-micro-a patch: - op: add path: /metadata/labels/manolo value: escobar execs: # Exec commands to run after rendering the manifests - command: \u0026#34;.github/certs_to_ca_yaml.py\u0026#34; args: [ \u0026#34;--ca_certs_path\u0026#34;, \u0026#34;./kubernetes/{{.StateValues.cluster}}/{{.StateValues.tenant}}/{{$.Environment.Name}}/ca-certs\u0026#34;, \u0026#34;--ca_yml_path\u0026#34;, \u0026#34;./kubernetes/{{.StateValues.cluster}}/{{.StateValues.tenant}}/{{$.Environment.Name}}/ca.yaml\u0026#34; ]Helm values files# Inside the kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/ directory, there must be a set of Helm values YAML files. These files contain the Helm values used to configure the Helm chart for the workload.\nHelmfile will use the hydrate-orchestrator helm-apps values.yaml.gotmpl template to render the values to be used for the Helm chart installation.\nGitHub Variables and Secrets# The feature\u0026rsquo;s workflows can need the following GitHub vars, or secrets configured (at organization or repository level), to manage the access to the Helm charts registries, depending on the publication method used by the organization:\nName Mandatory Description vars.HELM_CHARTS_PUBLICATION_TYPE NO The publication method for the organization\u0026rsquo;s Helm charts, and therefore, the access method to the organization\u0026rsquo;s helm charts registries (i.e., oci, https). Default to https (public URL) vars.DOCKER_REGISTRY_RELEASES NO 1Ô∏è‚É£ The registry name, or URL, for the OCI Helm charts releases registry. It must exists in .firestartr repository default branch, inside the /docker_registries folder. 4Ô∏è‚É£ vars.DOCKER_REGISTRY_SNAPSHOTS NO 1Ô∏è‚É£ The registry name, or URL, for the OCI Helm charts snapshots registry. It must exists in .firestartr repository default branch, inside the /docker_registries folder. 4Ô∏è‚É£ vars.AZURE_CLIENT_ID NO 2Ô∏è‚É£ The Managed Identity client ID, with access permissions to the Azure ACR, needed by the auth-oci tool to configure the azure_oidc integration. vars.AZURE_TENANT_ID NO 2Ô∏è‚É£ The Tenant ID where the ACR resides, needed by the auth-oci tool to configure the azure_oidc integration. vars.AZURE_SUBSCRIPTION_ID NO 2Ô∏è‚É£ The Azure subscription ID, where the ACR resides, needed by the auth-oci tool to configure the azure_oidc integration. vars.AWS_ROLE_ARN NO 2Ô∏è‚É£ The AWS IAM Role ARN, with access permissions to the ECR, needed by the auth-oci tool to configure the aws_oidc integration. vars.AWS_DEFAULT_REGION NO 2Ô∏è‚É£ The AWS region where the ECR resides, needed by the auth-oci tool to configure the aws_oidc integration. vars.DOCKER_REGISTRY_RELEASES_USERNAME NO 3Ô∏è‚É£ The username for the Helm OCI registry for releases. vars.DOCKER_REGISTRY_SNAPSHOTS_USERNAME NO 3Ô∏è‚É£ The username for the Helm OCI registry for snapshots. secrets.DOCKER_REGISTRY_RELEASES_PASSWORD NO 3Ô∏è‚É£ The password for the Helm OCI registry for released chart versions. secrets.DOCKER_REGISTRY_SNAPSHOTS_PASSWORD NO 3Ô∏è‚É£ The password for the Helm OCI registry for non-released chart versions. 1Ô∏è‚É£ Only needed if HELM_CHARTS_PUBLICATION_TYPE is set to oci\n2Ô∏è‚É£ Only needed if the registry authentication is OIDC, i.e.:\nazure_oidc using a Managed Identity. aws_oidc using an IAM Role. 3Ô∏è‚É£ Only needed if the registry authentication is not OIDC, i.e.:\nghcr using a PAT distinct from the default actions\u0026rsquo; GITHUB_TOKEN. Else, the action will use the GITHUB_TOKEN by default. generic, i.e. user \u0026amp; password. See auth-oci tool documentation for more details.\n4Ô∏è‚É£ The docker_registries folder contains a YAML files for each registry available in the organization, with the necessary configuration.\nWorkflows# The feature provides the following GitHub Actions workflows:\nüîé Validate pull-requests (Validate PR)# This workflow validates changes in pull requests to ensure they meet the required standards.\nPermissions:\nid-token: write: Needed for OIDC authentication to the helm charts registry, if applicable. contents: read: Needed to clone the repository. pull-requests: write: Needed to comment on the related pull request. packages: read: Needed to pull the Helm charts from GitHub Container Registry, if applicable. üñêÔ∏è Manual Deployment (Generate Kubernetes Deployment)# This workflow is triggered manually, normally after merging a pull request with kubernetes/** configuration changes, and generates deployment files (CRs) for a Kubernetes workload based on a platform, tenant, and environment you specify. It updates your GitOps repo (watched by ArgoCD) on the deployment branch.\nPermissions:\nid-token: write: Needed for OIDC authentication to the helm charts registry, if applicable. contents: write: Needed to clone the repository and push the deployment artifacts branch against the deployment branch. pull-requests: write: Needed to create the pull-request against the deployment branch, and comment on it. packages: read: Needed to pull the Helm charts from GitHub Container Registry, if applicable. ü§ñ Automatic Deployment (Auto-generate Deployments)# This workflow automatically creates a deployment pull-request when changes are merged to the main branch in this repository. It scans for changes in kubernetes/** and automatically launches the deployment generation workflow if changes are detected.\nPermissions:\ncontents: write: Needed to clone the repository. actions: write: Needed to execute the other workflows in the repository. üìã How to Use The Manual Workflow# Update Values\nGo to the relate state repo‚Äôs default branch. Normally this repositories are named app-\u0026lt;application_name\u0026gt;. Edit the helm values files (e.g., in kubernetes/\u0026lt;cluster\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/values.yaml) with the desired changes. Create a pull-request, wait for the PR Verify completion ‚úÖ and merge it into the default branch. Head to Actions tab\nGo to the \u0026ldquo;Actions\u0026rdquo; tab on the state repository. Locate the Generate Kubernetes deployment Workflow\nFind Generate kubernetes deployment in the list. Launch It\nClick \u0026ldquo;Run workflow\u0026rdquo;. Fill in the deployment coordinates: platform (e.g., my-eks-cluster). tenant (e.g., customer1). environment (e.g., prod). Hit \u0026ldquo;Run workflow\u0026rdquo; to start. 1.2 üåü What You Get# Updated Repo: Deployment manifests (CRs) are created or updated and land in a pull request against the deployment branch. Summary: Check the workflow logs on GitHub for details. Deploy: Merge the pull-request, and ArgoCD will sync the changes to the Kubernetes deployment cluster. 1.3 üõ†Ô∏è Troubleshooting# Fails? Look at the logs or summary in GitHub Actions. Verify your platform, tenant, and environment inputs. No PR? Ensure the inputs match a valid Kubernetes workload path (e.g., kubernetes/my-app/customer1/prod). ü§ñ Auto-Update (Dispatch Image to Kubernetes)# This workflow dispatches an event to trigger the auto-update of a Kubernetes workload when a new image is built and pushed to the registry.\nPermissions:\nid-token: write: Needed for OIDC authentication to the helm charts registry, if applicable. contents: write: Needed to clone the repository and push the deployment artifacts branch against the deployment branch. pull-requests: write: Needed to create the pull-request against the deployment branch, and comment on it. packages: read: Needed to pull the Helm charts from GitHub Container Registry, if applicable. This workflow automatically updates image versions in your Kubernetes workloads when a new image is built and pushed to the organization registry, creating a deployment pull-request for you.\n2.1 üîÑ How It Works# Trigger: Runs when a dispatch-image-kubernetes event hits the repo (e.g., a new image is built in the service\u0026rsquo;s code repository). Process: Updates your Kubernetes workload with the new image and generates a pull-request against the deployment branch. It grabs the new image from the event. Updates the workload in kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;. Opens a pull-request with updated deployment files (CRs). Auto-Merge feature: If kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/AUTO_MERGE exists in the state repo default branch, the pull-request is auto-merged! Otherwise, it waits for your approval and merge. 2.2 üåà What You Get# PR Ready: Updated CRs in a PR against deployment. Auto or Manual: Auto-merged if AUTO_MERGE is there; otherwise, merge it yourself. Logs: See the summary in the workflow logs on GitHub. 2.3 ‚öôÔ∏è Additional Configuration# config file: A config file can be added to the repository to select the helmfile image version and additional commands to the container before the rendering process. location: .github name: hydrate_k8s_config.yaml content: # example image: ghcr.io/helmfile/helmfile:v1.1.0 commands: - [apk, add, python] 2.4 üõ†Ô∏è Troubleshooting# Fails? Check the logs. Ensure the event includes valid image data. PR Not Merging? Verify AUTO_MERGE is in kubernetes/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment/\u0026gt;. No PR? Confirm the event was triggered correctly. 3. üóëÔ∏è Delete Deployment (Delete Kubernetes Deployment)# This workflow helps you delete a Kubernetes workload by creating two PRs: one for the deployment branch, and another for the main branch. It takes the same inputs as the \u0026ldquo;Generate Kubernetes Deployment\u0026rdquo; workflow: platform, tenant, and environment.\nüéâ Quick Tips# Manual (1): Great for testing or specific deployments. Auto (2): Keeps your workloads fresh with zero effort. Merge the PR, and ArgoCD will roll it out to your cluster! "},{"id":13,"href":"/docs/features/state_repo_apps/SECRETS_README/","title":"Secrets Readme","section":"State Repo Apps","content":"üîê How to Deploy Secrets# Deploying secrets to your GitOps repo is straightforward! Here‚Äôs how to do it manually with a GitHub Actions workflow.\n1. üñêÔ∏è Manual Deployment# This workflow generates deployment files (CRs) for secrets based on a tenant and environment you provide. It updates your GitOps repo (watched by ArgoCD) on the deployment branch.\nü§ñ Automatic Deployment (Auto-generate Deployments)# This workflow automatically creates a deployment pull-request when changes are merged to the main branch in this repository. It scans for changes in kubernetes/** and automatically launches the deployment generation workflow if changes are detected.\nPermissions:\ncontents: write: Needed to clone the repository. actions: write: Needed to execute the other workflows in the repository. 1.1 üìã How to Use The Manual Workflow# Update Values\nGo to your repo‚Äôs main/master branch. Edit the \u0026ldquo;values\u0026rdquo; files (e.g., in secrets/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/secret.yaml) with the desired changes. Create a PR, wait for the PR Verify completion ‚úÖ and merge it into main/master. Example secrets claim: --- kind: SecretsClaim lifeciycle: production system: test version: 1.0 providers: external_secrets: pushSecrets: # This array generates a PushSecret per item # The push secret will create a secret into the key vault (azure) # or parameter store (aws) - secretName: my-postgres # you can set the \u0026#39;refreshInterval\u0026#39; to null in case # you don¬¥t want to refresh the secret value in the key vault # or parameter store (aws) refreshInterval: null generator: # Points to a generator custom resource, # see: https://external-secrets.io/latest/api/generator/password/ name: pg-generator externalSecrets: # Filling the key \u0026#39;externalSecrets\u0026#39;, a ExternalSecret will be created, # and the system will access to the key vault (azure) or parameter store (aws), # and create a secret into the kubernetes cluster # that can be referenced from the TFWorkspaceClaim refreshInterval: 10m secrets: - secretName: rds_conn - secretName: my_test Head to Your Repo\nGo to the \u0026ldquo;Actions\u0026rdquo; tab on GitHub. Locate the Workflow\nFind Generate secrets deployment in the list. Launch It\nClick \u0026ldquo;Run workflow\u0026rdquo;. Fill in: tenant (e.g., customer1). environment (e.g., prod). Hit \u0026ldquo;Run workflow\u0026rdquo; to start. 1.2 üåü What You Get# Updated Repo: New deployment files (CRs) for secrets land in a PR against the deployment branch. Summary: Check the workflow logs on GitHub for details. Deploy: Merge the PR, and ArgoCD will sync the secrets to your system. 1.3 üõ†Ô∏è Troubleshooting# Fails? Check the logs or summary in GitHub Actions. Verify your tenant and environment inputs. No PR? Ensure the inputs match a valid secrets path (e.g., secrets/customer1/prod). 2. üóëÔ∏è Delete Deployment (Delete Secrets Deployment)# This workflow helps you delete a secret by creating two PRs: one for the deployment branch, and another for the main branch. It takes the same inputs as the \u0026ldquo;Generate Secrets Deployment\u0026rdquo; workflow: tenant, and environment.\nüéâ Quick Tip# Use this workflow to manually deploy secrets for a specific tenant and environment. Once the PR is merged, ArgoCD handles the rest! "},{"id":14,"href":"/docs/features/state_repo_apps/TFWORKSPACES_README/","title":"Tfworkspaces Readme","section":"State Repo Apps","content":"üöÄ How to Deploy a TFWorkspace# 1. üñêÔ∏è Manual Deployment# This GitHub Actions workflow lets you update your GitOps repo (watched by ArgoCD) by turning a TFWorkspace claim into deployment files (CRs) on the deployment branch.\nü§ñ Automatic Deployment (Auto-generate Deployments)# This workflow automatically creates a deployment pull-request when changes are merged to the default branch in this repository. It scans for changes in tfworkspaces/** and automatically launches the deployment generation workflow if changes are detected.\nPermissions:\ncontents: write: Needed to clone the repository. actions: write: Needed to execute the other workflows in the repository. 1.1 üìã How to Use The Manual Workflow# Update Values\nGo to your repo‚Äôs main/master branch. Edit the \u0026ldquo;values\u0026rdquo; files (e.g., in tfworkspaces/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/claim.yaml) with the desired changes. Create a PR, wait for the PR Verify completion ‚úÖ and merge it into main/master. Example values claims: --- kind: TFWorkspaceClaim lifecycle: staging name: app-my-db system: \u0026#34;system:app\u0026#34; version: \u0026#34;1.0\u0026#34; providers: terraform: tfStateKey: b512ff18-e324-4c54-8a56-a132382081a3 name: app-my-db source: Inline module: | resource \u0026#34;aws_db_instance\u0026#34; \u0026#34;default\u0026#34; { allocated_storage = 10 db_name = vars.db_name engine = \u0026#34;mysql\u0026#34; engine_version = \u0026#34;8.0\u0026#34; instance_class = \u0026#34;db.t3.micro\u0026#34; username = vars.user password = vars.pass parameter_group_name = \u0026#34;default.mysql8.0\u0026#34; skip_final_snapshot = true } variable \u0026#34;pass\u0026#34; { type = string sensitive = true } variable \u0026#34;db_name\u0026#34; { type = string } variable \u0026#34;user\u0026#34; { type = string } values: # You can reference to a claim secret # check the secrets README.md in ./secrets/README.md pass: \u0026#34;${{ secret:app-tenant-env.rds_pass }}\u0026#34; db_name: \u0026#34;my-db\u0026#34; user: \u0026#34;root\u0026#34; Head to Your Repo\nGo to the \u0026ldquo;Actions\u0026rdquo; tab on GitHub. Locate the Workflow\nFind Generate TFWorkspace Deployment in the list. Launch It\nClick \u0026ldquo;Run workflow\u0026rdquo;. Type a claim_name (e.g., my-claim-vmss). Hit \u0026ldquo;Run workflow\u0026rdquo; to kick it off. 1.2 üåü What You Get# Updated Repo: New deployment files (CRs) land in a PR against the deployment branch. Summary: Check the workflow logs on GitHub for details. Deploy: Merge the PR, and ArgoCD will sync the changes. 1.3 üõ†Ô∏è Troubleshooting# Fails? Peek at the logs or summary in GitHub Actions. Double-check your claim_name. Stuck? Ensure you‚Äôve entered a valid claim_name. 2. ü§ñ Auto-Update# This workflow updates your TFWorkspace image versions automatically when a new image is pushed, creating a PR for you.\n2.1 üîÑ How It Works# Trigger: Kicks off with a dispatch-image-tfworkspaces event (e.g., a new image push). Process: Updates your TFWorkspace claims and generates a PR against deployment. Grabs the new image from the event. Updates the TFWorkspace runtime with the new image version. Opens a PR with updated deployment files (CRs). Auto-Merge Magic: If tfworkspaces/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;env\u0026gt;/AUTO_MERGE exists in master, the PR merges itself! Otherwise, it waits for your approval. 2.2 üåà What You Get# PR Ready: Updated CRs in a PR against deployment. Auto or Manual: Auto-merged if AUTO_MERGE is there; otherwise, merge it yourself. Logs: See the summary in the workflow logs on GitHub. 2.3 ‚öôÔ∏è Additional Configuration# config file: A config file can be added to the repository to select the firestartr image version. That config will override the image coded in the workflow. location: .github name: hydrate_tfworkspaces_config.yaml content: # example image: ghcr.io/prefapp/gitops-k8s:v1.39.2_slim 2.4 üõ†Ô∏è Troubleshooting# Fails? Check the logs. Make sure the event includes valid image data. PR Not Merging? Verify AUTO_MERGE is in tfworkspaces/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;env/\u0026gt;. No PR? Confirm the event fired correctly. 3. üóëÔ∏è Delete Deployment (Delete TFWorkspaces Deployment)# This workflow helps you delete a Terraform workspace by creating two PRs: one for the deployment branch, and another for the main branch. It takes the same inputs as the \u0026ldquo;Generate TFWorkspaces Deployment\u0026rdquo; workflow: claim_name.\nüéâ Quick Tips# Manual (1): Perfect for testing or one-off changes. Auto (2): Ideal for keeping images fresh without lifting a finger. Either way, ArgoCD handles the rest once the PR lands in deployment! "},{"id":15,"href":"/docs/features/state_repo_sys_services/","title":"State Repo Sys Services","section":"Features","content":"‚ò∏Ô∏è How to Deploy a Kubernetes Workload# This feature allows to deploy Kubernetes sys-services, i.e. operators and controllers needed in the cluster, at cluster level, and managed by the platform team.\nThe deployment is done via GitOps, using ArgoCD. This means that the deployment is done when new changes arrive to the deployment branch in the state GitHub repository, which is then automatically picked up by ArgoCD and deployed to the destination Kubernetes cluster.\nConfiguration# To configure a new Kubernetes sys-service deployment, you need to define the Helmfile configuration and Helm values files for each sys-service, in the repository default branch, inside the kubernetes-sys-services/\u0026lt;platform\u0026gt;/\u0026lt;sys-service-name\u0026gt;/ directory.\nHelmfile Configuration# Each Kubernetes sys-service must have a corresponding configuration file, located at kubernetes-sys-services/\u0026lt;platform\u0026gt;/\u0026lt;sys-service-name\u0026gt;.yaml and a set of values files inside the kubernetes-sys-services/\u0026lt;platform\u0026gt;/\u0026lt;sys-service-name\u0026gt;/ directory, in the repository default branch.\n\u0026lt;sys-service\u0026gt;.yaml file# This file defines a set of parameters used by the common helmfile.yaml.gotmpl Go template which is used by the hydrate-orchestrator Dagger module, to download the specified charts and render the Kubernetes workloads. An example of such file is shown below: https://github.com/prefapp/daggerverse/blob/main/hydrate-orchestrator/modules/hydrate-kubernetes/fixtures/values-repo-dir-sys-services/kubernetes-sys-services/cluster-name/stakater.yaml\nversion: 1.2.0 chart: stakater/reloader hooks: [] extraPatches: # [] - target: group: rbac.authorization.k8s.io kind: ClusterRoleBinding name: stakater-reloader-role-binding patch: - op: add path: /metadata/labels/test-label value: test-value execs: []Helm values files# Inside the kubernetes-sys-services/\u0026lt;platform\u0026gt;/\u0026lt;sys-service-name\u0026gt;/ directory, there must be a set of Helm values YAML files. These files contain the Helm values used to configure the Helm chart for the sys-service.\nHelmfile will use the hydrate-orchestrator helm-sys-services values.yaml.gotmpl template to render the values to be used for the Helm chart installation.\nAn example of such file is shown below: https://github.com/prefapp/daggerverse/blob/main/hydrate-orchestrator/modules/hydrate-kubernetes/fixtures/values-repo-dir-sys-services/kubernetes-sys-services/cluster-name/stakater/values.yaml\nreplicaCount: 2 image: repository: stakater/reloader tag: v0.0.96GitHub Variables and Secrets# The feature\u0026rsquo;s workflows can need the following GitHub vars, or secrets configured (at organization or repository level), to manage the access to the Helm charts registries, depending on the publication method used by the organization:\nName Mandatory Description vars.HELM_CHARTS_PUBLICATION_TYPE NO The publication method for the organization\u0026rsquo;s Helm charts, and therefore, the access method to the organization\u0026rsquo;s helm charts registries (i.e., oci, https). Default to https (public URL) vars.DOCKER_REGISTRY_RELEASES NO 1Ô∏è‚É£ The registry name, or URL, for the OCI Helm charts releases registry. It must exists in .firestartr repository default branch, inside the /docker_registries folder. 4Ô∏è‚É£ vars.DOCKER_REGISTRY_SNAPSHOTS NO 1Ô∏è‚É£ The registry name, or URL, for the OCI Helm charts snapshots registry. It must exists in .firestartr repository default branch, inside the /docker_registries folder. 4Ô∏è‚É£ vars.AZURE_CLIENT_ID NO 2Ô∏è‚É£ The Managed Identity client ID, with access permissions to the Azure ACR, needed by the auth-oci tool to configure the azure_oidc integration. vars.AZURE_TENANT_ID NO 2Ô∏è‚É£ The Tenant ID where the ACR resides, needed by the auth-oci tool to configure the azure_oidc integration. vars.AZURE_SUBSCRIPTION_ID NO 2Ô∏è‚É£ The Azure subscription ID, where the ACR resides, needed by the auth-oci tool to configure the azure_oidc integration. vars.AWS_ROLE_ARN NO 2Ô∏è‚É£ The AWS IAM Role ARN, with access permissions to the ECR, needed by the auth-oci tool to configure the aws_oidc integration. vars.AWS_DEFAULT_REGION NO 2Ô∏è‚É£ The AWS region where the ECR resides, needed by the auth-oci tool to configure the aws_oidc integration. vars.DOCKER_REGISTRY_RELEASES_USERNAME NO 3Ô∏è‚É£ The username for the Helm OCI registry for releases. vars.DOCKER_REGISTRY_SNAPSHOTS_USERNAME NO 3Ô∏è‚É£ The username for the Helm OCI registry for snapshots. secrets.DOCKER_REGISTRY_RELEASES_PASSWORD NO 3Ô∏è‚É£ The password for the Helm OCI registry for released chart versions. secrets.DOCKER_REGISTRY_SNAPSHOTS_PASSWORD NO 3Ô∏è‚É£ The password for the Helm OCI registry for non-released chart versions. 1Ô∏è‚É£ Only needed if HELM_CHARTS_PUBLICATION_TYPE is set to oci\n2Ô∏è‚É£ Only needed if the registry authentication is OIDC, i.e.:\nazure_oidc using a Managed Identity. aws_oidc using an IAM Role. 3Ô∏è‚É£ Only needed if the registry authentication is not OIDC, i.e.:\nghcr using a PAT distinct from the default actions\u0026rsquo; GITHUB_TOKEN. Else, the action will use the GITHUB_TOKEN by default. generic, i.e. user \u0026amp; password. See auth-oci tool documentation for more details.\n4Ô∏è‚É£ The docker_registries folder contains a YAML files for each registry available in the organization, with the necessary configuration.\nWorkflows# The feature provides the following GitHub Actions workflows:\nValidate PR# This workflow validates changes in pull requests to ensure they meet the required standards.\nPermissions:\nid-token: write: Needed for OIDC authentication to the helm charts registry, if applicable. contents: read: Needed to clone the repository. pull-requests: write: Needed to comment on the related pull request. packages: read: Needed to pull the Helm charts from GitHub Container Registry, if applicable. Generate deployment# This workflow generates deployment files (CRs) for a Kubernetes sys-service workload based on a platform and sys_service you specify. It updates your GitOps repo (watched by ArgoCD) on the deployment branch. Permissions:\nid-token: write: Needed for OIDC authentication to the helm charts registry, if applicable. contents: write: Needed to clone the repository and push the deployment artifacts branch against the deployment branch. pull-requests: write: Needed to create the pull-request against the deployment branch, and comment on it. packages: read: Needed to pull the Helm charts from GitHub Container Registry, if applicable. 1. üñêÔ∏è Manual Deployment# This workflow generates deployment files (CRs) for a Kubernetes workload based on the platform you specify. It updates your GitOps repo (watched by ArgoCD) on the deployment branch.\n1.1 üìã How to Use It# Update Values\nGo to the state-sys-services repo‚Äôs default branch. Edit the helm values files (e.g., in kubernetes-sys-services/\u0026lt;cluster\u0026gt;/\u0026lt;service\u0026gt;/values.yaml) with the desired changes. Create a pull-request, wait for the PR Verify completion ‚úÖ and merge it into the default branch. Head to Actions tab\nGo to the \u0026ldquo;Actions\u0026rdquo; tab on the repository. Locate the Generate Kubernetes deployment Workflow\nFind Generate deployment workflow in the list. Launch It\nClick \u0026ldquo;Run workflow\u0026rdquo;. Fill in the deployment coordinates: platform (e.g., my-eks-cluster). sys_service (e.g., datadog). Hit \u0026ldquo;Run workflow\u0026rdquo; to start. 1.2 üåü What You Get# Updated Repo: Deployment manifests (CRs) are created or updated and land in a pull request against the deployment branch. Summary: Check the workflow logs on GitHub for details. Deploy: Merge the pull-request, and ArgoCD will sync the changes to the Kubernetes deployment cluster. 1.3 üõ†Ô∏è Troubleshooting# Fails? Look at the logs or summary in GitHub Actions. Verify your platform and sys_service inputs. No PR? Ensure the inputs match a valid Kubernetes workload path (e.g., kubernetes-sys-services/my-platform/my-sys_service). ‚öôÔ∏è Additional Configuration# config file: A config file can be added to the repository to select the helmfile image version and additional commands to the container before the rendering process. location: .github name: hydrate_k8s_config.yaml content: # example image: ghcr.io/helmfile/helmfile:v1.1.0 commands: - [apk, add, python] "},{"id":16,"href":"/docs/features/tech_docs/","title":"Tech Docs","section":"Features","content":"TechDocs# Custom documentation\nGetting started# Start writing your documentation by adding more markdown (.md) files to this folder (/docs) or replace the content in this file.\nTable of Contents# The Table of Contents on the right is generated automatically based on the hierarchy of headings. Only use one H1 (# in Markdown) per file.\nSite navigation# For new pages to appear in the left hand navigation you need edit the mkdocs.yml file in root of your repo. The navigation can also link out to other sites.\nAlternatively, if there is no nav section in mkdocs.yml, a navigation section will be created for you. However, you will not be able to use alternate titles for pages, or include links to other sites.\nNote that MkDocs uses mkdocs.yml, not mkdocs.yaml, although both appear to work. See also https://www.mkdocs.org/user-guide/configuration/.\n"},{"id":17,"href":"/docs/features/terraform-infra/","title":"Terraform Infra","section":"Features","content":"Terraform Infra workflows# This feature contains GitHub Actions workflows and supporting scripts to automate the planning and application of Terraform infrastructure changes across multiple tenants, environments, and modules.\nOverview# The automation is designed to:\nValidate changes to Terraform modules on pull requests. Run terraform plan for changed modules on pull requests. Apply Terraform changes automatically when a pull request is merged and labeled with terraform/apply, or manually via workflow dispatch. Provide detailed feedback and logs as pull request comments. Repository Structure# The repository is structured to support multiple accounts and environments, with Terraform modules organized by account and environment. Here‚Äôs a simplified view of the structure:\n. ‚îú‚îÄ‚îÄ accounts ‚îÇ¬†‚îú‚îÄ‚îÄ account1 ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ pro ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 010-vpc ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 020-eks ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 030-iam ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ 040-lambda ‚îÇ¬†‚îú‚îÄ‚îÄ account2 ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ dev ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 10-vpc ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ 20-eks ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ pre ‚îÇ¬†‚îî‚îÄ‚îÄ test-account ‚îÇ¬†‚îî‚îÄ‚îÄ dev ‚îÇ¬†‚îú‚îÄ‚îÄ 00-state1 ‚îÇ¬†‚îî‚îÄ‚îÄ 01-state2Workflow Summary# 1. Pull Request Workflow# When a pull request is opened or updated:\nValidation: The workflow checks which modules are affected by the changes. Plan: For each affected module, terraform plan is executed and the results are posted as a comment on the pull request. When a pull request is merged and closed with the terraform/apply label:\nApply: The workflow runs terraform apply for the affected modules and posts the results as a comment. 2. Manual Trigger (Workflow Dispatch)# You can manually trigger the workflow from the GitHub Actions UI:\nPlan or Apply: By setting the run_plan or run_apply input to true, you can run terraform plan or terraform apply for specified modules. Key Workflow Conditions# Terraform Apply runs when:\nThe workflow is manually triggered with run_apply: true, or A pull request is merged, closed, and labeled with terraform/apply. Terraform Plan runs when:\nThe workflow is manually triggered with run_plan: true, or A pull request is open or updated (not closed). Supporting Scripts# .github/scripts/functions.sh# Contains helper functions for running Terraform commands and posting results as PR comments. run_terrafire_command: Executes a Terraform command for a specific module, captures output, cleans logs, and posts results to the pull request if running in GitHub Actions. populate_github_vars_file: Loads and exports variables from a file in GitHub Actions format, supporting both simple and multi-line values. .github/scripts/run_terrafire.sh# Invoked by the workflow to run terraform plan or terraform apply for the specified modules, using the logic defined in functions.sh. Usage# Automatic# Open a pull request to trigger validation and planning. Merge a pull request with the terraform/apply label to trigger an apply. Manual# Go to the Actions tab in GitHub. Select the workflow and click \u0026ldquo;Run workflow\u0026rdquo;. Set the desired inputs (run_plan or run_apply, tenant, modules, etc.). Notes \u0026amp; Troubleshooting# Labels and PR Merges: The workflow checks for the terraform/apply label at the time the pull request is closed and merged. If the label is added too late (right before merging), GitHub\u0026rsquo;s event payload may not include it, and the apply step may not trigger. To ensure reliable automation, add the label before merging. Environment Variables: The workflows rely on several environment variables for backend configuration (see your workflow YAML for details). Logs and Feedback: All Terraform output is posted as a PR comment and grouped in the Actions logs for easy review. For more details, see the workflow file at .github/workflows/terraform-plan-apply.yaml and the scripts in .github/scripts/.\n"},{"id":18,"href":"/docs/features/terraform-infra/ACCOUNTS_README/","title":"Accounts Readme","section":"Terraform Infra","content":"Terrafire script usage guide# To bootstrap the required resources in the AWS account, we will use terraform, through a configuration script (terrafire.sh).\nPrerequisites# AWS CLI: Make sure you have the AWS CLI installed and configured with the necessary permissions to create resources in your AWS account. Minimum version required is 2.27.0.\nTerraform: Ensure you have Terraform installed on your machine. You can download it from the Terraform website.\naws cli must be configured with the following profiles:\nbeginners: For testing purposes firestartr-pro: For production Configuration# [profile profile1] ; For testing purposes sso_session = my-sso sso_account_id = 1234567890 sso_role_name = AdministratorAccess region = eu-west-1 output = json [profile profile2] ; Production account sso_session = my-sso sso_account_id = 0987654321 sso_role_name = AdministratorAccess region = eu-west-1 output = json [sso-session my-sso] sso_region = eu-west-1 sso_start_url = https://my-sso.awsapps.com/start sso_registration_scopes = sso:account:accessOverview# The terrafire.sh script wraps calls to the terraform command for a specific Terraform backend. Backends are structured as follows:\naccount: folders inside the project accounts directory. Each one represents an AWS account. environment: subfolders within each account, representing environments (development, production, staging, etc.). module: subfolders within each environment, representing distinct configuration units (e.g., VPC, EKS, Route53 domain). Thus, each module is referenced as account/environment/module The script is intended to run Terraform within module directories.\nTerraform commands such as plan, apply, and destroy support execution across multiple modules. In these cases, the script accepts a space-separated list of modules. For other commands (import, state, force-unlock), only a single module is supported.\nFor the specific case of multiple modules usage, if no module is provided, the script will try to automatically find all the modules within the account/environment folder and execute for all of them.\nPassing optional arguments to the script# As terrafire.sh wraps the usage of the terraform command, we need a way to pass optional parameters to the terraform commands. This is performed with the -- string.\n# For testing purposes ./terrafire.sh COMMAND ACCOUNT ENVIRONMENT [MODULE(s)] [SUBCOMMAND] -- [PARAMETERS] # Example ./terrafire.sh -f firestartr-pro dev vpc 12345abcd-6789-ef01-2345-6789abcdef00 -- -forceplan# To see the terraform plan of a micro-state, run the following command:\n# For testing purposes ./terrafire.sh -p ACCOUNT ENVIRONMENT [MODULE(S)] # For example, the state of the VPC module in production (\u0026#39;pro\u0026#39;) for \u0026#39;client-a\u0026#39;... ./terrafire.sh -p client-a pro vpc # And to test the VPC and the EKS modules in dev environment for \u0026#39;client-b\u0026#39;... ./terrafire.sh -p client-b dev vpc eks # And to test all modules in the predev environment for \u0026#39;client-c\u0026#39;... ./terrafire.sh -p client-c predevapply# To apply terraform of a micro-state, run the following command:\n# For testing purposes ./terrafire.sh -a ACCOUNT ENVIRONMENT [MODULE(s)] # For example, creating the VPC in production (pro) for \u0026#39;client-a\u0026#39;... ./terrafire.sh -a client-a pro vpc # And to create the VPC and the EKS in development (dev) for \u0026#39;client-b\u0026#39;... ./terrafire.sh -a client-b dev vpc eks # And to apply all microstates in the \u0026#39;testing\u0026#39; environment for \u0026#39;client-c\u0026#39;... ./terrafire.sh -a client-c testingdestroy# To destroy a terraform configuration of a micro-state, run the following command:\n# For testing purposes ./terrafire.sh -d ACCOUNT ENVIRONMENT [MODULE(s)] # For example, deleting the VPC in production (pro) for \u0026#39;client-a\u0026#39;... ./terrafire.sh -d client-a pro vpc # And to delete the VPC and the EKS in \u0026#39;dev\u0026#39; environment for \u0026#39;client-b\u0026#39;... ./terrafire.sh -d client-b dev vpc eks # And to delete all microstates in \u0026#39;test\u0026#39; environment for \u0026#39;client-c\u0026#39;... ./terrafire.sh -d client-c testforce-unlock# To forcibly unlock a terraform state. It needs the ID of the Terraform lock, and needs and accepts only one module.\n# For testing purposes ./terrafire.sh -f ACCOUNT ENVIRONMENT MODULE LOCK_ID # For example, unlocking the state 12345abcd-6789-ef01-2345-6789abcdef00 for the VPC in production for \u0026#39;client-a\u0026#39; ./terrafire.sh -f client-a pro vpc 12345abcd-6789-ef01-2345-6789abcdef00state# Allows commands that affect directly the Terraform state. Needs and accepts only one module.\n# For testing purposes ./terrafire.sh -s ACCOUNT ENVIRONMENT MODULE SUBCOMMAND [ -- SUBCOMMAND_OPTIONS] # For example, show details for the VPC in production for \u0026#39;client-a\u0026#39; ./terrafire.sh -s client-a pro vpc show -- \u0026#39;aws_subnet.public\u0026#39; # And for removing resources for the state in the EKS in dev environment for \u0026#39;client-b\u0026#39;... ./terrafire.sh -s client-b dev eks rm -- \u0026#39;aws_instance.obsolete\u0026#39;import# Imports existent infrastructure into Terraform control. Needs and accepts only one module.\nFor this purpose, it is imperative to create a Terraform file, e.g., main.tf, and define there the resources we want to import, just like with a standard Terraform import.\n# For testing purposes ./terrafire.sh -i ACCOUNT ENVIRONMENT MODULE RESOURCE_ADDRESS RESOURCE_ID # For example, importing an VPC in the production environment into the vpc module for \u0026#39;client-a\u0026#39; ./terrafire.sh -i client-a pro vpc aws_subnet.public subnet-1234567890abcdef0 # And to import the EKS in dev environment for \u0026#39;client-b\u0026#39;... ./terrafire.sh -i client-b dev eks aws_eks_cluster.main eks-0fedcba0987654321Launch script usage guide# This document outlines the usage of the launch.sh script for managing Terraform deployments.\n1. Synopsis# The launch.sh script is designed to execute Terraform commands (plan, apply, destroy, force-unlock) across different tenants, environments, and modules.\n2. Requirements# Terraform executable installed and configured. A directory structure as expected by the script (see below). 3. Directory Structure# The script expects a specific directory layout:\naccounts/ ‚îú‚îÄ‚îÄ \u0026lt;tenant_name\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;environment_name\u0026gt;/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;00-module_name1\u0026gt;/ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ... (Terraform files, terraform.tfvars) ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ \u0026lt;01-module_name2\u0026gt;/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ providers.tf ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îÇ ‚îú‚îÄ‚îÄ account.tfvars (optional) ‚îÇ ‚îú‚îÄ‚îÄ providers.tf ‚îÇ ‚îú‚îÄ‚îÄ variables.tf ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ globals.tfvars (optional) ‚îú‚îÄ‚îÄ backend.tf ‚îú‚îÄ‚îÄ launch.sh ‚îú‚îÄ‚îÄ README.md \u0026lt;SCRIPT_DIR\u0026gt;: The directory where launch.sh is located.\n\u0026lt;tenant_name\u0026gt;: A directory representing a tenant.\n\u0026lt;environment_name\u0026gt;: A subdirectory within a tenant, representing an environment (e.g., dev, pro). This folder needs the files variables.tf and providers.tf in order for the system to function properly. The providers.tf file cannot be placed directly in its final location because its path depends on the tenant and environment names. In the next section we will explain how this file should be generated.\n\u0026lt;module_name\u0026gt;: A subdirectory within an environment, containing the Terraform configuration for a specific module (e.g., vpc, eks).\n3.1 Providers file# As previously discussed, we cannot provide the providers.tf as it is dependent on both the tenant name and the environment name, so we can have different providers for each environment. Each environment needs its own providers.tf file.\nIn the following section, we provide a sample providers.tf file that can be used. You can prepare it and place it inside the relevant environment folder\nFor example, if you have a \u0026ldquo;my_name\u0026rdquo; tenant and \u0026ldquo;dev\u0026rdquo; environment, the path of this file should be accounts/my_name/dev/providers.tf.\nTo prepare the file, you can change the AWS region, and (this is the important part) the AWS account ID for the tenant you are adding this file to.\nYou can add whichever configurations to this provider file, such as aliases.\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-west-1\u0026#34; # change this as needed dynamic \u0026#34;assume_role_with_web_identity\u0026#34; { for_each = var.is_cicd ? [1] : [] content { role_arn = \u0026#34;arn:aws:iam::\u0026lt;AWS_ACCOUNT_ID\u0026gt;:role/tf-base-role\u0026#34; # add here the tenant AWS account ID web_identity_token_file = \u0026#34;/tmp/web_identity_token_file\u0026#34; } } dynamic \u0026#34;assume_role\u0026#34; { for_each = !var.is_cicd ? [1] : [] content { role_arn = \u0026#34;arn:aws:iam::\u0026lt;AWS_ACCOUNT_ID\u0026gt;:role/tf-base-role\u0026#34; # add here the tenant AWS account ID } } assume_role { role_arn = \u0026#34;arn:aws:iam::\u0026lt;AWS_ACCOUNT_ID\u0026gt;:role/AccountAdminRole\u0026#34; # add here the tenant AWS account ID } }Also, we have provided a sample examples/env/providers.tf file that you can use to add to your current configuration in the right place, in your own tenant/environment folders. This file is only a template, and can be modified to suit your needs.\n4. Usage# The script is invoked with the following command structure:\n./terrafire.sh OPTION TENANT ENVIRONMENT [MODULE] [-- EXTRA_ARGS] Arguments: - OPTION: (Required) The Terraform action to perform. -a: Run terraform apply on the specified modules. If no module is specified, it applies to all modules in the environment. -p: Run terraform plan on the specified modules. If no module is specified, it plans for all modules in the environment. -d: Run terraform destroy on the specified modules. If no module is specified, it destroys all modules in the environment. -f: Run terraform force-unlock for the state. This option requires a LOCK_ID to be passed as an extra argument (the first argument after --). TENANT: (Required) The name of the tenant. This must correspond to a directory at the same level as the script. Example: my-tenant ENVIRONMENT: (Required) The name of the environment. This must correspond to a subdirectory within the specified TENANT folder. Example: dev, staging, pro MODULE...: (Optional) The name(s) of specific module(s) to target. These must correspond to subdirectories within the specified TENANT/ENVIRONMENT folder. If omitted, the action applies to all modules within the environment. You can specify multiple modules separated by spaces. Example: vpc, eks database -- EXTRA_ARGS: (Optional) Any additional arguments to be passed directly to the terraform command (e.g., -var=\u0026#34;foo=bar\u0026#34;, -target=resource.id). These must be preceded by --. If using -f (force-unlock), the LOCK_ID is the first argument here.Displaying Available Tenants and Modules: If an invalid tenant or no tenant is provided, the script will list available tenants. If an invalid option or environment is provided (after a valid tenant), the script will attempt to list available modules for that tenant and environment.\n5. Examples# Plan all modules in the dev environment for tenant-alpha:\nApply the vpc module in the pro environment for tenant-beta:\nDestroy the eks and network modules in the staging environment for tenant-gamma:\nForce-unlock a state lock (LOCK_ID must be known):\n(Note: The module vpc is specified here as the force-unlock command in the script is associated with a module\u0026rsquo;s directory context, even if the lock ID itself might be global or apply to a specific state file named after the module.)\nPlan the webserver module in dev for tenant-delta and pass an extra variable to Terraform:\n6. Terraform Backend and Variables# Backend Configuration: The script expects a backend.tf file in \u0026lt;SCRIPT_DIR\u0026gt; for backend configuration. It dynamically sets the key for the S3 backend based on tenant, environment, and module. Provider Configuration: A providers.tf file can be placed in \u0026lt;SCRIPT_DIR\u0026gt;/// to be copied into the working directory for the Terraform operations. Variable Files (.tfvars): The script looks for and uses variable files in the following order of precedence (later files override earlier ones): \u0026lt;SCRIPT_DIR\u0026gt;/globals.tfvars \u0026lt;SCRIPT_DIR\u0026gt;//account.tfvars \u0026lt;SCRIPT_DIR\u0026gt;///environment.tfvars \u0026lt;SCRIPT_DIR\u0026gt;////terraform.tfvars\n7. Environment Variables for Backend# The script relies on the following environment variables being set for Terraform backend operations (typically sourced from /tmp/.firestartr-env or set manually):\nFIRESTARTR_BACKEND: S3 bucket name. FIRESTARTR_LOCK: DynamoDB table name for state locking. FIRESTARTR_BACKEND_REGION: AWS region for the backend resources. FIRESTARTR_BACKEND_ROLE_ARN: IAM role ARN for accessing backend resources. FIRESTARTR_BACKEND_PROFILE: (Assumed, based on script structure) AWS CLI profile to use for backend operations.\n8. TF state prefix# By default, the Terraform state in Amazon S3 uses the following key structure: firestartr/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/\u0026lt;module\u0026gt;\nIf you use multiple GitHub repositories with the terraform-infra feature installed, this structure may cause collisions if the \u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/\u0026lt;module\u0026gt; path is repeated across repositories.\nTo avoid this, you can define a custom prefix by creating a .firestartr_tfstate_prefix file. The value in this file will be inserted between firestartr/ and \u0026lt;tenant\u0026gt;, resulting in the following structure: firestartr/\u0026lt;custom_value\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;environment\u0026gt;/\u0026lt;module\u0026gt;\nWe recommend setting this custom value to your GitHub repository name, but you can use any valid string for Amazon S3 object keys.\n9. Error Handling# The script uses set -euo pipefail and a trap to exit immediately if any command fails or an error occurs. Error messages are printed to standard error.\n"},{"id":19,"href":"/docs/Migrating-to-our-new-app-state-repo-structure/","title":"Migrating to Our New App State Repo Structure","section":"Firestartr Documentation","content":"Introduction# Lately, we have the need to migrate old state repos to our new app state repo structure. This article will describe the steps needed to be followed to properly do this migration. Note that the following steps should be done in a dev or pre environment first, and once everything is working then applied to the pro environment (during an intervention window which must be request beforehand).\nStep 0: üõ†Ô∏è prerrequisites# Go to the charts repo of the organization and locate the deployment.yaml file (or equivalent, such as statefulset.yaml, daemonset.yaml, etc.) for the application of the repo you are updating. Update the following line in the annotations of the deployment (not the pod):\nannotations: {{ .Values.global.annotations | toYaml | nindent 2 }}To:\nannotations: {{- if .Values.global.annotations }} {{ .Values.global.annotations | toYaml | nindent 2 }} {{- end }} firestartr.dev/image: {{ .Values.image }} firestartr.dev/microservice: {{ .Chart.Name }}And then update the chart\u0026rsquo;s version number. Create a PR with these changes and once it\u0026rsquo;s merged the new chart will be published.\nStep 1: üìÇ creating the repo# If not already done, create the new state repo claim and install the latest version state_repo_apps feature (v2 at the time of writing). The recommended name nomenclature for state repos we are currently using is app-\u0026lt;application-name\u0026gt;.\nOnce the repo has been created, you will also need to create the folder structure (see state apps main/master branch) and copy the corresponding environment folder and YAML file for the cluster/tenant pair that is going to be migrated. You can copy all the environments at once as long as you don\u0026rsquo;t change the state repo the make_dispatches.yaml config dispatches to (see Leaving pro dispatching to the old state repo and make_dispatches config), but it\u0026rsquo;s recommended to migrate environments one by one whenever possible.\nOnce the repo has been created, give the Argo notifications GitHub app the necessary permissions to make it work over the new repo.\nTo be able to do pull of the charts, if they are private, add in Settings \u0026gt; Secrets and variables \u0026gt; Actions \u0026gt; Variables \u0026gt; New repository variable \u0026gt; DOCKER_REGISTRY_RELEASES=registryName\nStep 2: üîÑ updating the chart and it\u0026rsquo;s version# In the new state repo, the copied \u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;env\u0026gt;/\u0026lt;env\u0026gt;.yaml files are incomplete and need updating. Update them:\nnamespace: chart-namespace version: \u0026lt;old-version\u0026gt;To:\nnamespace: chart-namespace version: \u0026lt;new-version\u0026gt; chart: \u0026lt;chart-name-in-repo\u0026gt; hooks: [] extraPatches: [] remoteArtifacts: [] execs: [] set: - name: \u0026#34;global.chart_version\u0026#34; value: \u0026#34;{{ .StateValues.version }}\u0026#34;If the namespace also needs to be updated, it can be done now too. Note that doing so will require additional steps to be done, as described here\nStep 3: üõ†Ô∏èupdating the .firestartr configuration# Some files may be missing from the .firestartr configuration repository (usually the app configuration file) so create them as needed. See the .firestartr section to learn more about .firestartr, its folder structure and the configurations within.\nStep 4: üîß updating make_dispatches in the code repo# In the claims repository, go to the code repo claim and update (or install, if not already done) the build_and_dispatch_docker_images feature to the latest version available (v5 at the time of writing).\nOnce the installation has been completed, .github/make_dispatches.yaml needs to be updated to the new format (or created, if it\u0026rsquo;s a new installation). Here\u0026rsquo;s an example:\nOld format:\ndispatches: - type: snapshots flavors: - flavor-1 state_repos: - repo: state-repo dispatch_event_type: \u0026#34;dispatch-image-v5\u0026#34; base_path: apps tenant: councilbox application: app-1 env: dev service_names: [\u0026#39;service-1\u0026#39;] version: $branch_dev registry: registry.overwrite image_repository: img_repo/overwriteNew format from v4:\ndeployments: # \u0026lt;- Notice how \u0026#34;dispatches\u0026#34; was changed to \u0026#34;deployments\u0026#34; # Ensure the platform matches the one specified in the platforms directory in the .firestartr configuration repository. # name: cluster-1 in .firestartr/platforms/cluster-1.yaml file # Ensure the platform matches with the path segment corresponds to the cluster name in the application\u0026#39;s state repository. # app-\u0026lt;application\u0026gt;/kubernetes/cluster-1 - platform: cluster-1 # Ensure the tenant matches the one specified in the platforms directory in the .firestartr configuration repository. # tenants: [tenant-1, tenant-2] in .firestartr/platforms/cluster-1.yaml file # Ensure the tenant matches with the path segment corresponds to the tenant name in the application\u0026#39;s state repository. # app-\u0026lt;application\u0026gt;/kubernetes/cluster-1/tenant-1 tenant: tenant-1 # Ensure the application matches the one specified in the apps directory in the .firestartr configuration repository. # name: app-1 in .firestartr/apps/app-1.yaml file application: app-1 # Ensure the env matches the one specified in the platforms directory in the .firestartr configuration repository. # envs: [dev, pre] in .firestartr/platforms/cluster-1.yaml file # Ensure the env matches with the path segment corresponds to the environment name in the application\u0026#39;s state repository. # app-\u0026lt;application\u0026gt;/kubernetes/cluster-1/tenant-1/dev env: dev # Ensure the service matches the one specified in the apps directory in the .firestartr configuration repository. # services: # - repo: org/service-1 # service_names: [service-1] # Ensure the service matches with the top-level key corresponds to the service name in the application\u0026#39;s state repository. # app-\u0026lt;application\u0026gt;/kubernetes/cluster-1/tenant-1/dev/serive-1-values.yaml file service_names: [\u0026#39;service-1\u0026#39;] type: snapshots # Support snapshots and releases flavor: flavor-1 # Set on build_images.yaml file on the same folder version: $branch_dev # Support $branch_\u0026lt;branch_name\u0026gt;, $latest_prelease and $latest_release registry: registry.overwrite # Optional, only use if it was in the original config (old). For the rest of the cases it can be set but it is appropriate to take it from the organization\u0026#39;s action variable (or by overwriting the repository\u0026#39;s action variable) in the service repository\u0026#39;s setting area. # Ensure the image_repository matches the repo key in the apps directory in the .firestartr configuration repository. # services: # - repo: org/service-1 # service_names: [service-1] image_repository: org/service-1 # Optional, only use if it was in the original config. For the rest of the cases it can be configured but it is appropriate to let it take it from what was explained before. dispatch_event_type: \u0026#34;dispatch-image-v5\u0026#34; # Optional, only use if it was in the original config (old) # Ensure the state_repo matches the one specified in the apps directory in the .firestartr configuration repository. # state_repo: \u0026#34;org/app-\u0026lt;application\u0026gt;\u0026#34; in .firestartr/apps/app-1.yaml file state_repo: state-repo-1 # Optional, only use if it was in the original config (old). For the rest of the cases it can be set but it is appropriate to take it from the organization\u0026#39;s action variable (or by overwriting the repository\u0026#39;s action variable) in the service repository\u0026#39;s setting area.To learn more about the new config format and its parameters, read make_dispatches config\nIn the case of updating a make_dispatches.yaml file which contains one or more working pro environment configurations, see this section\nNOTES:\nMake sure the build_and_dispatch_docker_images feature is installed in the code repository from the corresponding component claim and at least version 5.0.1. It should contain at least these arguments in providers.github.features.build_and_dispatch_docker_images.args: build_snapshots_branch: '\u0026lt;branch\u0026gt;' # The branch that will trigger the dispatch of the snapshots. build_snapshots_filter: '' build_pre_releases_filter: '' build_releases_filter: '' default_snapshots_flavors_filter: '*' default_pre_releases_flavors_filter: '*' default_releases_flavors_filter: '*' firestartr_config_repo: 'org/.firestartr' # Where org is the organization of the firestartr configuration repository. If the branch set in build_snapshots_branch is not the default branch of the service code repository, once the build_and_dispatch_docker_images feature has been updated and the make_dispatches.yaml file has been migrated to the new model, bring the content of the main branch (where everything described above has been applied) to the build_snapshots_branch so that the new dispatch model is available. Take advantage of the changes promoted in the code repository claim to update and add the necessary permissions, in effect, those of platformOwner that concern us and that you can find in the files of the groups directory of the same claims repository. Eventually like this: platformOwner: group:infra. Step 5: üõ°Ô∏è create the Argo project and application set files# Over at the state-argocd repository, create a folder for the new app and add these two files to it:\n# apps/\u0026lt;application-name\u0026gt;/argo-\u0026lt;application-name\u0026gt;.ApplicationSet.yaml --- apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: app-\u0026lt;application-name\u0026gt; namespace: argocd spec: generators: - git: directories: - path: \u0026lt;technology.type\u0026gt;/*/*/* # technology.type -\u0026gt; kubernetes, vmss, etc. as defined by the cluster configuration in the .firestartr repo repoURL: https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git revision: deployment values: \u0026lt;cluster1-name\u0026gt;: \u0026lt;cluster1-url\u0026gt; \u0026lt;cluster2-name\u0026gt;: \u0026lt;cluster2-url\u0026gt; \u0026lt;cluster3-name\u0026gt;: \u0026lt;cluster3-url\u0026gt; ... goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: \u0026#39;app-\u0026lt;application-name\u0026gt;-{{index .path.segments 1}}-{{index .path.segments 2}}-{{index .path.segments 3}}\u0026#39; spec: destination: namespace: \u0026#39;{{index .path.segments 2}}-\u0026lt;application-name\u0026gt;-{{index .path.segments 3}}\u0026#39; server: \u0026#39;{{index .values (index .path.segments 1)}}\u0026#39; project: \u0026#39;app-\u0026lt;application-name\u0026gt;\u0026#39; source: path: \u0026#39;{{index .path.segments 0}}/{{index .path.segments 1}}/{{index .path.segments 2}}/{{index .path.segments 3}}\u0026#39; repoURL: https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git targetRevision: deployment syncPolicy: automated: null syncOptions: - CreateNamespace=true# apps/\u0026lt;application-name\u0026gt;/argo-\u0026lt;application-name\u0026gt;.Project.yaml --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: app-\u0026lt;application-name\u0026gt; namespace: argocd spec: description: \u0026lt;application-name\u0026gt; State Project # Obviously any description is valid clusterResourceWhitelist: - group: rbac.authorization.k8s.io kind: ClusterRole - group: rbac.authorization.k8s.io kind: ClusterRoleBinding - group: \u0026#34;\u0026#34; kind: Namespace - group: networking.k8s.io kind: IngressClass sourceRepos: - \u0026#34;https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git\u0026#34; destinations: - namespace: \u0026lt;tenant1-env1-application-namespace\u0026gt; server: \u0026lt;tenant1-env1-cluster-url\u0026gt; - namespace: \u0026lt;tenant2-env1-application-namespace\u0026gt; # Add only the namespaces and clusters that you plan to configure and deploy server: \u0026lt;tenant2-env1-cluster-url\u0026gt; # - namespace: \u0026#34;\u0026lt;tenant1-env2-application-namespace\u0026gt;\u0026#34; # \u0026lt;- You can add not yet configured namespaces and clusters as comments, and uncomment them with necessary # server: \u0026lt;tenant1-env2-cluster-url\u0026gt;In this example, the branch used is deployment, by default this orphan branch will be created in the app- state repo to host the templated artifacts.\nStep 6: üóëÔ∏è uninstall previous helm release# Before creating your first deployment, go ahead and uninstall the old release like this:\nhelm --namespace \u0026lt;namespace\u0026gt; uninstall \u0026lt;release\u0026gt; (use the old namespace if it was updated in the second step)\nNotes:\nIt\u0026rsquo;s important to check that when deleting a release, all the artifacts associated with it have been deleted, particularly those that provision or attach resources from a provider (LoadBalancer Services, PersistentVolumes, etc) Artifacts applied via hooks do not belong to the release and therefore must be removed manually. Additionally, it would be advisable to block the deployment of a new release from the push repository, i.e., before deleting the old state, it should be blocked, for example by deleting the environment in the helmfile. Step 7: üöÄ render the first deployments# Once all the previous steps have been completed, you can do a render of the relevant environments either directly in the state repo or by doing a dispatch from the code repo. A PR will be created for each environment. Review them and merge the changes if no errors are found (it\u0026rsquo;s recommended to go one by one instead of merging them all at the same time).\nStep 8: ‚úÖ check everything is OK# Go to the Argo control panel and confirm the Application Set has been correctly created (go to the Applications section in the left hand menu). To connect to ArgoCD, open k9s, port-forward argocd-server (Shift + F to port-forward) and log into localhost:\u0026lt;port-forwarded-port\u0026gt; with admin and the decoded argo-initial-admin-secret secret (X to decode) or use de kubectl command:\nTo establish a port-forward connection to the ArgoCD server:# kubectl port-forward svc/argocd-server --namespace argocd \u0026lt;local-forwarded-port\u0026gt;:80To decode the argo-initial-admin-secret secret:# kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 --decodeThen, go to the Applications section in the left hand menu and check the application has been created. If it hasn\u0026rsquo;t, check the logs of the argocd-application-set-controller pod. You may need to manually synchronize it if its status is reported as missing. Then check the pods have also been created inside the Kubernetes cluster.\nCheck any connections that might have changed between pods, if needed.\nNote: Some things might not be right. The ArgoCD deployment may give you temporary synchronization problems, or there may be connection issues between pods. Please check everything thoroughly, specially if it\u0026rsquo;s the first environment you are migrating for this application. Take note of any additional steps this guide may have missed and do them on subsequent app environment migrations.\nStep 9: cleaning up old files# The last thing needed to be done is the deletion of obsolete configuration files:\nIn the new state repo, create a PR deleting all of the \u0026lt;env\u0026gt;/images.yaml files of the environments that where correctly deployed, and merge the changes. In the old state repos, delete the \u0026lt;application\u0026gt;/\u0026lt;env\u0026gt; folder of each environment that was correctly deployed. If, for a given application, all of its environments are already deployed, delete the whole \u0026lt;application\u0026gt; folder instead üìù Leaving pro dispatching to the old state repo# In order to update an existing make_dispatches.yaml file\u0026rsquo;s dev and pre environments so they dispatch to the new state repo while leaving pro to dispatch to the old environment, additional steps need to be taken.\nFirst, make_dispatches@v3 state repo\u0026rsquo;s use a different process to dispatch, and create the path to update as follows: \u0026lt;platform.type\u0026gt;/\u0026lt;platform\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;env\u0026gt;. This has changed substantially from the previous version, which dispatched using \u0026lt;base_path\u0026gt;/\u0026lt;tenant\u0026gt;/\u0026lt;application\u0026gt;/\u0026lt;env\u0026gt; as its path. However, the payload make_dispatches sends to either repo is the same, and the only change that has happened between versions is that new fields have been added, which will be ignored by previous versions of the state repo workflow. Internally, the base_path keeps being sent in the payload, but it\u0026rsquo;s composed as \u0026lt;platform.type\u0026gt;/\u0026lt;platform\u0026gt; instead of using the value of the config file when found. This means we can keep backwards compatibility with a little bit of creativity:\nOver in the .firestartr repository, create a new platform configuration file for each base_path value you need to support, as follows: # platforms/legacy.yaml (or platforms/legacy-\u0026lt;base_path\u0026gt;.yaml when multiple \u0026lt;base_path\u0026gt; values need to be supported) type: \u0026#39;\u0026#39; # Leave this parameter empty name: \u0026lt;base_path\u0026gt; # Set the old base_path value tenants: [tenant1, tenant2, tenant3, ...] # Add whichever tenants need to be supported by the legacy repo envs: [dev, pre, pro] # Add whichever environments need to be supported by the legacy repoSince platform.type has no value, the resulting base_path for any dispatch using this platform configuration will be just whatever platform.name we specified, giving us the old behavior while keeping the new configuration style.\nIn the make_dispatches.yaml config of the code repo, update all the environments you want to dispatch to the old state repo as follows: Old version:\n- type: releases flavors: - default state_repos: - repo: \u0026lt;old-state-repo\u0026gt; base_path: \u0026lt;base_path\u0026gt; tenant: \u0026lt;tenant\u0026gt; dispatch_event_type: \u0026#34;dispatch-image-v5\u0026#34; application: \u0026lt;app\u0026gt; env: pro service_names: [\u0026#39;service\u0026#39;] version: $latest_releaseNew version from v4:\n- tenant: \u0026lt;tenant\u0026gt; platform: \u0026lt;base_path\u0026gt; # As discussed in the previous step, create a platform with the name \u0026lt;base_path\u0026gt; and set it as the new platform type: releases flavor: default # Create an additional deployment if the previous config had more than one flavor dispatch_event_type: \u0026#34;dispatch-image-v5\u0026#34; # Not needed with the new state repositories per application (app-xxx) application: \u0026lt;application\u0026gt; env: pro state_repo: \u0026lt;org\u0026gt;/\u0026lt;old-state-repo\u0026gt; # The state_repo field now must include the organization service_names: [\u0026#39;service\u0026#39;] version: $latest_releaseWith that, the pro environment dispatch should still go to the old state repository.\nüîÑ When also updating the deployment\u0026rsquo;s namespace# If the namespace also needs to be updated when migrating the state repo (because, for example, the old one was too broad and more specific namespaces are desired), a couple of extra steps need to be taken:\nChange the namespace field of the \u0026lt;env\u0026gt;.yaml file Look up the old namespace on the organization\u0026rsquo;s repositories, and update it on whatever files are necessary. When checking if everything is working, also check any connections that use the new namespace. üîê Migrating secrets from an external provider to ExternalSecrets# In order to migrate secrets from an external provider (e.g. Azure KeyVault) into a Kubernetes ExternalSecrets object, go to the state-sys-services repo and do the following:\nCreate the folders necessary for your application if not already done, following this structure Create an additional extra_artifacts folder inside kubernetes-sys-services/\u0026lt;cluster\u0026gt;/\u0026lt;application\u0026gt; with the following files: # external_secret.yaml apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt; spec: refreshInterval: 1h secretStoreRef: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt; kind: SecretStore target: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt; creationPolicy: Owner data: - secretKey: \u0026lt;kubernetes-secret-key-name-1\u0026gt; remoteRef: key: \u0026lt;remote-secret-key-name-1\u0026gt; - secretKey: \u0026lt;kubernetes-secret-key-name-2\u0026gt; remoteRef: key: \u0026lt;remote-secret-key-name-2\u0026gt; - secretKey: \u0026lt;kubernetes-secret-key-name-3\u0026gt; remoteRef: key: \u0026lt;remote-secret-key-name-3\u0026gt; ...# secret_store.yaml apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt; spec: provider: azurekv: # This object key name and its fields differ between providers. See https://external-secrets.io/latest/introduction/overview/ for documentation authType: WorkloadIdentity vaultUrl: \u0026lt;az-keyvault-url\u0026gt; serviceAccountRef: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt;apiVersion: v1 kind: ServiceAccount metadata: name: \u0026lt;cluster\u0026gt;-\u0026lt;application\u0026gt; annotations: # The annotations\u0026#39; names and values will differ between providers. See https://external-secrets.io/latest/introduction/overview/ for documentation azure.workload.identity/client-id: \u0026lt;az-client-id\u0026gt; azure.workload.identity/tenant-id: \u0026lt;az-tenant-id\u0026gt;üîë Migrating secrets from an external secret provider to Kubernetes CSI# In order to migrate secrets from an Azure KeyVault or AWS Parameter Store into a Kubernetes CSI, go to the state-sys-services repo and do the following:\nCreate the folders necessary for your application if not already done, following this structure Create an additional extra_artifacts folder inside kubernetes-sys-services/\u0026lt;cluster\u0026gt;/\u0026lt;application\u0026gt; with the following file: # secrets.yaml (AZ Keyvault example) \u0026lt;application\u0026gt;: kvSecrets: - kv: \u0026lt;keyvault-1\u0026gt; data: \u0026lt;kubernetes-env-variable-1\u0026gt;: \u0026lt;keyvault-secret-name-1\u0026gt; # The keyvault secret with the name \u0026lt;keyvault-secret-name-1\u0026gt; will become a environment variable with the name \u0026lt;kubernetes-env-variable-1\u0026gt; inside the pod \u0026lt;kubernetes-env-variable-2\u0026gt;: \u0026lt;keyvault-secret-name-2\u0026gt; # The keyvault secret with the name \u0026lt;keyvault-secret-name-2\u0026gt; will become a environment variable with the name \u0026lt;kubernetes-env-variable-2\u0026gt; inside the pod ... - kv: \u0026lt;keyvault-2\u0026gt; data: \u0026lt;kubernetes-env-variable-3\u0026gt;: \u0026lt;keyvault-secret-name-3\u0026gt; # The keyvault secret with the name \u0026lt;keyvault-secret-name-3\u0026gt; will become a environment variable with the name \u0026lt;kubernetes-env-variable-3\u0026gt; inside the pod \u0026lt;kubernetes-env-variable-4\u0026gt;: \u0026lt;keyvault-secret-name-4\u0026gt; # The keyvault secret with the name \u0026lt;keyvault-secret-name-4\u0026gt; will become a environment variable with the name \u0026lt;kubernetes-env-variable-4\u0026gt; inside the pod ... - ...Additional documentation\n"},{"id":20,"href":"/docs/providers/","title":"Providers","section":"Firestartr Documentation","content":"Claims providers# Terraform "},{"id":21,"href":"/docs/providers/terraform/","title":"Terraform","section":"Providers","content":"Terraform Provider Documentation# This section contains documentation for the Firestartr Pro Terraform provider.\nFeatures# Workspace Synchronization# Terraform Workspace Policies - Configure the policies and operations in Terraform workspaces. Terraform Workspace Sync - Configure synchronization for Terraform workspaces. Provider Configuration# The Firestartr Pro Terraform provider includes various configuration options and resources to manage your infrastructure and synchronization settings.\n"},{"id":22,"href":"/docs/providers/terraform/workspace-policies/","title":"Workspace Policies","section":"Terraform","content":"Terraform Provider Policies \u0026amp; Operation Security# The policy setting in the providers.terraform block acts as the primary security gate for all infrastructure operations. Each policy defines the controller\u0026rsquo;s level of authority over the Terraform workspace through a set of allowed operations.\nPolicy Hierarchy# Firestartr uses a hierarchical system to determine policy compatibility. Policies range from read-only observation to full lifecycle management.\nPolicy Name Aliases Allowed Operations full-control N/A UPDATED, CREATED, RENAMED, SYNC, MARKED_TO_DELETION, RETRY, NOTHING apply create-update-only UPDATED, CREATED, RENAMED, SYNC, RETRY, NOTHING observe observe-only SYNC create-only N/A CREATED, RETRY, SYNC Detailed Operation Mapping for Terraform# Each policy explicitly enables a set of internal controller actions during a Terraform run:\nSYNC: Allowed by all policies. This enables the controller to fetch the current state and perform a terraform plan to identify drift. CREATED: Permits the initial terraform apply to create new resources. Enabled in full-control, apply, and create-only. UPDATED / RENAMED: Allows modification of existing infrastructure through terraform apply. These are restricted in observe and create-only policies to prevent state changes. MARKED_TO_DELETION: The most sensitive operation. It grants permission to perform a terraform destroy or remove resources from state. Only permitted under the full-control policy. RETRY: Allows the controller to automatically re-attempt a failed Terraform run. Available in all policies except observe. NOTHING: Allows the controller to run and perform no operation. Available in all policies. General Policy vs. Sync Policy# To get more information about sync you can check Workspace Synchronization.\nIn a TFWorkspaceClaim, you define policies in two different places. Understanding their distinct roles is key to a secure configuration:\nGeneral Provider Policy (providers.terraform.policy):\nDefinition: This is the \u0026ldquo;Master Gate\u0026rdquo;. It defines the maximum level of authority that the Firestartr controller has over this specific workspace. Scope: It affects every operation, including manual triggers, initial provisioning, and automated tasks. Security Role: It acts as a safety guardrail. For example, if set to apply, the controller is physically prevented from performing a destroy operation, even if a user or a script requests it. Sync Policy (providers.terraform.sync.policy):\nDefinition: This is the \u0026ldquo;Execution Permission\u0026rdquo; for scheduled tasks. It defines what the controller is allowed to do during an automated synchronization cycle (defined by period or schedule). Scope: It only applies to background synchronization events. Security Role: It allows you to have a permissive workspace (e.g., full-control for manual fixes) but a conservative background process (e.g., observe only for automated drift detection). Crucial Concept: The Sync Policy can never exceed the permissions granted by the General Policy.\nPolicy Compatibility Logic# When using scheduled synchronization (providers.terraform.sync), Firestartr validates that the sync.policy is compatible with the general providers.terraform.policy.\nValidation Rule# For a synchronization cycle to be valid, the General Policy must be at least as permissive as the Sync Policy. In other words, you cannot grant a background sync task more authority than the workspace itself has.\nCompatibility Scenarios:# Valid: General Policy full-control and Sync Policy apply. The workspace allows everything, but the sync task is restricted to only performing updates. Valid: General Policy apply and Sync Policy observe. The workspace allows updates, but the scheduled sync only monitors for drift without applying changes. Invalid: General Policy observe and Sync Policy apply. This will throw a Policy {syncPolicy} or {generalPolicy} not compatible error because the scheduled task is trying to perform updates (apply) that are forbidden by the workspace\u0026rsquo;s master policy (observe). Technical Summary of Aliases# To maintain flexibility and backward compatibility in your TFWorkspaceClaim definitions, the following aliases are resolved internally:\ncreate-update-only: Maps to apply. Ideal for GitOps flows where deletions are not desired. observe-only: Maps to observe. Best for strictly read-only monitoring. "},{"id":23,"href":"/docs/providers/terraform/workspace-sync/","title":"Workspace Sync","section":"Terraform","content":"Terraform Workspace Synchronization# The Terraform provider sync configuration allows you to control how and when Terraform workspaces are synchronized within TFWorkspaceClaim definitions.\nOverview# The sync section within providers.terraform in a TFWorkspaceClaim enables automatic synchronization of Terraform state and resources at specified intervals or schedules.\nConfiguration# TFWorkspaceClaim Sync Configuration# Configure synchronization in your TFWorkspaceClaim YAML file within the providers.terraform.sync section:\nkind: TFWorkspaceClaim lifecycle: production name: example-workspace type: database owner: \u0026#39;group:firestartr-team\u0026#39; system: \u0026#39;system:firestartr-system\u0026#39; version: \u0026#39;1.0\u0026#39; providers: terraform: policy: full-control tfStateKey: a1850b50-677d-4a81-92a4-1318503b5568 name: example-workspace source: Inline sync: enabled: true period: \u0026#34;5m\u0026#34; policy: \u0026#34;apply\u0026#34; module: | # Your Terraform module content output \u0026#34;example\u0026#34; { value = \u0026#34;Hello World\u0026#34; } values: {} context: providers: - name: provider-aws-workspaces backend: name: firestartr-terraform-stateSync Configuration Options# The sync configuration supports the following properties:\nSetting Type Description Required Format enabled boolean Enable/disable synchronization Yes true or false period string Sync interval using duration format No* ^[0-9]+[smhd]$ (e.g., 5m, 1h, 30s) schedule string Cron schedule expression No* Cron format with optional seconds schedule_timezone string Timezone for cron schedule No Standard timezone (e.g., UTC, America/New_York) policy string Sync policy determining allowed operations No observe, apply, create-only, full-control *Note: Either period or schedule must be specified, but not both.\nScheduling Options# You have two mutually exclusive options for scheduling synchronization:\n1. Period-based Synchronization# Use the period property for simple interval-based synchronization:\nproviders: terraform: sync: enabled: true period: \u0026#34;5m\u0026#34; # Sync every 5 minutesPeriod Format: ^[0-9]+[smhd]$\ns = seconds (e.g., 30s) m = minutes (e.g., 5m) h = hours (e.g., 2h) d = days (e.g., 1d) 2. Schedule-based Synchronization# Use the schedule property for cron-based scheduling:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;*/5 * * * *\u0026#34; # Every 5 minutes schedule_timezone: \u0026#34;UTC\u0026#34;Schedule Format: Uses cron-parser with optional seconds field\nStandard 5-field format: minute hour day month dayofweek Optional 6-field format: second minute hour day month dayofweek Cron Format Reference# * * * * * * ‚î¨ ‚î¨ ‚î¨ ‚î¨ ‚î¨ ‚î¨ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ day of week (0-7, 1L-7L) (0 or 7 is Sun) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1-12, JAN-DEC) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of month (1-31, L) ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0-23) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0-59) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ second (0-59, optional) Field Values Description Second (optional) 0-59 Second field (when using 6-field format) Minute 0-59 Minute field Hour 0-23 Hour field Day of Month 1-31, L Day of the month, or L for last day Month 1-12, JAN-DEC Month field, numeric or abbreviated name Day of Week 0-7, SUN-SAT, 1L-7L Day of week (0 or 7 is Sunday) Special Characters:\nCharacter Description Example * Any value * * * * * (every minute) ? Any value (alias for *) ? * * * * (every minute) , Value list separator 1,2,3 * * * * (1st, 2nd, and 3rd minute) - Range of values 1-5 * * * * (every minute from 1 through 5) / Step values */5 * * * * (every 5th minute) L Last day of month/week 0 0 L * * (midnight on last day of month) # Nth day of month 0 0 * * 1#1 (first Monday of month) Common Cron Patterns# Frequency-Based Patterns:\n\u0026#34;*/5 * * * *\u0026#34; # Every 5 minutes \u0026#34;0 */4 * * *\u0026#34; # Every 4 hours at the top of the hour \u0026#34;0 0 */2 * *\u0026#34; # Every 2 days at midnight \u0026#34;0 0 0 * *\u0026#34; # Daily at midnight \u0026#34;0 0 0 * * 1\u0026#34; # Weekly on Mondays at midnight (6-field format)Business Hours Patterns:\n\u0026#34;0 9-17 * * 1-5\u0026#34; # Every hour from 9 AM to 5 PM, weekdays only \u0026#34;0 9,12,15 * * 1-5\u0026#34; # At 9 AM, noon, and 3 PM on weekdays \u0026#34;0 8 * * 1-5\u0026#34; # Every weekday at 8 AM \u0026#34;0 18 * * 1-5\u0026#34; # Every weekday at 6 PM \u0026#34;0 9 * * 1\u0026#34; # Every Monday at 9 AMMaintenance Window Patterns:\n\u0026#34;0 2 * * 0\u0026#34; # Every Sunday at 2 AM \u0026#34;0 3 1 * *\u0026#34; # First day of every month at 3 AM \u0026#34;0 4 * * 6\u0026#34; # Every Saturday at 4 AM \u0026#34;0 1 15 * *\u0026#34; # 15th of every month at 1 AM \u0026#34;0 0 1 1,7 *\u0026#34; # January 1st and July 1st at midnightDevelopment Environment Patterns:\n\u0026#34;*/2 * * * *\u0026#34; # Every 2 minutes (high frequency) \u0026#34;*/15 8-18 * * 1-5\u0026#34; # Every 15 minutes during work hours on weekdays \u0026#34;0 */1 * * 1-5\u0026#34; # Every hour on weekdays \u0026#34;30 9-17/2 * * 1-5\u0026#34; # Every 2 hours at 30 minutes past, 9 AM to 5 PM, weekdaysProduction Environment Patterns:\n\u0026#34;0 2,14 * * *\u0026#34; # Twice daily at 2 AM and 2 PM \u0026#34;0 6 * * *\u0026#34; # Once daily at 6 AM \u0026#34;0 3 * * 0,3\u0026#34; # Twice weekly on Sunday and Wednesday at 3 AM \u0026#34;0 4 1,15 * *\u0026#34; # Twice monthly on 1st and 15th at 4 AMAdvanced Patterns with Seconds (6-field format):\n\u0026#34;0 */5 * * * *\u0026#34; # Every 5 minutes at the start of the minute \u0026#34;30 */10 * * * *\u0026#34; # Every 10 minutes at 30 seconds past \u0026#34;0,30 * * * * *\u0026#34; # Every 30 seconds \u0026#34;15 0 9 * * 1-5\u0026#34; # Every weekday at 9:00:15 AMQuick Reference - Common Use Cases# Use Case Cron Expression Description Every minute \u0026quot;* * * * *\u0026quot; High-frequency monitoring Every 5 minutes \u0026quot;*/5 * * * *\u0026quot; Development environments Every hour \u0026quot;0 * * * *\u0026quot; Regular monitoring Business hours only \u0026quot;0 9-17 * * 1-5\u0026quot; Weekday office hours Daily maintenance \u0026quot;0 2 * * *\u0026quot; Daily at 2 AM Weekly maintenance \u0026quot;0 2 * * 0\u0026quot; Sunday at 2 AM Monthly maintenance \u0026quot;0 2 1 * *\u0026quot; First of month at 2 AM Twice daily \u0026quot;0 6,18 * * *\u0026quot; Morning and evening Weekdays only \u0026quot;0 9 * * 1-5\u0026quot; Business days at 9 AM Weekends only \u0026quot;0 10 * * 6,0\u0026quot; Saturday and Sunday at 10 AM Usage Examples# Basic Period-based Sync# Simple synchronization every 10 minutes:\nkind: TFWorkspaceClaim name: basic-sync-example providers: terraform: sync: enabled: true period: \u0026#34;10m\u0026#34; # ... other terraform configurationAdvanced Schedule-based Sync# Synchronize during business hours only:\nkind: TFWorkspaceClaim name: business-hours-sync providers: terraform: sync: enabled: true schedule: \u0026#34;0 9-17 * * 1-5\u0026#34; # Every hour from 9 AM to 5 PM, Monday to Friday schedule_timezone: \u0026#34;America/New_York\u0026#34; policy: \u0026#34;observe\u0026#34; # ... other terraform configurationHigh-frequency Development Sync# For development environments requiring frequent updates:\nkind: TFWorkspaceClaim name: dev-environment providers: terraform: sync: enabled: true period: \u0026#34;2m\u0026#34; # Sync every 2 minutes # ... other terraform configurationReal-World Cron Scheduling Examples# Multi-Environment Sync Strategy# Production Environment - Conservative Approach:\nkind: TFWorkspaceClaim name: prod-environment providers: terraform: sync: enabled: true schedule: \u0026#34;0 2 * * 0\u0026#34; # Every Sunday at 2 AM schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;observe\u0026#34; # Audit only, no changesStaging Environment - Regular Testing:\nkind: TFWorkspaceClaim name: staging-environment providers: terraform: sync: enabled: true schedule: \u0026#34;0 6,18 * * 1-5\u0026#34; # Twice daily at 6 AM and 6 PM, weekdays schedule_timezone: \u0026#34;America/New_York\u0026#34; policy: \u0026#34;apply\u0026#34;Development Environment - Continuous Sync:\nkind: TFWorkspaceClaim name: dev-environment providers: terraform: sync: enabled: true schedule: \u0026#34;*/15 8-20 * * 1-5\u0026#34; # Every 15 minutes during work hours schedule_timezone: \u0026#34;America/Los_Angeles\u0026#34; policy: \u0026#34;full-control\u0026#34;Industry-Specific Patterns# Financial Services - Compliance Window:\n# Sync during non-trading hours only providers: terraform: sync: enabled: true schedule: \u0026#34;0 20 * * 1-5\u0026#34; # Weekdays at 8 PM after markets close schedule_timezone: \u0026#34;America/New_York\u0026#34; policy: \u0026#34;apply\u0026#34;E-commerce - Low Traffic Windows:\n# Sync during low traffic periods providers: terraform: sync: enabled: true schedule: \u0026#34;0 3,15 * * *\u0026#34; # Daily at 3 AM and 3 PM schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;apply\u0026#34;Global Operations - Follow-the-Sun:\n# Sync during business hours in different regions providers: terraform: sync: enabled: true schedule: \u0026#34;0 9,21 * * 1-5\u0026#34; # 9 AM and 9 PM to cover multiple timezones schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;observe\u0026#34;Maintenance and Deployment Patterns# Monthly Maintenance Window:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;0 2 1 * *\u0026#34; # First day of each month at 2 AM schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;full-control\u0026#34;Second Tuesday Deployments:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;0 3 * * 2#2\u0026#34; # Second Tuesday of each month at 3 AM schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;apply\u0026#34;Weekend Deployments:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;0 4 * * 6\u0026#34; # Every Saturday at 4 AM schedule_timezone: \u0026#34;America/Chicago\u0026#34; policy: \u0026#34;full-control\u0026#34;High-Frequency Monitoring Patterns# Infrastructure Monitoring:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;*/5 * * * *\u0026#34; # Every 5 minutes for monitoring schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;observe\u0026#34; # Monitor only, no changesSecurity Compliance Checks:\nproviders: terraform: sync: enabled: true schedule: \u0026#34;0 */4 * * *\u0026#34; # Every 4 hours for security monitoring schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;observe\u0026#34;Sync Behavior# Synchronization Process# The synchronization operates based on the configured schedule or period:\nTrigger: Sync is triggered either by period interval or cron schedule State Check: The system checks for changes in the Terraform workspace Execution: If changes are detected, the sync policy is applied Logging: All sync activities are logged for monitoring and debugging Policy Configuration# The policy field determines what actions can be performed during synchronization. The following policies are available:\nPolicy Create Update Delete Use Case observe ‚úó ‚úó ‚úó Audit/monitor only apply ‚úì ‚úì (‚úó)* Standard GitOps (update/patch only) full-control ‚úì ‚úì ‚úì Strict enforcement, full reconciliation create-only ‚úì ‚úó ‚úó Seeding resources, preserving manual edits Note: The apply policy typically does not delete resources, focusing on updates and patches.\nPolicy Usage Examples# Production Environment (Audit/Monitor Only):\nproviders: terraform: sync: enabled: true schedule: \u0026#34;0 */6 * * *\u0026#34; # Every 6 hours policy: \u0026#34;observe\u0026#34; # No changes, audit onlyDevelopment Environment (Full Reconciliation):\nproviders: terraform: sync: enabled: true period: \u0026#34;5m\u0026#34; policy: \u0026#34;full-control\u0026#34; # Create, update, and delete resourcesStaging Environment (GitOps Updates):\nproviders: terraform: sync: enabled: true period: \u0026#34;15m\u0026#34; policy: \u0026#34;apply\u0026#34; # Create and update, but preserve existing resourcesInitial Deployment (Create Resources Only):\nproviders: terraform: sync: enabled: true period: \u0026#34;10m\u0026#34; policy: \u0026#34;create-only\u0026#34; # Only create new resources, preserve manual changesTimezone Handling# When using schedule, you can specify a timezone with schedule_timezone. If not specified, UTC is used by default.\nBest Practices# Scheduling Recommendations# Production Environments: Use longer intervals (e.g., 30m or scheduled during maintenance windows) Development Environments: Use shorter intervals for rapid iteration (e.g., 2m to 5m) Staging Environments: Balance between development and production (e.g., 10m) Schedule vs Period Selection# Use period for simple, regular intervals Use schedule for complex timing requirements (business hours, specific days, etc.) Use timezone when coordinating across different geographical locations Cron Expression Validation# Before Deployment:\nValidate syntax using online tools:\ncrontab.guru - For 5-field expressions cron-job.org - Supports 6-field expressions cronhub.io - Alternative validator Test timing with your timezone:\nConsider daylight saving time transitions Verify business hours align with intended times Check impact of timezone changes on schedules Document your schedule:\nAdd comments explaining the business logic Include expected execution times in documentation Note any timezone-specific considerations Configuration Guidelines# # Production example - conservative sync providers: terraform: sync: enabled: true schedule: \u0026#34;0 */2 * * *\u0026#34; # Every 2 hours schedule_timezone: \u0026#34;UTC\u0026#34; policy: \u0026#34;observe\u0026#34; # Development example - frequent sync providers: terraform: sync: enabled: true period: \u0026#34;5m\u0026#34; # Every 5 minutesTroubleshooting# Common Issues# Sync Not Triggering\nVerify enabled is set to true Check that either period or schedule is specified (not both) Validate cron expression format for schedule Ensure timezone is correctly specified Schedule Format Errors\nUse cron-parser compatible format Remember that seconds field is optional Test cron expressions before deploying Common Cron Expression Mistakes\nInvalid range: \u0026quot;0 9-5 * * *\u0026quot; ‚ùå (hour range goes backwards) ‚Üí Use \u0026quot;0 9-17 * * *\u0026quot; ‚úÖ Wrong day format: \u0026quot;0 9 * * Monday\u0026quot; ‚ùå ‚Üí Use \u0026quot;0 9 * * 1\u0026quot; or \u0026quot;0 9 * * MON\u0026quot; ‚úÖ Month confusion: \u0026quot;0 9 * 13 *\u0026quot; ‚ùå (month 13 doesn\u0026rsquo;t exist) ‚Üí Use \u0026quot;0 9 * 12 *\u0026quot; ‚úÖ Mixed formats: \u0026quot;30 0 9 * * 1-5\u0026quot; ‚ùå (6 fields but inconsistent) ‚Üí Use \u0026quot;0 9 * * 1-5\u0026quot; ‚úÖ (5 fields) or \u0026quot;0 30 9 * * 1-5\u0026quot; ‚úÖ (6 fields) Timezone issues: Schedule in wrong timezone ‚Üí Always specify schedule_timezone explicitly Cron Expression Testing\nUse online cron validators like crontab.guru for 5-field expressions Test expressions with your specific timezone settings Consider daylight saving time changes when using non-UTC timezones Period Format Errors\nEnsure format matches ^[0-9]+[smhd]$ pattern Valid examples: 30s, 5m, 2h, 1d Invalid examples: 5mins, 2hours, 1 day Schema Validation# The sync configuration follows this JSON schema:\n{ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabled\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34; }, \u0026#34;period\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;^[0-9]+[smhd]$\u0026#34; }, \u0026#34;schedule\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;schedule_timezone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;additionalProperties\u0026#34;: false, \u0026#34;required\u0026#34;: [\u0026#34;enabled\u0026#34;], \u0026#34;oneOf\u0026#34;: [ { \u0026#34;required\u0026#34;: [\u0026#34;period\u0026#34;] }, { \u0026#34;required\u0026#34;: [\u0026#34;schedule\u0026#34;] }, { \u0026#34;not\u0026#34;: { \u0026#34;anyOf\u0026#34;: [ { \u0026#34;required\u0026#34;: [\u0026#34;period\u0026#34;] }, { \u0026#34;required\u0026#34;: [\u0026#34;schedule\u0026#34;] } ] } } ] }Validation Rules# enabled is always required Either period OR schedule must be specified (mutually exclusive) schedule_timezone can only be used with schedule period must match the pattern ^[0-9]+[smhd]$ schedule uses cron-parser format with optional seconds field Migration Guide# Adding Sync to Existing TFWorkspaceClaim# To add synchronization to an existing workspace:\nUpdate the YAML: Add the sync section to your TFWorkspaceClaim Apply Changes: Deploy the updated claim to your cluster Monitor: Watch the sync behavior and adjust timing as needed # Before providers: terraform: name: existing-workspace # ... other config # After providers: terraform: name: existing-workspace sync: enabled: true period: \u0026#34;10m\u0026#34; # ... other configChanging Sync Configuration# Simply update the sync section and redeploy the TFWorkspaceClaim. Changes take effect on the next sync cycle.\n"},{"id":24,"href":"/docs/state-apps-repository/","title":"State Apps Repository","section":"Firestartr Documentation","content":"üîç Overview# This documentation focuses on the structure and purpose of application repositories for deploying workloads in Kubernetes. Each application has its own dedicated repository to ensure isolation, clear organization, and streamlined management of configuration and deployment files.\nThe repository serves as the single source of truth for the application‚Äôs configuration across clusters, tenants, and environments.\nüìÇ Repository directory structure# Applications are organized in the following directory structure within the Git repository:\nüå± main (or master) branch# This branch serves as the source for environment-specific configurations and high-level settings for the application.\nkubernetes/ ‚îú‚îÄ‚îÄ \u0026lt;cluster-name\u0026gt;/ # Target Kubernetes cluster ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;tenant-name\u0026gt;/ # Team or tenant managing the app ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env-name\u0026gt;/ # Environment (dev, staging, prod) ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ values.yaml # Parameters specific to the environment ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ extra_artifacts/ # Optional extra artifacts for kubernetes deploymentsEach application deployment requires a configuration file at the following path kubernetes/\u0026lt;cluster-name\u0026gt;/\u0026lt;tenant-name\u0026gt;/\u0026lt;env-name\u0026gt;.yaml with the following structure:\nchart: \u0026lt;alias\u0026gt;/\u0026lt;chart-name\u0026gt; registry: \u0026lt;registry_url\u0026gt; version: \u0026lt;version\u0026gt; releaseName: \u0026lt;release_name\u0026gt; hooks: [] extraPatches: []üöÄ deployment branch# This branch contains the raw Kubernetes manifests that ArgoCD applies to clusters. These manifests are generated from the configurations in the main (or master) branch.\nkubernetes/ ‚îú‚îÄ‚îÄ \u0026lt;cluster-name\u0026gt;/ # Cluster where the application is deployed ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;tenant-name\u0026gt;/ # Tenant or team managing the application ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;env-name\u0026gt;/ # Environment (dev, staging, prod) ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Deployment.\u0026lt;metadata.name\u0026gt;.yaml # Kubernetes Deployment resource ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Configmap.\u0026lt;metadata.name\u0026gt;.yaml # ConfigMaps or other configs ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...other-resourcesüìö Repository Usage Design# There are two main flows in the state repository:\nAutomatic image update: Whenever a new spanshot, pre-release or release is created on any of the microservices, the image is built and pushed to the state repository. On demand deployments: State values may be updated at any time by the user, just by updating the values files. Then, to apply the changes, the user must trigger the deployment. Has been made this way to follow the GitOps principles, where the state of the system is stored in a Git repository as code, and the changes are applied through pull requests and merges. Keeping the actual state of the system in a wet repository or branch in this case.\nüîÑ Automatic image update# The automatic image update flow is triggered by the creation of a new snapshot, pre-release or release on any of the microservices.\nThen it dispatches new deployments with the new microservice images to the state repository.\nAdditionally the person that triggered the image update is added as reviewer together with appropiate labels to be abel to track it down more easily\nImages versions are handled by the charts, adding an annotation with the image version and microservice name for each resource affected. Whenever a deployment is triggered, it retrieves the image version from the annotation if not specified by the new release. Therefore if a deployment uses two different microservices, it will use the image version specified in the release for the microservice that has a new version, and the one specified in the annotation for the other microservice.\nüñêÔ∏è On demand deployments# On demand deployments flow is triggered by the user using the manual workflow dispatch action, being able to use glob patterns to deploy multiple environments at once.\nThe user must update the values files in the state repository before triggering the deployment.\nThen n deployment pull requests are created, the user must review and merge them to apply the changes.\nIf a pull request for a given environment is already open, the deployment will throw an error for that environment, making the user to close or merging the pull request before triggering the deployment again.\nüîß Branch and Deployment Revision# The deployment configuration is based on a specific Git branch and revision:\nBranch: deployment Path: kubernetes/*/*/* ‚Äî The application is retrieved dynamically from a path that matches the directory structure described above. This means that the application is linked to a specific folder within the deployment branch of the repository, enabling easy identification and deployment based on the cluster, tenant, and environment.\n‚öôÔ∏è Dynamic application deployment with ApplicationSet# Argo CD‚Äôs ApplicationSet configuration enables dynamic creation of application deployments. When new deployments are needed, they are automatically created by referencing the directory structure in the repository.\nHow It Works:# The ApplicationSet reads the structure of the repository and dynamically creates applications for each combination of cluster, tenant, and environment found in the paths under kubernetes/*/*/*. The Application name is generated based on the directory segments, following this format: \u0026lt;tenant\u0026gt;-\u0026lt;my-app\u0026gt;-\u0026lt;environment\u0026gt; # apps/\u0026lt;application-name\u0026gt;/argo-\u0026lt;application-name\u0026gt;.ApplicationSet.yaml --- apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: app-\u0026lt;application-name\u0026gt; namespace: argocd spec: generators: - git: directories: - path: \u0026lt;technology.type\u0026gt;/*/*/* # technology.type -\u0026gt; kubernetes, vmss, etc. as defined by the cluster configuration in the .firestartr repo repoURL: https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git revision: deployment values: \u0026lt;cluster1-name\u0026gt;: \u0026lt;cluster1-url\u0026gt; \u0026lt;cluster2-name\u0026gt;: \u0026lt;cluster2-url\u0026gt; \u0026lt;cluster3-name\u0026gt;: \u0026lt;cluster3-url\u0026gt; ... goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: \u0026#39;app-\u0026lt;application-name\u0026gt;-{{index .path.segments 1}}-{{index .path.segments 2}}-{{index .path.segments 3}}\u0026#39; labels: app-name: \u0026#39;\u0026lt;application-name\u0026gt;\u0026#39; spec: destination: namespace: \u0026#39;{{index .path.segments 2}}-\u0026lt;application-name\u0026gt;-{{index .path.segments 3}}\u0026#39; server: \u0026#39;{{index .values (index .path.segments 1)}}\u0026#39; project: \u0026#39;app-\u0026lt;application-name\u0026gt;\u0026#39; source: path: \u0026#39;{{index .path.segments 0}}/{{index .path.segments 1}}/{{index .path.segments 2}}/{{index .path.segments 3}}\u0026#39; repoURL: https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git targetRevision: deployment syncPolicy: automated: null syncOptions: - CreateNamespace=true - ServerSideApply=true ignoreDifferences: - group: \u0026#34;policy\u0026#34; kind: PodDisruptionBudget jsonPointers: - /status - /metadata/uid - /metadata/generation - /metadata/resourceVersion - /metadata/creationTimestampüîî Notification system with ArgoCD# ArgoCD notifications provides feedback to the user about the status of the deployments. There are three types of notifications:\nGithub Commit Status: Adds a status to the commit synced by the app. Github Deployment: Creates a deployment in the repository. üî® How It Works# ArgoCD notifications are configured with the following components:\nTemplates: Define the content of the notification, using Go templates. They can be customized for each service (e.g., Slack, GitHub). Triggers: Define the conditions that trigger the notification. They can be based on the status of the application, the environment, or other factors. It also specifies what template to use by template name. Subscriptions: Define what triggers to use in each application. We are configuring it globally but it can be configured per application using annotations. Check the docs for further reading.\n‚öôÔ∏è Setup# Create a GitHub App fs-argocd-notifications in the client with the following permissions:\nDeployments: Read \u0026amp; Write Commit statuses: Read \u0026amp; Write Environments: Read \u0026amp; Write Then, generate a private key and store it in your cloud provider:\naz keyvault secret set --name fs-argocd-notifications-pem --vault-name \u0026lt;VAULT_NAME\u0026gt; --file fs-argocd-notifications.\u0026lt;DATE\u0026gt;.private-key.pemCreate a Slack App, install it in the desired workspace, and generate a token.\nStore the token in your cloud provider:\naz keyvault secret set --name slack-token --vault-name \u0026lt;VAULT_NAME\u0026gt; --value \u0026lt;SLACK_TOKEN\u0026gt;Make sure to have the following permissions:\nchat:write chat:write.customize üìù ArgoCD helm chart# We need to configure the chart with the following values:\nnotifications: secret: # As we are using state repository, we cannot store the secret in the repository create: false argocdUrl: \u0026#34;\u0026lt;ARGOCD_URL\u0026gt;\u0026#34; notifiers: service.github: | appID: \u0026lt;GITHUB_APP_ID\u0026gt; installationID: \u0026lt;GITHUB_INSTALLATION_ID\u0026gt; privateKey: $github-privateKey service.slack: | token: $slack-token subscriptions: - recipients: - github:\u0026lt;GITHUB_APP_NAME\u0026gt; # fs-argocd-notifications triggers: - on-sync-failed - on-health-degraded - on-deployed templates: template.app-deployed: | github: repoURLPath: \u0026#34;{{.app.spec.source.repoURL}}\u0026#34; revisionPath: \u0026#34;{{.app.status.operationState.syncResult.revision}}\u0026#34; status: state: success label: \u0026#34;continuous-delivery/{{.app.metadata.name}}\u0026#34; targetURL: \u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\u0026#34; deployment: state: success environment: \u0026#34;{{ .app.spec.source.path }}\u0026#34; environmentURL: \u0026#34;{{index .app.status.summary.externalURLs 0}}\u0026#34; logURL: \u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\u0026#34; requiredContexts: [] autoMerge: false transientEnvironment: false message: | {{if eq .serviceType \u0026#34;slack\u0026#34;}}:white_check_mark:{{end}} Application {{.app.metadata.name}} is now running new version of deployments manifests. slack: attachments: | [{ \u0026#34;title\u0026#34;: \u0026#34;{{ .app.metadata.name}}\u0026#34;, \u0026#34;title_link\u0026#34;:\u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#18be52\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Sync Status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{.app.status.sync.status}}\u0026#34;, \u0026#34;short\u0026#34;: true }, { \u0026#34;title\u0026#34;: {{- if .app.spec.source }} \u0026#34;Repository\u0026#34; {{- else if .app.spec.sources }} \u0026#34;Repositories\u0026#34; {{- end }}, \u0026#34;value\u0026#34;: {{- if .app.spec.source }} \u0026#34;:arrow_heading_up: {{ .app.spec.source.repoURL }}\u0026#34; {{- else if .app.spec.sources }} \u0026#34;{{- range $index, $source := .app.spec.sources }}{{ if $index }}\\n{{ end }}:arrow_heading_up: {{ $source.repoURL }}{{- end }}\u0026#34; {{- end }}, \u0026#34;short\u0026#34;: true }, { \u0026#34;title\u0026#34;: \u0026#34;Revision\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{.app.status.sync.revision}}\u0026#34;, \u0026#34;short\u0026#34;: true } {{range $index, $c := .app.status.conditions}} , { \u0026#34;title\u0026#34;: \u0026#34;{{$c.type}}\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{$c.message}}\u0026#34;, \u0026#34;short\u0026#34;: true } {{end}} ] }] deliveryPolicy: Post groupingKey: \u0026#34;\u0026#34; notifyBroadcast: false template.app-sync-failed: | github: repoURLPath: \u0026#34;{{.app.spec.source.repoURL}}\u0026#34; revisionPath: \u0026#34;{{.app.status.operationState.syncResult.revision}}\u0026#34; status: state: failure label: \u0026#34;continuous-delivery/{{.app.metadata.name}}\u0026#34; targetURL: \u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\u0026#34; deployment: state: failure environment: \u0026#34;{{ .app.spec.source.path }}\u0026#34; environmentURL: \u0026#34;{{index .app.status.summary.externalURLs 0}}\u0026#34; logURL: \u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\u0026#34; requiredContexts: [] autoMerge: false transientEnvironment: false message: | {{if eq .serviceType \u0026#34;slack\u0026#34;}}:exclamation:{{end}} The sync operation of application {{.app.metadata.name}} has failed at {{.app.status.operationState.finishedAt}} with the following error: {{.app.status.operationState.message}} Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true . slack: attachments: | [{ \u0026#34;title\u0026#34;: \u0026#34;{{ .app.metadata.name}}\u0026#34;, \u0026#34;title_link\u0026#34;:\u0026#34;{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#E96D76\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Sync Status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{.app.status.sync.status}}\u0026#34;, \u0026#34;short\u0026#34;: true }, { \u0026#34;title\u0026#34;: {{- if .app.spec.source }} \u0026#34;Repository\u0026#34; {{- else if .app.spec.sources }} \u0026#34;Repositories\u0026#34; {{- end }}, \u0026#34;value\u0026#34;: {{- if .app.spec.source }} \u0026#34;:arrow_heading_up: {{ .app.spec.source.repoURL }}\u0026#34; {{- else if .app.spec.sources }} \u0026#34;{{- range $index, $source := .app.spec.sources }}{{ if $index }}\\n{{ end }}:arrow_heading_up: {{ $source.repoURL }}{{- end }}\u0026#34; {{- end }}, \u0026#34;short\u0026#34;: true } {{range $index, $c := .app.status.conditions}} , { \u0026#34;title\u0026#34;: \u0026#34;{{$c.type}}\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{$c.message}}\u0026#34;, \u0026#34;short\u0026#34;: true } {{end}} ] }] deliveryPolicy: Post groupingKey: \u0026#34;\u0026#34; notifyBroadcast: false triggers: # Make sure that the send field is the same as the template name and you subscribe the app with the trigger name (e.g. on-deployed) trigger.on-deployed: | - description: Application is synced and healthy. Triggered once per commit. oncePer: app.status.sync.revision send: - app-deployed when: app.status.operationState.phase in [\u0026#39;Succeeded\u0026#39;] and app.status.health.status == \u0026#39;Healthy\u0026#39; trigger.on-health-degraded: | - description: Application has degraded send: - app-sync-failed when: app.status.health.status == \u0026#39;Degraded\u0026#39; trigger.on-sync-failed: | - description: Application syncing has failed send: - app-sync-failed when: app.status.operationState != nil and app.status.operationState.phase in [\u0026#39;Error\u0026#39;, \u0026#39;Failed\u0026#39;]üîí ArgoCD External Secrets# As we said before, we cannot store the private key in the repository, so we need to create an external secret with the private key. Therefore we need to create the following manifests:\nargocd-notifications-service-account.yml\napiVersion: v1 kind: ServiceAccount metadata: name: argocd-notifications annotations: azure.workload.identity/client-id: \u0026lt;CLIENT_ID\u0026gt; azure.workload.identity/tenant-id: \u0026lt;TENANT_ID\u0026gt;argocd-notifications-secret-store.yml\napiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: argocd-notifications spec: provider: azurekv: authType: WorkloadIdentity vaultUrl: \u0026#34;\u0026lt;VAULT_URL\u0026gt;\u0026#34; serviceAccountRef: name: argocd-notifications In this case we are using Azure Key Vault as the secret store, check the docs for other providers.\nargocd-notifications-external-secret.yml\napiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: argocd-notifications spec: refreshInterval: 1h secretStoreRef: kind: SecretStore name: argocd-notifications target: name: argocd-notifications-secret creationPolicy: Owner data: - secretKey: github-privateKey remoteRef: key: secret/fs-argocd-notifications-pem # If you used a different name for the secret in the vault, change it hereüîí Platform control through Argo Project# With the defined directory structure, the ApplicationSet generates namespaces based on the paths of the GitOps repository. This allows us to add deployment control through the AppProject, specifically per application, with a list of allowed namespaces and destinations.\nTherefore, if someone a new deployment of an application to the directories in the GitOps repository without going through the platform control, the ApplicationSet will attempt to create an Application with a namespace derived from the directories where the developer placed their artifacts. However, the AppProject will block the deployment since the generated namespace, or the configured cluster destination, is not among the allowed ones.\n--- # apps/\u0026lt;application-name\u0026gt;/argo-\u0026lt;application-name\u0026gt;.Project.yaml --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: app-\u0026lt;application-name\u0026gt; namespace: argocd annotations: # Make sure it matches the trigger name in the notifications configuration (e.g. on-deployed) notifications.argoproj.io/subscribe.on-health-degraded.slack: \u0026lt;slack-channel\u0026gt; notifications.argoproj.io/subscribe.on-sync-failed.slack: \u0026lt;slack-channel\u0026gt; notifications.argoproj.io/subscribe.on-deployed.github: \u0026lt;github-app-name\u0026gt; notifications.argoproj.io/subscribe.on-health-degraded.github: \u0026lt;github-app-name\u0026gt; notifications.argoproj.io/subscribe.on-sync-failed.github: \u0026lt;github-app-name\u0026gt; spec: description: \u0026lt;application-name\u0026gt; State Project # Obviously any description is valid clusterResourceWhitelist: # Add cluster wide resources that the application can manage # For instance we may need to create an IngressClass - group: networking.k8s.io kind: IngressClass sourceRepos: - \u0026#34;https://github.com/\u0026lt;org\u0026gt;/\u0026lt;new-state-repo\u0026gt;.git\u0026#34; destinations: - namespace: \u0026lt;tenant1-env1-application-namespace\u0026gt; server: \u0026lt;tenant1-env1-cluster-url\u0026gt; - namespace: \u0026lt;tenant2-env1-application-namespace\u0026gt; # Add only the namespaces and clusters that you plan to configure and deploy server: \u0026lt;tenant2-env1-cluster-url\u0026gt; # - namespace: \u0026#34;\u0026lt;tenant1-env2-application-namespace\u0026gt;\u0026#34; # \u0026lt;- You can add not yet configured namespaces and clusters as comments, and uncomment them with necessary # server: \u0026lt;tenant1-env2-cluster-url\u0026gt;"},{"id":25,"href":"/docs/state-sys-services-repository/","title":"State Sys Services Repository","section":"Firestartr Documentation","content":"üîç Overview# This repository is dedicated to managing system services for Kubernetes clusters, referred to as sys-services. These services are critical components that ensure the smooth operation of the Kubernetes clusters, such as ingress controllers, configuration reloading utilities (e.g., Stakater Reloader), and other essential utilities.\nThe repository centralizes all sys-service definitions and deployment configurations to ensure consistency, manageability, and automation across multiple Kubernetes clusters.\nüìÇ Structure of the repository# The repository is organized into a hierarchical folder structure to map services to clusters:\nüå± main (or master) branch# The main branch contains configuration files for sys-services. These files include values.yaml or similar templates that define the input parameters and settings required for generating Kubernetes manifests.\nPurpose: Acts as the source of truth for configuration data. It allows developers to manage high-level settings without directly interacting with raw Kubernetes manifests. Structure: kubernetes-sys-services/ ‚îú‚îÄ‚îÄ \u0026lt;cluster-name\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;service-name\u0026gt;.yaml ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;service-name\u0026gt;/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ values.yaml ‚îú‚îÄ‚îÄ another-cluster/ ‚îÇ ‚îî‚îÄ‚îÄ another-service/Each service requires a configuration file at the following path kubernetes-sys-services//.yaml with the following structure:\nchart: \u0026lt;alias\u0026gt;/\u0026lt;chart-name\u0026gt; registry: \u0026lt;registry_url\u0026gt; version: \u0026lt;version\u0026gt; releaseName: \u0026lt;release_name\u0026gt; hooks: [] extraPatches: []üöÄ deployment branch# The deployment branch contains raw Kubernetes manifests that are generated based on the configurations from the main branch. These manifests are directly applied to the Kubernetes clusters by ArgoCD.\nPurpose: Provides the final rendered Kubernetes objects that ArgoCD can deploy to clusters. These are the manifests ArgoCD synchronizes with the clusters.\nStructure:\nkubernetes-sys-services/ ‚îú‚îÄ‚îÄ \u0026lt;cluster-name\u0026gt;/ ‚îÇ ‚îú‚îÄ‚îÄ \u0026lt;service-name\u0026gt;/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Deployment.\u0026lt;metadata.name\u0026gt;.yaml ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Configmap.\u0026lt;metada.name\u0026gt;.yaml ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ...other-resources ‚îú‚îÄ‚îÄ another-cluster/ ‚îÇ ‚îî‚îÄ‚îÄ another-service/üóÇÔ∏è ApplicationSets and AppProjects per Sys-Service# In this setup, each sys-service has its own ApplicationSet and AppProject, providing granular control over deployments and ensuring clear separation of concerns between services.\nüì¶ ApplicationSet per Sys-Service# Each sys-service is managed by a dedicated ApplicationSet, which is responsible for dynamically creating and managing ArgoCD applications for that service across all relevant clusters.\nDynamic Application Generation: The ApplicationSet scans the deployment branch and generates ArgoCD Applications for each / path. Granularity: Each sys-service has its own ApplicationSet, which keeps configurations modular and easy to manage. Scalability: Adding a new sys-service requires only a new ApplicationSet definition, making the system easily extendable. Cluster Mapping: The ApplicationSet uses folder paths to map clusters and services, ensuring deployments target the correct clusters without manual intervention. --- apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: state-sys-service-\u0026lt;sys-service-name\u0026gt; namespace: argocd spec: generators: - git: directories: - path: kubernetes-sys-services/*/* # kubernetes-sys-services/cluster/service repoURL: https://github.com/\u0026lt;org\u0026gt;/state-sys-services.git revision: deployment values: central-aks: https://kubernetes.default.svc goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: \u0026#39;{{index .path.segments 1}}-{{index .path.segments 2}}\u0026#39; labels: app-name: \u0026#39;\u0026lt;application-name\u0026gt;\u0026#39; spec: destination: namespace: \u0026#39;kube-system\u0026#39; # specific namespace for the sys-service server: \u0026#39;{{index .values (index .path.segments 1)}}\u0026#39; # server project: \u0026#39;\u0026lt;sys-service-name\u0026gt;\u0026#39; # one project for each sys-service source: path: \u0026#39;{{index .path.segments 0}}/{{index .path.segments 1}}/{{index .path.segments 2}}\u0026#39; repoURL: https://github.com/\u0026lt;org\u0026gt;/state-sys-services.git targetRevision: deployment syncPolicy: automated: prune: true syncOptions: - CreateNamespace=trueüõ†Ô∏è AppProject per Sys-Service# Every sys-service is assigned a dedicated AppProject, which defines the scope, permissions, and boundaries for the applications generated by its ApplicationSet.\nIsolated Scope: Each sys-service operates within its own AppProject, preventing interference between different services. Source and Destination Restrictions: The AppProject limits: Source Repositories: Only allows applications to use the specific sys-services repository. Destinations: Restricts deployments to the kube-system namespace in the permitted clusters. Security: Ensures that no sys-service can deploy outside its intended scope, reducing the risk of misconfigurations or unauthorized access. --- apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: \u0026lt;sys-service-name\u0026gt; namespace: argocd spec: description: \u0026#39;Project for \u0026lt;sys-service-name\u0026gt;\u0026#39; sourceRepos: - \u0026#34;https://github.com/\u0026lt;org\u0026gt;/state-sys-services\u0026#34; destinations: - namespace: \u0026#34;{kube-system}\u0026#34; # \u0026lt;- allowed namespaces server: \u0026gt;- { https://kubernetes.default.svc, https://kubernetes.another.svc }"},{"id":26,"href":"/docs/The-dot-firestartr-repository/","title":"The Dot Firestartr Repository","section":"Firestartr Documentation","content":"üìÑ Introduction# When using Firestartr, each client should have a repository called .firestartr, which should contain a variety of YAML files describing many different aspects of the client\u0026rsquo;s repository, registry and application structures.\nüìÅ Repo structure# The repo structure is as follows:\napps/ ‚îú‚îÄ app1.yaml # app1 configuration YAML ‚îú‚îÄ app2.yml # app2 configuration YAML docker_registries/ ‚îú‚îÄ releases.yaml # releases registry configuration YAML ‚îú‚îÄ snapshots.yaml # snapshots registry configuration YAML platforms/ ‚îú‚îÄ cluster1.yaml # cluster1 configuration YAML ‚îî‚îÄ cluster2.yml # cluster2 configuration YAML providers/ ‚îî‚îÄ [app-name]/ ‚îî‚îÄ [claim-kind]/ # currently only the \u0026#39;TFWorkspace\u0026#39; kind is supported ‚îî‚îÄ [tenant]/ ‚îî‚îÄ [env]/ ‚îî‚îÄ provider1.yaml ‚îî‚îÄ provider2.yaml validations/ ‚îî‚îÄ apps/ ‚îî‚îÄ [app-name]/ ‚îî‚îÄ [claim-kind]/ # currently only the \u0026#39;TFWorkspace\u0026#39; kind is supported ‚îî‚îÄ validation1.yaml ‚îî‚îÄ validation2.yaml ‚îî‚îÄ policies/ ‚îî‚îÄ policy1.rego ‚îî‚îÄ policy2.regoA detailed explanation of each configuration file will be provided below.\nApp Registry Platform Provider App validation Policy üîß App configuration example and field description# name: app1 state_repo: \u0026#34;firestartr-test/state-app-sample-app\u0026#34; services: - repo: firestartr-test/build-and-dispatch-images-react service_names: [micro-a, micro-b] name: the name of the application for which this configuration will be applied. This value is used in the make_dispatches.yaml file (more info here) state_repo: the state repo related to the application (each application should have its own state repo). services: a list of service objects. Each of them contains: repo: the repository where objects will be uploaded. If they are prefixed by whether they are a docker image or a helm chart, this field\u0026rsquo;s value should be only the last part of the repo name (i.e., if images are prefixed with service/ and a image is uploaded as service/client/service-name, this fields value should be client/service-name). See the registry configuration files for more info on how to set the prefix. service_names: names of the services allowed to be saved for this app. These service names are the ones that are later written to the images.yaml file. üîß Registry configuration example and field description# name: snapshots # This is going to be used in the helmfile registry alias registry: prefappacr.azurecr.io image_types: [snapshots] default: true auth_strategy: azure_oidc base_paths: services: \u0026#34;service\u0026#34; charts: \u0026#34;charts\u0026#34; name: a name for the registry, which will be used by some of our apps. registry: url to the registry where the objects will be uploaded to. image_types: a list of strings, which can be either \u0026ldquo;snapshots\u0026rdquo;, \u0026ldquo;releases\u0026rdquo; or both. Specifies which type of images can be uploaded to this registry. default: when no registry is specified, use the one that has this field\u0026rsquo;s value set to true. There shouldn\u0026rsquo;t be multiple registries with this field set to true, but if there are, the first one found will be used. auth_strategy: type of authentication to use when login to the registry. Can be one of azure_oidc or aws_oidc base_paths: an object detailing the prefixes to use when uploading a service or chart to this registry. It has two properties: services: the prefix to use when uploading docker images (e.g. if this field\u0026rsquo;s value is service and we upload to the prefapp/application repository, the final coordinate will be service/prefapp/application). charts: the prefix to use when uploading helm charts (e.g. if this field\u0026rsquo;s value is chart and we upload to the prefapp/deployment repository, the final coordinate will be chart/prefapp/deployment). üîß Platform configuration example and field description# type: kubernetes name: cluster-name tenants: [test-tenant] envs: [dev, pre] type: describes the technology this platform uses. Allowed values are kubernetes or vmss. name: the name of this platform. This value is used in the make_dispatches.yaml file (more info here). tenants: A list of strings, used when this platform is set as a configuration value alongside one or multiple tenants for validation. envs: A list of strings, used when this platform is set as a configuration value alongside one or multiple environments for validation. üîß Providers configuration example and field description# name: provider-my-tenant-dev resourceTypes: - resourceType1 - resourceType2 ... name: the name of this provider. resourceTypes: A list of strings, each of which is the name of a resource type this provider applies to. üîß App validation configuration example and field description# name: my-validation description: \u0026#34;My validation description\u0026#34; regoFile: path/to_the/rego_file.rego applyTo: [condition list] data: - key1: value1 - key2: value2 ... name: the name of the validation file, which must be unique between them. description: a brief description of what this validation does. Purely for human readability purpouses. regoFile: path to the rego file, relative to the policies folder (i.e., this field\u0026rsquo;s value will be concatenated to .firestartr/validations/policies like so: .firestartr/validations/policies/[this_fields_value]) applyTo: a list of conditions which describe to which claims this validation applies to. For this, for each value in each element of the list, an AND operation is done with each other value of that element, then for each element an OR operation is done against each other element. See \u0026ldquo;About the applyTo field values\u0026rdquo; to learn more about the possible values of this field. data: key-value pairs, where each key is a variable name and each name its value, to be used inside the regoFile file. üì¢ About validation policies# These are regular .rego files and have no custom or special values. You can learn more about Rego here\n"},{"id":27,"href":"/docs/Validating-our-claims/","title":"Validating Our Claims","section":"Firestartr Documentation","content":" Go to the .firestartr repository and create the validations folder structure. More info here Inside the validations/policies folder, create the policy as a .rego file. You can learn more about Rego here Inside the apps/[app-name]/[claim-kind] folder create a YAML file as follows: name: my-validation description: \u0026#34;My validation description\u0026#34; regoFile: path/to_the/rego_file.rego applyTo: - app: sample-app data: data1: test-data1 data2: test-data2 ...Description of each field:\nname: the name of the validation file, which must be unique between them. description: a brief description of what this validation does. Purely for human readability purpouses. regoFile: path to the rego file, relative to the policies folder (i.e., this field\u0026rsquo;s value will be concatenated to .firestartr/validations/policies like so: .firestartr/validations/policies/[this_fields_value]) applyTo: a list of conditions which describe to which claims this validation applies to. For this, for each value in each element of the list, an AND operation is done with each other value of that element, then for each element an OR operation is done against each other element. See \u0026ldquo;About the applyTo field values\u0026rdquo; to learn more about the possible values of this field. data: key-value pairs, where each key is a variable name and each name its value, to be used inside the regoFile file. ‚ùó About the applyTo field values# Each element of the applyTo field can have the following values:\nApp: apply only to claims who will deploy to exactly the app with this value as its name Name: apply only to the claim with this value as its name (no two claims can have the same name as each other) Kind: apply only to the claims with this value as their kind ResourceType: apply only to the claims with this value as their ResourceType Environment: apply only to claims who will deploy to exactly the env with this value as its name Tenant: apply only to claims who will deploy to exactly the tenant with this value as its name Platform: apply only to claims who will deploy to exactly the platform with this value as its name ‚ÄºÔ∏è Example of the applyTo field criteria# ... applyTo: - app: app1 kind: TFWorkspace env: dev - platform: kubernetes app: app2 env: pre - name: my-claim-1will be converted to:\n(app == app1 \u0026amp;\u0026amp; kind == TFWorkspace \u0026amp;\u0026amp; env == dev) || (platform == kubernetes \u0026amp;\u0026amp; app == app2 \u0026amp;\u0026amp; env == pre) || (name == my-claim-1)\nwhich means \u0026ldquo;Apply this validation to:\nthe claim which deploys to the application app1 AND the environment dev AND has the kind TFWorkspace\nOR\nthe claim which deploys to the platform kubernetes AND the application app2 AND the environment pre\nOR\nthe claim with the name my-claim-1\u0026rdquo;\nüß™ Test your validations quickly# Create your data rule # my-data-rule.yaml --- name: \u0026#34;prefix-test-app\u0026#34; description: \u0026#34;Prefix all claim names with soups- in soups app\u0026#34; regoFile: claim_prefix_name.rego applyTo: - app: test data: prefixName: test- Create your policy .rego package main deny contains msg if { not startswith(input.name, data.data.prefixName) msg := sprintf(\u0026#34;Claim name must start with \u0026#39;%v\u0026#39;, but got: \u0026#39;%v\u0026#39;\u0026#34;, [data.data.prefixName, input.name]) } Create the claim # claim.yaml --- kind: TFWorkspaceClaim lifecycle: production name: test-tenant-pre-vmss resourceType: az-vmss system: \u0026#34;system:test\u0026#34; version: \u0026#34;1.0\u0026#34; providers: terraform: tfStateKey: e951e631-305a-414e-be4f-5562fc952122 policy: apply name: test-tenant-pre-vmss source: remote module: git::https://github.com/test/tfm.git//modules/vmss-soups?ref=vmss-soups-v0.3.5 values: common: resource_group_name: \u0026#34;tenant-test-pre\u0026#34; location: \u0026#34;westeurope\u0026#34; vmss: name: \u0026#34;tenant-soup-pre\u0026#34; sku: \u0026#34;Standard_D2as_v5\u0026#34; instances: 1 Run docker command echo \u0026#34; --- name: \\\u0026#34;prefix-test-app\\\u0026#34; description: \\\u0026#34;Prefix all claim names with soups- in soups app\\\u0026#34; regoFile: claim_prefix_name.rego applyTo: - app: test data: prefixName: test- \u0026#34; \u0026gt; my-data-rule.yaml \u0026amp;\u0026amp; echo \u0026#39; package main deny contains msg if { not startswith(input.name, data.data.prefixName) msg := sprintf(\u0026#34;Claim name must start with \u0026#39;\\\u0026#39;\u0026#39;%v\u0026#39;\\\u0026#39;\u0026#39;, but got: \u0026#39;\\\u0026#39;\u0026#39;%v\u0026#39;\\\u0026#39;\u0026#39;\u0026#34;, [data.data.prefixName, input.name]) } \u0026#39; \u0026gt; claim_prefix_name.rego \u0026amp;\u0026amp; echo \u0026#39; --- kind: TFWorkspaceClaim lifecycle: production name: test-tenant-pre-vmss resourceType: az-vmss system: \u0026#34;system:test\u0026#34; version: \u0026#34;1.0\u0026#34; providers: terraform: tfStateKey: e951e631-305a-414e-be4f-5562fc952122 policy: apply name: test-tenant-pre-vmss source: remote module: git::https://github.com/test/tfm.git//modules/vmss-soups?ref=vmss-soups-v0.3.5 values: common: resource_group_name: \u0026#34;corpme-soups-pre\u0026#34; location: \u0026#34;westeurope\u0026#34; vmss: name: \u0026#34;test-soup-pre\u0026#34; sku: \u0026#34;Standard_D2as_v5\u0026#34; instances: 1 \u0026#39; \u0026gt; claim.yaml \u0026amp;\u0026amp; docker run --rm \\ -v $(pwd)/claim_prefix_name.rego:/validation/claim_prefix_name.rego \\ -v $(pwd)/my-data-rule.yaml:/validation/my-data-rule.yaml \\ -v $(pwd)/claim.yaml:/validation/claim.yaml \\ -w /validation \\ openpolicyagent/conftest \\ --rego-version v1 --output stdout test claim.yaml --data my-data-rule.yaml --policy claim_prefix_name.rego"},{"id":28,"href":"/docs/features/","title":"Features","section":"Firestartr Documentation","content":"üìñ Introduction# We have developed a variety of features that can be installed with firestartr. The ones that have documentation are documented within our features repo, but for convenience they will all be listed here with a link to their docs if they have any.\nüåü Build and dispatch Docker images# Latest v3 v2 üåü Claims repo# Latest üåü Issue templates# This feature only adds templates for Github issues, and thus has no documentation. You can see the templates here:\nBug report (latest) Feature request (latest) üåü Release please# This feature uses the release-please-action action and configures it to manage releases, in a GitHub repository. It also accepts monorepos.\nRelease Please docs (latest) üåü State infra# Latest üåü State repo (legacy)# This is a legacy feature, replaced by the new state repo apps and state repo sys services features\nLatest üåü State Repo Apps# Description:\nState Repo Apps puts you in charge of manual deployments in your GitOps repo! üöÄ With this feature, you can manage infrastructure (TFWorkspace), Kubernetes workloads, and secrets‚Äîall from one spot. üåç Start by tweaking the \u0026ldquo;values\u0026rdquo; in your main/master branch with a commit or PR. üìù Then, fire up GitHub Actions workflows to whip up deployment files (CRs) that land in a PR against deployment, ready for ArgoCD to sync.\nWhat‚Äôs Included:\nTFWorkspace Deployment üõ†Ô∏è: Deploy infrastructure resources with a claim_name. Kubernetes Deployment ‚ò∏Ô∏è: Set up Kubernetes workloads using platform, tenant, and environment. Secrets Deployment üîê: Add secrets for a specific tenant and environment. How It Works üîÑ:\nUpdate the \u0026ldquo;values\u0026rdquo; in main/master (commit directly or via PR). ‚úèÔ∏è Run the corresponding workflow from the \u0026ldquo;Actions\u0026rdquo; tab. ‚ñ∂Ô∏è Get a PR with CRs against deployment. üì¶ Merge it, and ArgoCD takes it from there! ‚úÖ Usage by deployment kind\nKubernetes Deployment: https://github.com/prefapp/features/blob/main/packages/state_repo_apps/templates/docs/KUBERNETES_README.md TFWorkspace Deployment: https://github.com/prefapp/features/blob/main/packages/state_repo_apps/templates/docs/TFWORKSPACES_README.md Secrets Deployment: https://github.com/prefapp/features/blob/main/packages/state_repo_apps/templates/docs/SECRETS_README.md üåü State repo sys services# This feature manages system services for your Kubernetes clusters (sys-services). It organizes critical components like ingress controllers and configuration utilities in a structured repository.\nLatest üåü Tech docs# This features uses mkdocs to ease in documentation creation\nMkDocs documentation "},{"id":29,"href":"/docs/features/build_and_dispatch_docker_images/CHANGELOG/","title":"Changelog","section":"Build and Dispatch Docker Images","content":"Changelog# 5.1.4 (2026-02-09)# Bug Fixes# Simplify UX (#820) (abe10d1) Updated build_images docs to include new fields (#807) (835b2db) 5.1.3 (2025-10-30)# Bug Fixes# Add default value to .firestartr arg (#735) (aed0204) Document features args (#737) (5bdd3c5) 5.1.2 (2025-10-20)# Bug Fixes# Show user input on make_dispatches.yaml workflow (#726) (1292f64) 5.1.1 (2025-07-16)# Bug Fixes# Updated build_and_dispatch_docker_images\u0026rsquo; readme file (#596) (9eadec1) 5.1.0 (2025-07-03)# Features# [build_and_dispatch_docker_images] Use specific github app and feature maintaineance (#586) (abe5370) Bug Fixes# install latest features_renderer (#582) (7944e51) 5.0.7 (2025-06-19)# Bug Fixes# Removed folder creation when checking out the caller repo in make_dispatches workflow (#571) (1e1290e) 5.0.6 (2025-06-17)# Bug Fixes# Added missing build_and_dispatch_docker_images test (#473) (3535bd5) Trigger snapshots workflow (#567) (861b423) 5.0.5 (2025-06-07)# Bug Fixes# permissions (#541) (146e715) 5.0.4 (2025-05-30)# Bug Fixes# Fixed config file pointing to non existent file (#513) (e4aa36b) 5.0.3 (2025-05-28)# Bug Fixes# Fixed tests in all modules (#472) (c77267b) YAMLLint config (#511) (c064e46) 5.0.2 (2025-04-30)# Bug Fixes# Improved clarity of build_and_dispatch_docker_images docs (#447) (8d3bbb2) 5.0.1 (2025-03-25)# Bug Fixes# Added lit constants to build_and_dispatch_docker_images feature (c42605d) Renamed UBUNTU_VERSION to RUNNER_VERSION (167806a) Replace constants with lit (3f31d38) 5.0.0 (2025-03-05)# ‚ö† BREAKING CHANGES# Update make_dispatches.yaml (#379) Features# Update make_dispatches.yaml (#379) (ad542fc) 4.0.5 (2025-02-20)# Bug Fixes# Fixed build_and_dispatch_docker_images workflow inputs documentation (fd03f6a) Improved build_and_dispatch_docker_images documentation (1219e6d) Update build_and_dispatch_docker_image docs for make_dispatches v3 (27b56d0) 4.0.4 (2025-01-27)# Bug Fixes# build_and_dispatch workflows (#334) (83f1fed) 4.0.3 (2025-01-23)# Bug Fixes# Merge pull request #332 from prefapp/fix/release (a46907d) release (a46907d) release (7d2fe55) 4.0.2 (2025-01-23)# Bug Fixes# Merge pull request #328 from prefapp/test/test-commit (6bf4362) Update make_dispatches.yaml (6bf4362) Update make_dispatches.yaml (4ed0174) 4.0.1 (2025-01-23)# Bug Fixes# Parameter quoting on make_dispatches workflow file (8ba2f3e) 4.0.0 (2025-01-23)# ‚ö† BREAKING CHANGES# Merge pull request #324 from prefapp/feat/release-new-build-and-dispatch-docker-image-version Updated build-and-dispatch-docker-images make_dispatches workflow to latest version Merge pull request #322 from prefapp/feat/update-build-and-dispatch-docker-images-version Updated build-and-dispatch-docker-images make_dispatches workflow to latest version Features# Added new cluster parameters (73f1f82) Merge pull request #322 from prefapp/feat/update-build-and-dispatch-docker-images-version (c0b370e) Updated build-and-dispatch-docker-images make_dispatches workflow to latest version (c0b370e) Updated build-and-dispatch-docker-images make_dispatches workflow to latest version (ae4830f) Updated make_dispatches workflow to use app authentication instead of a PAT (e4149eb) Bug Fixes# Fixed filter_by_platform parameter name and description (690d729) Merge pull request #324 from prefapp/feat/release-new-build-and-dispatch-docker-image-version (aff72c8) Removed 11th workflow input parameter (d4ab3ef) Removed unused parameter (3914feb) Small change to release new version (aff72c8) Small change to release new version (924b323) Small update to workflow parameters description (be3dcab) 3.0.0 (2024-10-29)# ‚ö† BREAKING CHANGES# Updated make_dispatches.yaml workflow to v2 Merge pull request #314 from prefapp/feat/update-make-dispatches-version Updated make_dispatches.yaml workflow to v2 Features# Merge pull request #314 from prefapp/feat/update-make-dispatches-version (ba5ab8f) Updated make_dispatches.yaml workflow to v2 (ba5ab8f) Updated make_dispatches.yaml workflow to v2 (6e4c1c1) 2.0.1 (2024-10-11)# Bug Fixes# pass secrets explicity (#308) (e8e3d89) 2.0.0 (2024-10-07)# ‚ö† BREAKING CHANGES# update build_and_dispatch_docker_images (#296) Features# update build_and_dispatch_docker_images (#296) (9073a26) 1.1.0 (2024-09-27)# Features# Added config verification workflow for this feature (e3aa776) Added support for verifying old extension YAMLs (1694244) Merge pull request #287 from prefapp/feat/verify-features-config (e3aa776) Updated workflow to use dorny/paths-filter to check if the config files have been changed (0dd2bec) Bug Fixes# Added missing parameter to dorny/paths-filter (fd0c4d1) Removed depth: 0 from actions/checkout (1696da0) Simplified workflow (1efb417) 1.0.0 (2024-09-18)# ‚ö† BREAKING CHANGES# add permission packages:write and remove unused step (#283) Features# add permission packages:write and remove unused step (#283) (79c32d0) Bug Fixes# overwrite_version workflow documentation (bdf43e7) Updated actual feature documentation (355d1e5) 0.7.0 (2024-08-23)# Features# Added new flavor filter argument (7f09016) Added new flavor filter argument (c953673) Merge pull request #279 from prefapp/feat/add-flavor-filter (7f09016) 0.6.1 (2024-08-21)# Bug Fixes# Merge pull request #275 from prefapp/fix/remove-emojis-from-workflows (d196d19) Removed emojis from make_dispatches.yaml as they are not properly base64 encoded (d196d19) Removed emojis from make_dispatches.yaml as they are not properly base64 encoded (75f9000) 0.6.0 (2024-08-21)# Features# Merge pull request #273 from prefapp/feat/update-make-dispatches-workflow (e18f572) Updated make_dispatches.yaml to latest version (e18f572) Updated make_dispatches.yaml to latest version (7c630e0) 0.5.0 (2024-08-14)# Features# support dynamic default branch interpolation (#271) (bd038e5) 0.4.0 (2024-08-14)# Features# Merge pull request #267 from prefapp/feat/rename-build-and-dispatch (c9dba6b) renamed build_and_dispatch feature to build_and_dispatch_docker_images (4756f0a) Renamed build_and_dispatch feature to build_and_dispatch_docker_images (c9dba6b) 0.3.1 (2024-07-29)# Bug Fixes# patches array to object (#261) (e72fb2c) 0.3.0 (2024-07-29)# Features# new build and dispatch (#260) (532d333) 0.2.1 (2024-02-08)# Bug Fixes# Bump features version (8f398b9) Merge pull request #229 from prefapp/juanjosevazquezgil-patch-3 (8f398b9) 0.2.0 (2024-01-08)# Features# Merge pull request #221 from prefapp/fix/build_and_dispatch-config-file (c428f6c) Update config.yaml (c428f6c) 0.1.3 (2023-10-24)# Bug Fixes# Added missing parameters to build_and_dispatch and fixed existing ones (63d2562) Added missing parameters to build_and_dispatch and fixed existing ones (0f92da8) Merge pull request #209 from prefapp/fix/add-missing-parameters-to-build-and-dispatch (63d2562) 0.1.2 (2023-10-24)# Bug Fixes# Merge pull request #207 from prefapp/test/add-additional-arguments-to-build-and-dispatch (da112de) Updated build_and_dispatch config file to do some testing (da112de) Updated build_and_dispatch config file to do some testing (1a2c768) 0.1.1 (2023-10-24)# Bug Fixes# Fixed build_and_dispatch files location (939f78e) Fixed build_and_dispatch files location (d1d1468) Merge pull request #205 from prefapp/fix/build-and-dispatch-files (939f78e) 0.1.0 (2023-10-24)# Features# Add build_and_dispatch feature (178e792) Add build_and_dispatch feature (b2bc03e) Merge pull request #202 from prefapp/build_and_dispatch (178e792) Bug Fixes# Test errors (4626d70) "},{"id":30,"href":"/docs/features/catalog_repo/CHANGELOG/","title":"Changelog","section":"Catalog Repo","content":"Changelog# 1.3.1 (2026-02-26)# Bug Fixes# auto-merge logic (#878) (a83ada9) 1.3.0 (2026-02-26)# Features# Add AUTO_MERGE control file support for state-infra, state-github, and catalog (#856) (73ead5e) new validation structure (#635) (7e3f1cb) Bug Fixes# Fixed all git config calls in all feature\u0026rsquo;s workflows (#811) (3c961d5) 1.2.0 (2025-07-21)# Features# actions version in lit for catalog (#609) (2338f10) add scheduler for hydrate on catalog (#606) (4740367) 1.1.0 (2025-07-15)# Features# [catalog_repo] Use specific github app and feature maintaineance (#569) (16fa403) support PAT on features rendering process (#523) (cd40976) Bug Fixes# Fixed tests in all modules (#472) (c77267b) install latest features_renderer (#582) (7944e51) 1.0.0 (2025-04-30)# Features# Added catalog_repo feature (#438) (697e56f) "},{"id":31,"href":"/docs/features/charts_repo/CHANGELOG/","title":"Changelog","section":"Charts Repo","content":"Changelog# 1.4.3 (2025-12-17)# Bug Fixes# charts_repo: fix version extraction in generate-artifact.yaml (#784) (728c539) 1.4.2 (2025-09-10)# Bug Fixes# charts: add yamllint (#680) (89faf78) 1.4.1 (2025-09-09)# Bug Fixes# user managed files (#678) (8f6b9df) 1.4.0 (2025-09-05)# Features# add release please config (#671) (716fe87) 1.3.0 (2025-09-04)# Features# add release please config (#667) (1f0c336) 1.2.0 (2025-09-01)# Features# use new validation (#631) (eaf2bbf) Bug Fixes# missing arg in template (#660) (e5830c3) 1.1.0 (2025-08-04)# Features# add charts repo (#616) (c4c2f8e) "},{"id":32,"href":"/docs/features/claims_repo/CHANGELOG/","title":"Changelog","section":"Claims Repo","content":"Changelog# 1.17.1 (2026-02-26)# Bug Fixes# auto-merge logic (#878) (a83ada9) 1.17.0 (2026-02-26)# Features# Add AUTO_MERGE control file support for state-infra, state-github, and catalog (#856) (73ead5e) 1.16.3 (2026-02-25)# Bug Fixes# Disable auto merge for infra hydration workflow (#849) (e985348) 1.16.2 (2026-02-09)# Bug Fixes# Fixed all git config calls in all feature\u0026rsquo;s workflows (#811) (3c961d5) Simplify UX (#820) (abe10d1) Updated deletion workflows to include link to last state PR (#816) (355a88e) Updated name and run-name of update features workflow (#819) (0332c23) 1.16.1 (2026-01-23)# Bug Fixes# claims_defaults.yaml platformOwner example (#781) (f72c0ae) PR not merging sometimes (#792) (cdc910b) 1.16.0 (2025-12-09)# Features# Add automerge input (#776) (1749ca3) 1.15.4 (2025-10-21)# Bug Fixes# Document features args (#737) (5bdd3c5) set network mode and fixed dns servers in docker-compose (f9e06ab) 1.15.3 (2025-10-06)# Bug Fixes# add orgwebhooks folder to claims_repo (ee40337) argument with special chars (#713) (15fd4db) update claims feature to specify proper cr defaults path (#708) (3faf414) Update default system name in claims_repo feature (#662) (484e5c5) 1.15.2 (2025-09-19)# Bug Fixes# Change user_managed flag to true for docker-compose (#688) (7f95774) claims: remove unused inputs from docker-compose (#693) (46e52fa) 1.15.1 (2025-09-15)# Bug Fixes# some improvements to PR-verify claims workflow (#685) (576a6f4) 1.15.0 (2025-08-28)# Features# claims control docker-compose (#654) (df2e8ec) 1.14.1 (2025-08-25)# Bug Fixes# Annotations now are also added to features CRs (#645) (9566207) 1.14.0 (2025-08-22)# Features# new validation structure (#635) (7e3f1cb) Bug Fixes# Update features to use the local gh binary (#637) (de5b778) Updated update-claims-features workflow to use new dagger module functionality (#646) (4c62cca) 1.13.0 (2025-08-04)# Features# Add validation workflow (#607) (64ea107) Bug Fixes# actionlint errors in claims_repo feature (#627) (b26c8df) PR-verify (#620) (cf77037) update claims feature fixtures (#624) (69ef296) 1.12.0 (2025-07-21)# Features# place actions version in $lit for claims_repo (#608) (5ffe8cd) 1.11.0 (2025-07-14)# Features# [claims_repo] Use specific github app and feature maintaineance (#577) (86e89b4) Bug Fixes# pr-verify (#594) (143b46c) 1.10.4 (2025-07-03)# Bug Fixes# install latest features_renderer (#582) (7944e51) 1.10.3 (2025-07-02)# Bug Fixes# Added sparse-checkout property to update-claims-features workflow (#583) (ba74e11) 1.10.2 (2025-07-01)# Bug Fixes# pr-verify (#578) (61b6d4a) 1.10.1 (2025-06-18)# Bug Fixes# hardcoded repo name (#563) (2d924cf) Simplify pr-verify logic (#561) (6512590) Update workflow PR creation process (#547) (9edcbfe) 1.10.0 (2025-06-12)# Features# change to new githubapps model and fix actions versions (#533) (57f17d5) Bug Fixes# add checkout infra repo (#553) (4a9529d) 1.9.3 (2025-06-11)# Bug Fixes# pr-verify (#551) (0a178c2) 1.9.2 (2025-06-10)# Bug Fixes# Add OrgWebhookClaim to delete workflow (#516) (21ba720) update-claims workflows (#544) (7690ecf) 1.9.1 (2025-06-09)# Bug Fixes# Add lazy loading to pr-verify workflow (#537) (44ec91d) Moved plan label to cr repo PR (#543) (9eedad6) 1.9.0 (2025-06-04)# Features# Individual claim rendering (#508) (2d29428) 1.8.2 (2025-06-03)# Bug Fixes# Fixed read path on infra claim hydration (#529) (605df42) 1.8.1 (2025-06-03)# Bug Fixes# Fixed claimsPath calculation logic not being recursive (#525) (f04e568) Infra claims hydration (#527) (ac9c835) 1.8.0 (2025-06-03)# Features# support PAT on features rendering process (#523) (cd40976) 1.7.7 (2025-06-02)# Bug Fixes# Wrong YAML parse function (#521) (c13d361) 1.7.6 (2025-06-02)# Bug Fixes# Don\u0026rsquo;t send CRs as env variables (#519) (eb12ea9) 1.7.5 (2025-05-14)# Bug Fixes# Small fixes to uninstall_features_workflow (#503) (d7763ff) 1.7.4 (2025-05-13)# Bug Fixes# hydrate-github-claims workflows (#501) (799cecc) 1.7.3 (2025-05-13)# Bug Fixes# uninstall-repository-feature workflow (#499) (85b9702) 1.7.2 (2025-05-13)# Bug Fixes# Fixed hydrate-github-claim not hydrating features (#497) (234b4a8) 1.7.1 (2025-05-12)# Bug Fixes# Fixed missing workflow from config (#492) (cc4ce22) 1.7.0 (2025-05-08)# Features# Added feature uninstallation workflow (#489) (25a869b) 1.6.8 (2025-05-07)# Bug Fixes# checkout refs in claims feature (#469) (4687303) remove unexpected administrators group and not-compatible paths with bootstrap process (b47dacb) 1.6.7 (2025-05-05)# Bug Fixes# Added plan label to delete-infra-claim workflow (#462) (dc8c2d8) 1.6.6 (2025-04-30)# Bug Fixes# Fixed claim deletion workflows claim path (#460) (3bb67b1) 1.6.5 (2025-04-30)# Bug Fixes# Fixed hydrate-github-claim workflow (#459) (5260df3) 1.6.4 (2025-04-30)# Bug Fixes# Improve hydrate workflows\u0026rsquo; PR body (#439) (5ede26b) 1.6.3 (2025-04-30)# Bug Fixes# claims_repo PR Verify workflow (#450) (f7ab0c5) 1.6.2 (2025-04-29)# Bug Fixes# Fixed install method for new firestartr-cli version (#448) (a5ef08d) 1.6.1 (2025-04-24)# Bug Fixes# claims_repo feature config file (#441) (7946b15) 1.6.0 (2025-04-24)# Features# claims_repo feature now also creates the basic folder structure of the claims repo (#434) (0fff2ed) 1.5.3 (2025-04-21)# Bug Fixes# Fixed wrong parameter name (#430) (bca4883) 1.5.2 (2025-04-21)# Bug Fixes# Updated docs (#428) (c14654c) 1.5.1 (2025-04-09)# Bug Fixes# Dagger module version (#425) (2ed5c86) 1.5.0 (2025-04-09)# Features# Added additional inputs to update-claims-features workflow (#423) (f1c0f9a) 1.4.2 (2025-04-08)# Bug Fixes# default-branch value (#421) (e637311) 1.4.1 (2025-04-08)# Bug Fixes# Update update claims workflow to claims repo (#419) (9f1a2c6) 1.4.0 (2025-04-08)# Features# Added new workflow to claims_repo feature (#415) (9e153b6) 1.3.0 (2025-04-07)# Features# Added plan label to infra claims hydration (#413) (89b2bf6) 1.2.6 (2025-04-03)# Bug Fixes# Removed lit values with a slash in them (a9a84c6) Removed lit values with a slash in them (80e13e5) 1.2.5 (2025-03-31)# Bug Fixes# Delete other files when main yaml file is deleted (163b0be) Delete other files when main yaml file is deleted (5c451f4) 1.2.4 (2025-03-25)# Bug Fixes# Added lit constants to claims_repo feature (aef1d2e) Replace constants with lit (3f31d38) 1.2.3 (2025-03-18)# Bug Fixes# resultList variable (d9d62cd) resultList variable (a7e01ae) 1.2.2 (2025-03-18)# Bug Fixes# Env variables (ad869f0) Env variables (cb1c976) 1.2.1 (2025-03-17)# Bug Fixes# claims_repo deletion wf (2402921) claims_repo deletion wf (6f95313) 1.2.0 (2025-03-17)# Features# Update documentation and workflows (52f98cd) Updated docs and workflows (8128062) Bug Fixes# Config errors (dc802f1) Documentation errors (dfc1fe4) Minor corrections (82ef2ec) Removed tests (4d5b8f3) 1.1.0 (2025-03-13)# Features# Add documentation (3cddff3) Added documentation (fbbd44c) Added pr-verify workflow to feature (f972950) Added pr-verify workflow to feature (125b408) Bug Fixes# Doc improvements (19efd91) Fixed tests (f55d4b8) Improved docs (261f0a8) Small gramatical fixes (6f70d2f) 1.0.0 (2025-03-11)# Features# Added claims_repo feature (6f5c39c) Added claims_repo feature (c68f308) Bug Fixes# Added tests (ceb59c1) Merge pull request #384 from prefapp/fix/release-please (91c178d) Update config.yaml (91c178d) "},{"id":33,"href":"/docs/features/features_repo/CHANGELOG/","title":"Changelog","section":"Features Repo","content":"Changelog# 0.3.1 (2026-02-26)# Bug Fixes# use temporal mustache tags to scape block (c18b4d1) use temporal mustache tags to scape block (#857) (c18b4d1) 0.3.0 (2026-02-25)# Features# features_repo: add doc about tests and update packages urls (#853) (6c3902b) 0.2.0 (2026-02-25)# Features# Add feature to create custom features repo (#846) (1971086) "},{"id":34,"href":"/docs/features/firestartr_repo/CHANGELOG/","title":"Changelog","section":"Firestartr Repo","content":"Changelog# 1.0.0 (2025-04-24)# Features# Added firestartr_repo feature (#437) (5a5b807) "},{"id":35,"href":"/docs/features/issue_templates/CHANGELOG/","title":"Changelog","section":"Issue Templates","content":"Changelog# 1.3.0 (2026-02-23)# Features# new validation structure (#635) (7e3f1cb) 1.2.1 (2024-02-08)# Bug Fixes# Bump features version (8f398b9) Merge pull request #229 from prefapp/juanjosevazquezgil-patch-3 (8f398b9) 1.2.0 (2024-01-08)# Features# support new features interface (2b414b2) 1.1.0 (2023-06-08)# Features# release please (#188) (20374e6) 1.0.2 (2023-06-07)# Bug Fixes# Add package json for issues templates (#181) (f65d98a) Update upgradeable attribute name (#171) (535c663) 1.0.1 (2023-05-12)# Bug Fixes# Issue customizations are now upgradeable (#159) (195746a) 1.0.0 (2023-05-12)# ‚ö† BREAKING CHANGES# New feature to customizae issue templates (#157) Features# New feature to customizae issue templates (#157) (8072939) "},{"id":36,"href":"/docs/features/release_please/CHANGELOG/","title":"Changelog","section":"Release Please","content":"Changelog# 1.4.1 (2026-02-23)# Bug Fixes# release-please: Avoid trigger other workflows (#839) (5514867) 1.4.0 (2025-11-13)# Features# new validation structure (#635) (7e3f1cb) release_please: update to v4 version, update checkout and docs (#758) (6985a65) Bug Fixes# Fixed tests in all modules (#472) (c77267b) install latest features_renderer (#582) (7944e51) 1.3.1 (2024-02-08)# Bug Fixes# Bump features version (8f398b9) Merge pull request #229 from prefapp/juanjosevazquezgil-patch-3 (8f398b9) 1.3.0 (2024-01-08)# Features# release-please support new interface (#219) (0ac56b5) 1.2.0 (2024-01-05)# Features# update to new version (a812c69) update to new version (4455c47) 1.1.0 (2023-09-25)# Features# update release please feature (4b14f47) 1.0.1 (2023-06-16)# Bug Fixes# Update tag used by renderer (#192) (49d4968) 1.0.0 (2023-06-08)# ‚ö† BREAKING CHANGES# Add release please feature to release please (#190) Features# release please (#188) (20374e6) Bug Fixes# Add release please feature to release please (#190) (b1d891d) "},{"id":37,"href":"/docs/features/state_github/CHANGELOG/","title":"Changelog","section":"State Github","content":"Changelog# 1.2.0 (2026-02-26)# Features# Add AUTO_MERGE control file support for state-infra, state-github, and catalog (#856) (73ead5e) 1.1.0 (2026-02-20)# Features# new validation structure (#635) (7e3f1cb) Bug Fixes# Branch protection on expander_branch_strategies.yaml (#663) (1acf3c2) Fixed tests in all modules (#472) (c77267b) install latest features_renderer (#582) (7944e51) 1.0.0 (2025-04-24)# Features# Added new state_github feature (#436) (e8f3791) "},{"id":38,"href":"/docs/features/state_infra/CHANGELOG/","title":"Changelog","section":"State Infra","content":"Changelog# 1.2.0 (2026-02-26)# Features# add actionlint to pr-verify (#617) (31ad8fa) Add AUTO_MERGE control file support for state-infra, state-github, and catalog (#856) (73ead5e) new validation structure (#635) (7e3f1cb) Bug Fixes# Fixed tests in all modules (#472) (c77267b) install latest features_renderer (#582) (7944e51) 1.1.0 (2025-04-24)# Features# Updated repo skeleton files (#435) (ac5eecb) 1.0.0 (2025-01-23)# Features# Added new state_infra feature (58a18dc) Added STATE_INFRA_README.md (82a49d2) Merge pull request #294 from prefapp/feat/add-notify-workflows-to-state-repo-feature (58a18dc) Bug Fixes# Renamed state_repo_init to state_infra (3504ffb) Updated notify-and-hydrate.yaml workflow to latest version (8936469) Updated package.json (fc255b5) Variable names (FIRESTARTER -\u0026gt; FIRESTARTR) - hydrate.yaml workflow still using Azure (bad1f94) "},{"id":39,"href":"/docs/features/state_repo/CHANGELOG/","title":"Changelog","section":"State Repo","content":"Changelog# 2.2.1 (2026-02-09)# Bug Fixes# Fixed all git config calls in all feature\u0026rsquo;s workflows (#811) (3c961d5) 2.2.0 (2025-10-21)# Features# new validation structure (#635) (7e3f1cb) Bug Fixes# PR-verify (#620) (cf77037) Update config.yaml (#321) (debc738) Update state-repo documentation after splitting sys-service helmfile (ad647b8) 2.1.0 (2025-07-22)# Features# [state-repo] Add manual apply workflows (#613) (afd33b8) 2.0.0 (2025-07-18)# ‚ö† BREAKING CHANGES# [state_repo] Add helm uninstall and feature maintaneance (#592) Features# [state_repo] Add helm uninstall and feature maintaneance (#592) (66e873e) 1.6.1 (2025-03-03)# Bug Fixes# Continue on error (#375) (d328915) 1.6.0 (2025-01-27)# Features# Create new state repo features (#335) (054b7c0) Bug Fixes# Recover legacy package (#338) (3cdea5c) 1.5.0 (2025-01-23)# Features# Get pat from github app (#318) (276f148) 1.4.0 (2024-10-30)# Features# Added support for state-repo-update-images action (85777db) Added support for state-repo-update-images action (4f07e76) Merge pull request #316 from prefapp/feat/state-repo-update-images-v5-support (85777db) 1.3.2 (2024-10-29)# Bug Fixes# Fix cluster documentation (fcf53fd) Fix documentation (2feff97) 1.3.1 (2024-10-14)# Bug Fixes# Fix state repo deploy step (#310) (58850e5) 1.3.0 (2024-10-11)# Features# Add check run workflow collector (#306) (ea714e6) 1.2.2 (2024-10-10)# Bug Fixes# Update docs path (#304) (d400b15) 1.2.1 (2024-10-10)# Bug Fixes# Specify reusable workflow version (#302) (194b27d) 1.2.0 (2024-10-10)# Features# Deploy documentation and usage guide (#293) (c4e88cb) Updated feature with automerge and hydrate workflows (ae93abf) Bug Fixes# Moved workflows to their own unique feature (6f9b80f) 1.1.2 (2024-09-19)# Bug Fixes# Fix template name (#290) (5164cc0) 1.1.1 (2024-09-19)# Bug Fixes# Fix sops argument configuration (#289) (f5f46f2) 1.1.0 (2024-09-18)# Features# Add state repo workflow feature (#286) (a3a2be3) "},{"id":40,"href":"/docs/features/state_repo_apps/CHANGELOG/","title":"Changelog","section":"State Repo Apps","content":"Changelog# 3.8.1 (2026-02-09)# Bug Fixes# Fixed all git config calls in all feature\u0026rsquo;s workflows (#811) (3c961d5) 3.8.0 (2026-01-23)# Features# Add deletion workflows (#795) (156a155) New automatic workflows (#794) (03981fb) Bug Fixes# README (d2dd0d5) readmes (b51fb44) Update docs (#797) (d8bd6c8) 3.7.1 (2025-12-23)# Bug Fixes# Bold CR name (#789) (e802577) 3.7.0 (2025-12-23)# Features# Add validations to deployment branch state repo apps (#780) (5c96504) 3.6.2 (2025-12-10)# Bug Fixes# Include gitkeep in state features (#743) (7250b58) Set oci path using home instead of pwd (#782) (1362466) 3.6.1 (2025-11-26)# Bug Fixes# state_repo_apps generate deployment (#752) (d69436d) 3.6.0 (2025-10-09)# Features# state_repo_apps: add support for ghcr and non-oci helm registries (#720) (ef918b2) 3.5.0 (2025-08-18)# Features# new validation structure (#635) (7e3f1cb) Bug Fixes# Update features to use the local gh binary (#637) (de5b778) 3.4.0 (2025-07-02)# Features# [state-repo-apps] Use specific github app and feature maintaineance (#575) (0992905) 3.3.0 (2025-05-30)# Features# get dagger version from module (#518) (e0fcf89) update state-repo-apps documentation (#432) (4004b76) 3.2.1 (2025-04-30)# Bug Fixes# make firestartr file not managed (445c7a9) 3.2.0 (2025-04-29)# Features# update orchestrator and dagger version (#451) (a8d37fd) 3.1.1 (2025-03-22)# Bug Fixes# use $lit in common configs across workflows to avoid missconfigurations (#403) (1cd659f) 3.1.0 (2025-03-19)# Features# update runners to ubuntu 24.04, doc and dagger module (#399) (855f443) update state_repo_apps hydrator version to v4 and add docs (#397) (5da1ed1) 3.0.0 (2025-03-05)# ‚ö† BREAKING CHANGES# state-repo-apps - update hydrate-orchestrator to v3 Features# state-repo-apps - update hydrate-orchestrator to v3 (d66a3c5) 2.3.2 (2025-02-24)# Bug Fixes# Update workflow job names (#371) (335f461) 2.3.1 (2025-02-20)# Bug Fixes# state_apps update dagger module version (#370) (d6f5897) 2.3.0 (2025-02-13)# Features# update to version v2.1.1 (7fd2123) 2.2.0 (2025-02-12)# Features# include dispatch kubernetes workflow (040ec1d) 2.1.0 (2025-02-12)# Features# update prs automatically (#360) (6fdf772) 2.0.1 (2025-02-10)# Bug Fixes# Fixed missing update to validate-pr workflow (6a7c630) Fixed missing update to validate-pr workflow (fcbd6d7) Merge pull request #355 from prefapp/fix/missing-workflow-update (6a7c630) 2.0.0 (2025-02-10)# ‚ö† BREAKING CHANGES# Merge pull request #353 from prefapp/fix/remove-app-parameter Features# Merge pull request #353 from prefapp/fix/remove-app-parameter (fe8fc52) Bug Fixes# Removed additional use of the APP_NAME parameter (ce81294) Removed APP_NAME parameter (fe8fc52) Removed APP_NAME parameter (60d9d1c) 1.3.0 (2025-02-04)# Features# Update orchestrator version (#351) (e416a7e) 1.2.2 (2025-02-04)# Bug Fixes# bump validate-pr dagger version (d472763) 1.2.1 (2025-02-04)# Bug Fixes# update dagger version (e96e7e8) 1.2.0 (2025-01-31)# Features# update state repo feature fixing new pr bugs (89c969e) 1.1.0 (2025-01-27)# Features# Create new state repo features (#335) (054b7c0) Bug Fixes# Add oci auth step in sys service pr verify (#337) (d259463) Recover legacy package (#338) (3cdea5c) 1.5.0 (2025-01-23)# Features# Get pat from github app (#318) (276f148) 1.4.0 (2024-10-30)# Features# Added support for state-repo-update-images action (85777db) Added support for state-repo-update-images action (4f07e76) Merge pull request #316 from prefapp/feat/state-repo-update-images-v5-support (85777db) 1.3.2 (2024-10-29)# Bug Fixes# Fix cluster documentation (fcf53fd) Fix documentation (2feff97) 1.3.1 (2024-10-14)# Bug Fixes# Fix state repo deploy step (#310) (58850e5) 1.3.0 (2024-10-11)# Features# Add check run workflow collector (#306) (ea714e6) 1.2.2 (2024-10-10)# Bug Fixes# Update docs path (#304) (d400b15) 1.2.1 (2024-10-10)# Bug Fixes# Specify reusable workflow version (#302) (194b27d) 1.2.0 (2024-10-10)# Features# Deploy documentation and usage guide (#293) (c4e88cb) Updated feature with automerge and hydrate workflows (ae93abf) Bug Fixes# Moved workflows to their own unique feature (6f9b80f) 1.1.2 (2024-09-19)# Bug Fixes# Fix template name (#290) (5164cc0) 1.1.1 (2024-09-19)# Bug Fixes# Fix sops argument configuration (#289) (f5f46f2) 1.1.0 (2024-09-18)# Features# Add state repo workflow feature (#286) (a3a2be3) "},{"id":41,"href":"/docs/features/state_repo_sys_services/CHANGELOG/","title":"Changelog","section":"State Repo Sys Services","content":"Changelog# 2.3.7 (2026-01-20)# Bug Fixes# readmes (b51fb44) 2.3.6 (2026-01-20)# Bug Fixes# README (d2dd0d5) 2.3.5 (2026-01-20)# Bug Fixes# Update docs (#797) (d8bd6c8) 2.3.4 (2025-12-10)# Bug Fixes# Set oci path using home instead of pwd (#782) (1362466) 2.3.3 (2025-11-26)# Bug Fixes# github emojis (c395560) 2.3.2 (2025-11-07)# Bug Fixes# state_repo_apps generate deployment (#752) (d69436d) 2.3.1 (2025-10-09)# Bug Fixes# state_repo_sys_services: add support for ghcr and non-oci helm registries (#718) (03f1acf) 2.3.0 (2025-08-28)# Features# new validation structure (#635) (7e3f1cb) Bug Fixes# Update features to use the local gh binary (#637) (de5b778) 2.2.0 (2025-07-02)# Features# [state_repo_sys_services] Use specific github app and feature maintaineance (#576) (23d1388) 2.1.0 (2025-06-05)# Features# support dagger version dinamically (#535) (98be657) 2.0.1 (2025-02-24)# Bug Fixes# Update workflow job names (#371) (335f461) 2.0.0 (2025-02-12)# ‚ö† BREAKING CHANGES# Merge pull request #358 from prefapp/feat/update-state-repo-sys-services Features# Merge pull request #358 from prefapp/feat/update-state-repo-sys-services (217cd88) Update state_repo_sys_services (217cd88) 1.3.0 (2025-02-04)# Features# Update orchestrator version (#351) (e416a7e) 1.2.1 (2025-02-03)# Bug Fixes# update dagger version (7973aaf) 1.2.0 (2025-01-31)# Features# update state repo feature fixing new pr bugs (89c969e) 1.1.4 (2025-01-29)# Bug Fixes# remove unnecesary input (63493c6) 1.1.3 (2025-01-29)# Bug Fixes# update auth dir file path (3ce9f12) 1.1.2 (2025-01-29)# Bug Fixes# add required flag (#341) (f4e5d7c) 1.1.1 (2025-01-27)# Bug Fixes# template path (#339) (0b41442) 1.1.0 (2025-01-27)# Features# Create new state repo features (#335) (054b7c0) Bug Fixes# Add oci auth step in sys service pr verify (#337) (d259463) "},{"id":42,"href":"/docs/features/tech_docs/CHANGELOG/","title":"Changelog","section":"Tech Docs","content":"Changelog# 0.10.2 (2024-02-08)# Bug Fixes# tech_docs repository name (#231) (516ec21) 0.10.1 (2024-02-08)# Bug Fixes# Bump features version (8f398b9) Merge pull request #229 from prefapp/juanjosevazquezgil-patch-3 (8f398b9) 0.10.0 (2024-01-08)# Features# Merge pull request #217 from prefapp/feat/updated-repo-name (03823db) Update config.yaml (03823db) 0.9.0 (2024-01-08)# Features# Merge pull request #214 from prefapp/juanjosevazquezgil-patch-2 (fcd703a) Updated tech_docs config.yaml $ref format to latest version (fcd703a) Bug Fixes# Errors on test (da7d884) Fixed unused test field (88000f0) Updated fixture file (4755aee) 0.8.0 (2024-01-05)# Features# new features format (#211) (a6cbcf0) 0.7.0 (2023-10-11)# Features# Merge pull request #199 from prefapp/juanjosevazquezgil-patch-1 (52329ea) Update README.md (52329ea) 0.6.1 (2023-06-16)# Bug Fixes# Update tag used by renderer (#192) (49d4968) 0.6.0 (2023-06-08)# Features# release please (#188) (20374e6) 0.5.0 (2023-06-07)# Features# Add build-images (#176) (d9e6e40) Bug Fixes# Update upgradeable attribute name (#171) (535c663) 0.4.0 (2023-05-24)# Features# Remove unused packages (#153) (d12d28e) 0.3.1 (2023-05-04)# Bug Fixes# config.yaml from tech_docs (9060afb) 0.3.0 (2023-05-04)# Features# Add back upgradable field (#150) (e69be5b) Bug Fixes# Tech docs config.yaml (188d3b5) 0.2.0 (2023-04-17)# Features# Feature/add tech docs (#148) (8161681) "},{"id":43,"href":"/docs/features/terraform-infra/CHANGELOG/","title":"Changelog","section":"Terraform Infra","content":"Changelog# 1.7.1 (2026-01-23)# Bug Fixes# terraform-infra: update git diff for PR (#803) (fe6e42a) 1.7.0 (2025-11-26)# Features# exclude files from terraform plan (#762) (e448ad6) 1.6.0 (2025-11-07)# Features# add files to terraform-infra (#748) (c9a9b12) 1.5.2 (2025-10-20)# Bug Fixes# add missing backends.tf to terraform-infra package (#732) (4ca10c2) 1.5.1 (2025-10-15)# Bug Fixes# remove duplicate action output (#730) (5130784) 1.5.0 (2025-10-03)# Features# add cardinality tfstate infra (#706) (43a10bf) 1.4.1 (2025-09-22)# Bug Fixes# fixes for github action workflows (f727563) refactoring for exit conditions (becf82e) terraform destroy (#694) (f5b3246) 1.4.0 (2025-09-10)# Features# update terraform infra workflows (5d04d04) 1.3.2 (2025-09-05)# Bug Fixes# bootstrap script and cloudformation role (#674) (f54fbb1) 1.3.1 (2025-09-05)# Bug Fixes# app-token step (#669) (6bcd020) 1.3.0 (2025-09-04)# Features# update to terraform-infra template with tested code from bootstrap repo (#659) (91d6882) 1.2.0 (2025-08-27)# Features# include option to not wait for inputs in terraform commands (#655) (49ea634) new validation structure (#635) (7e3f1cb) 1.1.0 (2025-08-07)# Features# Add new feature for Terraform infrastructure (#598) (f3ff30e) "}]